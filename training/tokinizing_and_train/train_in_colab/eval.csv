abstract,question,answer,predict_answer,exact_match,containment_match
"In this paper, we present a QA system enabling NL questions against Linked Data, designed and adopted by the Tor Vergata University AI group in the QALD-3 evaluation. The system integrates lexical semantic modeling and statistical inference within a complex architecture that decomposes the NL interpretation task into a cascade of three different stages: (1) The selection of key ontological information from the question (i.e. predicate, arguments and properties), (2) the location of such salient information in the ontology through the joint disambiguation of the different candidates and (3) the compilation of the final SPARQL query. This architecture characterizes a novel approach for the task and exploits a graphical model (i.e. an Hidden Markov Model) to select the proper ontological triples according to the graph nature of RDF. In particular, for each query an HMM model is produced whose Viterbi solution is the comprehensive joint disambiguation across the sentence elements. The combination of these approaches achieved interesting results in the QALD competition. The RTV is in fact within the group of participants performing slightly below the best system, but with smaller requirements and on significantly poorer input information.",what implementation ?,RTV,qa,False,False
"Visualizing Resource Description Framework (RDF) data to support decision-making processes is an important and challenging aspect of consuming Linked Data. With the recent development of JavaScript libraries for data visualization, new opportunities for Web-based visualization of Linked Data arise. This paper presents an extensive evaluation of JavaScript-based libraries for visualizing RDF data. A set of criteria has been devised for the evaluation and 15 major JavaScript libraries have been analyzed against the criteria. The two JavaScript libraries with the highest score in the evaluation acted as the basis for developing LODWheel (Linked Open Data Wheel) - a prototype for visualizing Linked Open Data in graphs and charts - introduced in this paper. This way of visualizing RDF data leads to a great deal of challenges related to data-categorization and connecting data resources together in new ways, which are discussed in this paper.",what implementation ?,LODWheel,lodwheel,True,True
"Querying the Semantic Web and analyzing the query results are often complex tasks that can be greatly facilitated by visual interfaces. A major challenge in the design of these interfaces is to provide intuitive and efficient interaction support without limiting too much the analytical degrees of freedom. This paper introduces SemLens, a visual tool that combines scatter plots and semantic lenses to overcome this challenge and to allow for a simple yet powerful analysis of RDF data. The scatter plots provide a global overview on an object collection and support the visual discovery of correlations and patterns in the data. The semantic lenses add dimensions for local analysis of subsets of the objects. A demo accessing DBpedia data is used for illustration.",what implementation ?,SemLens,"semlens,",True,True
"Clustered graph visualization techniques are an easy to understand way of hiding complex parts of a visualized graph when they are not needed by the user. When visualizing RDF, there are several situations where such clusters are defined in a very natural way. Using this techniques, we can give the user optional access to some detailed information without unnecessarily occupying space in the basic view of the data. This paper describes algorithms for clustered visualization used in the Trisolda RDF visualizer. Most notable is the newly added clustered navigation technique.",what implementation ?,Trisolda,trisolda rdf visualizer.,False,True
"In an effort to optimize visualization and editing of OWL ontologies we have developed GrOWL: a browser and visual editor for OWL that accurately visualizes the underlying DL semantics of OWL ontologies while avoiding the difficulties of the verbose OWL syntax. In this paper, we discuss GrOWL visualization model and the essential visualization techniques implemented in GrOWL.",what implementation ?,GrOWL,growl :,True,True
"QAnswer is a question answering system that uses DBpedia as a knowledge base and converts natural language questions into a SPARQL query. In order to improve the match between entities and relations and natural language text, we make use of Wikipedia to extract lexicalizations of the DBpedia entities and then match them with the question. These entities are validated on the ontology, while missing ones can be inferred. The proposed system was tested in the QALD-5 challenge and it obtained a F1 score of 0.30, which placed QAnswer in the second position in the challenge, despite the fact that the system used only a small subset of the properties in DBpedia, due to the long extraction process.",what implementation ?,QAnswer,qanswer,True,True
"With the development of Semantic Web in recent years, an increasing amount of semantic data has been created in form of Resource Description Framework (RDF). Current visualization techniques help users quickly understand the underlying RDF data by displaying its structure in an overview. However, detailed information can only be accessed by further navigation. An alternative approach is to display the global context as well as the local details simultaneously in a unified view. This view supports the visualization and navigation on RDF data in an integrated way. In this demonstration, we present ZoomRDF, a framework that: i) adapts a space-optimized visualization algorithm for RDF, which allows more resources to be displayed, thus maximizes the utilization of display space, ii) combines the visualization with a fisheye zooming concept, which assigns more space to some individual nodes while still preserving the overview structure of the data, iii) considers both the importance of resources and the user interaction on them, which offers more display space to those elements the user may be interested in. We implement the framework based on the Gene Ontology and demonstrate that it facilitates tasks like RDF data exploration and editing.",what implementation ?,ZoomRDF,"zoomrdf,",True,True
"This paper consists of three parts: a preliminary typology of summaries in general; a description of the current and planned modules and performance of the SUMMARIST automated multilingual text summarization system being built sat ISI, and a discussion of three methods to evaluate summaries.",what implementation ?,SUMMARIST,summarist,True,True
"Constructing focused, context-based multi-document summaries requires an analysis of the context questions, as well as their corresponding document sets. We present a fuzzy cluster graph algorithm that finds entities and their connections between context and documents based on fuzzy coreference chains and describe the design and implementation of the ERSS summarizer implementing these ideas.",what implementation ?,ERSS summarizer,erss,False,False
"The multi-document summarizer using genetic algorithm-based sentence extraction (MSBGA) regards summarization process as an optimization problem where the optimal summary is chosen among a set of summaries formed by the conjunction of the original articles sentences. To solve the NP hard optimization problem, MSBGA adopts genetic algorithm, which can choose the optimal summary on global aspect. The evaluation function employs four features according to the criteria of a good summary: satisfied length, high coverage, high informativeness and low redundancy. To improve the accuracy of term frequency, MSBGA employs a novel method TFS, which takes word sense into account while calculating term frequency. The experiments on DUC04 data show that our strategy is effective and the ROUGE-1 score is only 0.55% lower than the best participant in DUC04",what implementation ?,MSBGA,msbga ),True,True
"In this paper we propose LODeX, a tool that produces a representative summary of a Linked open Data (LOD) source starting from scratch, thus supporting users in exploring and understanding the contents of a dataset. The tool takes in input the URL of a SPARQL endpoint and launches a set of predefined SPARQL queries, from the results of the queries it generates a visual summary of the source. The summary reports statistical and structural information of the LOD dataset and it can be browsed to focus on particular classes or to explore their properties and their use. LODeX was tested on the 137 public SPARQL endpoints contained in Data Hub (formerly CKAN), one of the main Open Data catalogues. The statistical and structural information extraction was successfully performed on 107 sources, among these the most significant ones are included in the online version of the tool.",what implementation ?,LODeX,"lodex,",True,True
"This paper presents a flexible method to enrich and populate an existing OWL ontology from XML data. Basic mapping rules are defined in order to specify the conversion rules on properties. Advanced mapping rules are defined on XML schemas a nd OWL XML schema elements in order to define rules for th e population process. In addition, this flexible method allows u sers to reuse rules for other conversions and populations.",what Input format ?,XML schema,owl,False,False
"One of the promises of the Semantic Web is to support applications that easily and seamlessly deal with heterogeneous data. Most data on the Web, however, is in the Extensible Markup Language (XML) format, but using XML requires applications to understand the format of each data source that they access. To achieve the benefits of the Semantic Web involves transforming XML into the Semantic Web language, OWL (Ontology Web Language), a process that generally has manual or only semi-automatic components. In this paper we present a set of patterns that enable the direct, automatic transformation from XML Schema into OWL allowing the integration of much XML data in the Semantic Web. We focus on an advanced logical representation of XML Schema components and present an implementation, including a comparison with related work.",what Input format ?,XML schema,extensible markup language ( xml ),False,False
"DTD and its instance have been considered the standard for data representation and information exchange format on the current web. However, when coming to the next generation of web, the Semantic Web, the drawbacks of XML and its schema are appeared. They mainly focus on the structure level and lack support for data representation. Meanwhile, some Semantic Web applications such as intelligent information services and semantic search engines require not only the syntactic format of the data, but also the semantic content. These requirements are supported by the Web Ontology Language (OWL), which is one of the recent W3C recommendation. But nowadays the amount of data presented in OWL is small in compare with XML data. Therefore, finding a way to utilize the available XML documents for the Semantic Web is a current challenge research. In this work we present an effective solution for transforming XML document into OWL domain knowledge. While keeping the original structure, our work also adds more semantics for the XML document. Moreover, whole of the transformation processes are done automatically without any outside intervention. Further, unlike previous approaches which focus on the schema level, we also extend our methodology for the data level by transforming specific XML instances into OWL individuals. The results in existing OWL syntaxes help them to be loaded immediately by the Semantic Web applications.",what Input format ?,DTD,dtd,True,True
"DTD and its instance have been considered the standard for data representation and information exchange format on the current web. However, when coming to the next generation of web, the Semantic Web, the drawbacks of XML and its schema are appeared. They mainly focus on the structure level and lack support for data representation. Meanwhile, some Semantic Web applications such as intelligent information services and semantic search engines require not only the syntactic format of the data, but also the semantic content. These requirements are supported by the Web Ontology Language (OWL), which is one of the recent W3C recommendation. But nowadays the amount of data presented in OWL is small in compare with XML data. Therefore, finding a way to utilize the available XML documents for the Semantic Web is a current challenge research. In this work we present an effective solution for transforming XML document into OWL domain knowledge. While keeping the original structure, our work also adds more semantics for the XML document. Moreover, whole of the transformation processes are done automatically without any outside intervention. Further, unlike previous approaches which focus on the schema level, we also extend our methodology for the data level by transforming specific XML instances into OWL individuals. The results in existing OWL syntaxes help them to be loaded immediately by the Semantic Web applications.",what Input format ?,XML instances,dtd,False,False
"With the increase in smart devices and abundance of video contents, efficient techniques for the indexing, analysis and retrieval of videos are becoming more and more desirable. Improved indexing and automated analysis of millions of videos could be accomplished by getting videos tagged automatically. A lot of existing methods fail to precisely tag videos because of their lack of ability to capture the video context. The context in a video represents the interactions of objects in a scene and their overall meaning. In this work, we propose a novel approach that integrates the video scene ontology with CNN (Convolutional Neural Network) for improved video tagging. Our method captures the content of a video by extracting the information from individual key frames. The key frames are then fed to a CNN based deep learning model to train its parameters. The trained parameters are used to generate the most frequent tags. Highly frequent tags are used to summarize the input video. The proposed technique is benchmarked on the most widely used dataset of video activities, namely, UCF-101. Our method managed to achieve an overall accuracy of 99.8% with an F1- score of 96.2%.",what Input format ?,Video,cnn,False,False
"Significant amounts of knowledge in science and technology have so far not been published as Linked Open Data but are contained in the text and tables of legacy PDF publications. Making such information available as RDF would, for example, provide direct access to claims and facilitate surveys of related work. A lot of valuable tabular information that till now only existed in PDF documents would also finally become machine understandable. Instead of studying scientific literature or engineering patents for months, it would be possible to collect such input by simple SPARQL queries. The SemAnn approach enables collaborative annotation of text and tables in PDF documents, a format that is still the common denominator of publishing, thus maximising the potential user base. The resulting annotations in RDF format are available for querying through a SPARQL endpoint. To incentivise users with an immediate benefit for making the effort of annotation, SemAnn recommends related papers, taking into account the hierarchical context of annotations in a novel way. We evaluated the usability of SemAnn and the usefulness of its recommendations by analysing annotations resulting from tasks assigned to test users and by interviewing them. While the evaluation shows that even few annotations lead to a good recall, we also observed unexpected, serendipitous recommendations, which confirms the merit of our low-threshold annotation support for the crowd.",what Output format ?,RDF,pdf,False,False
"In this work, we offer an approach to combine standard multimedia analysis techniques with knowledge drawn from conceptual metadata provided by domain experts of a specialized scholarly domain, to learn a domain-specific multimedia ontology from a set of annotated examples. A standard Bayesian network learning algorithm that learns structure and parameters of a Bayesian network is extended to include media observables in the learning. An expert group provides domain knowledge to construct a basic ontology of the domain as well as to annotate a set of training videos. These annotations help derive the associations between high-level semantic concepts of the domain and low-level MPEG-7 based features representing audio-visual content of the videos. We construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos. To encode this knowledge, we use MOWL, a multimedia extension of Web Ontology Language (OWL) which is capable of describing domain concepts in terms of their media properties and of capturing the inherent uncertainties involved. We use the ontology specified knowledge for recognizing concepts relevant to a video to annotate fresh addition to the video database with relevant concepts in the ontology. These conceptual annotations are used to create hyperlinks in the video collection, to provide an effective video browsing interface to the user.",what Output format ?,OWL,"mpeg - 7 based features representing audio - visual content of the videos. we construct a more robust and refined version of this ontology by learning from this set of conceptually annotated videos. to encode this knowledge, we use mowl,",False,True
"The aims of XML data conversion to ontologies are the indexing, integration and enrichment of existing ontologies with knowledge acquired from these sources. The contribution of this paper consists in providing a classification of the approaches used for the conversion of XML documents into OWL ontologies. This classification underlines the usage profile of each conversion method, providing a clear description of the advantages and drawbacks belonging to each method. Hence, this paper focuses on two main processes, which are ontology enrichment and ontology population using XML data. Ontology enrichment is related to the schema of the ontology (TBox), and ontology population is related to an individual (Abox). In addition, the ontologies described in these methods are based on formal languages of the Semantic Web such as OWL (Ontology Web Language) or RDF (Resource Description Framework). These languages are formal because the semantics are formally defined and take advantage of the Description Logics. In contrast, XML data sources are without formal semantics. The XML language is used to store, export and share data between processes able to process the specific data structure. However, even if the semantics is not explicitly expressed, data structure contains the universe of discourse by using a qualified vocabulary regarding a consensual agreement. In order to formalize this semantics, the OWL language provides rich logical constraints. Therefore, these logical constraints are evolved in the transformation of XML documents into OWL documents, allowing the enrichment and the population of the target ontology. To design such a transformation, the current research field establishes connections between OWL constructs (classes, predicates, simple or complex data types, etc.) and XML constructs (elements, attributes, element lists, etc.). Two different approaches for the transformation process are exposed. The instance approaches are based on XML documents without any schema associated. The validation approaches are based on the XML schema and document validated by the associated schema. The second approaches benefit from the schema definition to provide automated transformations with logic constraints. Both approaches are discussed in the text.",what Output format ?,OWL,,False,False
"As the Semantic Web initiative gains momentum, a fundamental problem of integrating existing data-intensive WWW applications into the Semantic Web emerges. In order for today’s relational database supported Web applications to transparently participate in the Semantic Web, their associated database schemas need to be converted into semantically equivalent ontologies. In this paper we present a solution to an important special case of the automatic mapping problem with wide applicability: mapping well-formed Entity-Relationship (ER) schemas to semantically equivalent OWL Lite ontologies. We present a set of mapping rules that fully capture the ER schema semantics, along with an overview of an implementation of the complete mapping algorithm integrated into the current SFSU ER Design Tools software.",what Output format ?,OWL,"www applications into the semantic web emerges. in order for today ’ s relational database supported web applications to transparently participate in the semantic web, their associated database schemas need to be converted into semantically equivalent ontologies. in this paper we present a solution to an important special case of the automatic mapping problem with wide applicability : mapping well - formed entity - relationship ( er )",False,False
"Name ambiguity has long been viewed as a challenging problem in many applications, such as scientific literature management, people search, and social network analysis. When we search a person name in these systems, many documents (e.g., papers, web pages) containing that person's name may be returned. It is hard to determine which documents are about the person we care about. Although much research has been conducted, the problem remains largely unsolved, especially with the rapid growth of the people information available on the Web. In this paper, we try to study this problem from a new perspective and propose an ADANA method for disambiguating person names via active user interactions. In ADANA, we first introduce a pairwise factor graph (PFG) model for person name disambiguation. The model is flexible and can be easily extended by incorporating various features. Based on the PFG model, we propose an active name disambiguation algorithm, aiming to improve the disambiguation performance by maximizing the utility of the user's correction. Experimental results on three different genres of data sets show that with only a few user corrections, the error rate of name disambiguation can be reduced to 3.1%. A real system has been developed based on the proposed method and is available online.",what dataset ?,Web page,"adana,",False,False
"Extraction of relevant features from high-dimensional multi-way functional MRI (fMRI) data is essential for the classification of a cognitive task. In general, fMRI records a combination of neural activation signals and several other noisy components. Alternatively, fMRI data is represented as a high dimensional array using a number of voxels, time instants, and snapshots. The organisation of fMRI data includes a number of Region Of Interests (ROI), snapshots, and thousand of voxels. The crucial step in cognitive task classification is a reduction of feature size through feature selection. Extraction of a specific pattern of interest within the noisy components is a challenging task. Tensor decomposition techniques have found several applications in the scientific fields. In this paper, a novel tensor gradient-based feature extraction technique for cognitive task classification is proposed. The technique has efficiently been applied on StarPlus fMRI data. Also, the technique has been used to discriminate the ROIs in fMRI data in terms of cognitive state classification. The method has been achieved a better average accuracy when compared to other existing feature extraction methods.",what dataset ?,StarPlus fMRI data,starplus fmri data.,True,True
"This paper presents a new challenging information extraction task in the domain of materials science. We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions. With this paper, we publish our annotation guidelines, as well as our SOFC-Exp corpus consisting of 45 open-access scholarly articles annotated by domain experts. A corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality. We also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set. On all tasks, using BERT embeddings leads to large performance gains, but with increasing task complexity, adding a recurrent neural network on top seems beneficial. Our models will serve as competitive baselines in future work, and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions.",what dataset ?,SOFC-Exp,sofc - exp,False,False
"Microorganisms are well adapted to their habitat but are partially sensitive to toxic metabolites or abiotic compounds secreted by other organisms or chemically formed under the respective environmental conditions. Thermoacidophiles are challenged by pyroglutamate, a lactam that is spontaneously formed by cyclization of glutamate under aerobic thermoacidophilic conditions. It is known that growth of the thermoacidophilic crenarchaeon Saccharolobus solfataricus (formerly Sulfolobus solfataricus) is completely inhibited by pyroglutamate. In the present study, we investigated the effect of pyroglutamate on the growth of S. solfataricus and the closely related crenarchaeon Sulfolobus acidocaldarius. In contrast to S. solfataricus, S. acidocaldarius was successfully cultivated with pyroglutamate as a sole carbon source. Bioinformatical analyses showed that both members of the Sulfolobaceae have at least one candidate for a 5-oxoprolinase, which catalyses the ATP-dependent conversion of pyroglutamate to glutamate. In S. solfataricus, we observed the intracellular accumulation of pyroglutamate and crude cell extract assays showed a less effective degradation of pyroglutamate. Apparently, S. acidocaldarius seems to be less versatile regarding carbohydrates and prefers peptidolytic growth compared to S. solfataricus. Concludingly, S. acidocaldarius exhibits a more efficient utilization of pyroglutamate and is not inhibited by this compound, making it a better candidate for applications with glutamate-containing media at high temperatures.",what has research problem ?,Utilization of pyroglutamate,,False,False
"Treatment of breast cancer underwent extensive progress in recent years with molecularly targeted therapies. However, non-specific pharmaceutical approaches (chemotherapy) persist, inducing severe side-effects. Phytochemicals provide a promising alternative for breast cancer prevention and treatment. Specifically, resveratrol (res) is a plant-derived polyphenolic phytoalexin with potent biological activity but displays poor water solubility, limiting its clinical use. Here we have developed a strategy for delivering res using a newly synthesized nano-carrier with the potential for both diagnosis and treatment. Methods: Res-loaded nanoparticles were synthesized by the emulsion method using Pluronic F127 block copolymer and Vitamin E-TPGS. Nanoparticle characterization was performed by SEM and tunable resistive pulse sensing. Encapsulation Efficiency (EE%) and Drug Loading (DL%) content were determined by analysis of the supernatant during synthesis. Nanoparticle uptake kinetics in breast cancer cell lines MCF-7 and MDA-MB-231 as well as in MCF-10A breast epithelial cells were evaluated by flow cytometry and the effects of res on cell viability via MTT assay. Results: Res-loaded nanoparticles with spherical shape and a dominant size of 179±22 nm were produced. Res was loaded with high EE of 73±0.9% and DL content of 6.2±0.1%. Flow cytometry revealed higher uptake efficiency in breast cancer cells compared to the control. An MTT assay showed that res-loaded nanoparticles reduced the viability of breast cancer cells with no effect on the control cells. Conclusions: These results demonstrate that the newly synthesized nanoparticle is a good model for the encapsulation of hydrophobic drugs. Additionally, the nanoparticle delivers a natural compound and is highly effective and selective against breast cancer cells rendering this type of nanoparticle an excellent candidate for diagnosis and therapy of difficult to treat mammary malignancies.",what has research problem ?,Breast cancer,breast cancer,True,True
"Purpose – To develop an analytical framework through which the organizational cultural dimension of enterprise resource planning (ERP) implementations can be analyzed.Design/methodology/approach – This paper is primarily based on a review of the literature.Findings – ERP is an enterprise system that offers, to a certain extent, standard business solutions. This standardization is reinforced by two processes: ERP systems are generally implemented by intermediary IT organizations, mediating between the development of ERP‐standard software packages and specific business domains of application; and ERP systems integrate complex networks of production divisions, suppliers and customers.Originality/value – In this paper, ERP itself is presented as problematic, laying heavy burdens on organizations – ERP is a demanding technology. While in some cases recognizing the mutual shaping of technology and organization, research into ERP mainly addresses the economic‐technological rationality of ERP (i.e. matters of eff...",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",what has research problem ?,Sentiment Analysis,"question answering,",False,False
A self-referencing dual fluorescing carbon dot-based nanothermometer can ratiometrically sense thermal events in HeLa cells with very high sensitivity.,what has research problem ?,Nanothermometer,,False,False
"This study is the first attempt that assembled published academic work on critical success factors (CSFs) in supply chain management (SCM) fields. The purpose of this study are to review the CSFs in SCM and to uncover the major CSFs that are apparent in SCM literatures. This study apply literature survey techniques from published CSFs studies in SCM. A collection of 42 CSFs studies in various SCM fields are obtained from major databases. The search uses keywords such as as supply chain management, critical success factors, logistics management and supply chain drivers and barriers. From the literature survey, four major CSFs are proposed. The factors are collaborative partnership, information technology, top management support and human resource. It is hoped that this review will serve as a platform for future research in SCM and CSFs studies. Plus, this study contribute to existing SCM knowledge and further appraise the concept of CSFs.",what has research problem ?,Supply chain management,supply chain management,True,True
"Unsupervised neural machine translation (NMT) is a recently proposed approach for machine translation which aims to train the model without using any labeled data. The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared-latent space, which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides, two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks.",what has research problem ?,Machine Translation,unsupervised neural machine translation,False,True
"This paper examines the impact of exchange rate volatility on trade flows in the U.K. over the period 1990–2000. According to the conventional approach, exchange rate volatility clamps down trade volumes. This paper, however, identifies the existence of a positive relationship between exchange rate volatility and imports in the U.K. in the 1990s by using a bivariate GARCH-in-mean model. It highlights a possible emergence of a polarized version with conventional proposition that ERV works as an impediment factor on trade flows.",what has research problem ?,Exchange rate volatility,exchange rate volatility,True,True
"Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.",what has research problem ?,Machine Translation,"language modeling by using deep neural networks. however, in practice, large scale neural language models have been shown to be prone to overfitting. in this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. the idea is to introduce adversarial noise to the output embedding layer while training the models. we show that the optimal adversarial noise yields a simple closed - form solution, thus allowing us to develop a simple and time efficient algorithm. theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. empirically, we show that our method improves on the single model state - of - the - art results for language modeling on penn treebank ( ptb ) and wikitext - 2, achieving test perplexity scores of 46. 01 and 38. 07, respectively. when applied to machine translation,",False,True
"ABSTRACT The research related to Enterprise Resource Planning (ERP) has grown over the past several years. This growing body of ERP research results in an increased need to review this extant literature with the intent of identifying gaps and thus motivate researchers to close this breach. Therefore, this research was intended to critique, synthesize and analyze both the content (e.g., topics, focus) and processes (i.e., methods) of the ERP literature, and then enumerates and discusses an agenda for future research efforts. To accomplish this, we analyzed 49 ERP articles published (1999-2004) in top Information Systems (IS) and Operations Management (OM) journals. We found an increasing level of activity during the 5-year period and a slightly biased distribution of ERP articles targeted at IS journals compared to OM. We also found several research methods either underrepresented or absent from the pool of ERP research. We identified several areas of need within the ERP literature, none more prevalent than the need to analyze ERP within the context of the supply chain. INTRODUCTION Davenport (1998) described the strengths and weaknesses of using Enterprise Resource Planning (ERP). He called attention to the growth of vendors like SAP, Baan, Oracle, and People-Soft, and defined this software as, ""...the seamless integration of all the information flowing through a companyfinancial and accounting information, human resource information, supply chain information, and customer information."" (Davenport, 1998). Since the time of that article, there has been a growing interest among researchers and practitioners in how organization implement and use ERP systems (Amoako-Gyampah and Salam, 2004; Bendoly and Jacobs, 2004; Gattiker and Goodhue, 2004; Lander, Purvis, McCray and Leigh, 2004; Luo and Strong, 2004; Somers and Nelson, 2004; Zoryk-Schalla, Fransoo and de Kok, 2004). This interest is a natural continuation of trends in Information Technology (IT), such as MRP II, (Olson, 2004; Teltumbde, 2000; Toh and Harding, 1999) and in business practice improvement research, such as continuous process improvement and business process reengineering (Markus and Tanis, 2000; Ng, Ip and Lee, 1999; Reijers, Limam and van der Aalst, 2003; Toh and Harding, 1999). This growing body of ERP research results in an increased need to review this extant literature with the intent of ""identifying critical knowledge gaps and thus motivate researchers to close this breach"" (Webster and Watson, 2002). Also, as noted by Scandura & Williams (2000), in order for research to advance, the methods used by researchers must periodically be evaluated to provide insights into the methods utilized and thus the areas of need. These two interrelated needs provide the motivation for this paper. In essence, this research critiques, synthesizes and analyzes both the content (e.g., topics, focus) and processes (i.e., methods) of the ERP literature and then enumerates and discusses an agenda for future research efforts. The remainder of the paper is organized as follows: Section 2 describes the approach to the analysis of the ERP research. Section 3 contains the results and a review of the literature. Section 4 discusses our findings and the needs relative to future ERP research efforts. Finally, section 5 summarizes the research. RESEARCH STUDY We captured the trends pertaining to (1) the number and distribution of ERP articles published in the leading journals, (2) methodologies employed in ERP research, and (3) emphasis relative to topic of ERP research. During the analysis of the ERP literature, we identified gaps and needs in the research and therefore enumerate and discuss a research agenda which allows the progression of research (Webster and Watson, 2002). In short, we sought to paint a representative landscape of the current ERP literature base in order to influence the direction of future research efforts relative to ERP. …",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"This paper studies the importance of identifying and categorizing scientific concepts as a way to achieve a deeper understanding of the research literature of a scientific community. To reach this goal, we propose an unsupervised bootstrapping algorithm for identifying and categorizing mentions of concepts. We then propose a new clustering algorithm that uses citations' context as a way to cluster the extracted mentions into coherent concepts. Our evaluation of the algorithms against gold standards shows significant improvement over state-of-the-art results. More importantly, we analyze the computational linguistic literature using the proposed algorithms and show four different ways to summarize and understand the research community which are difficult to obtain using existing techniques.",what has research problem ?,Identifying and categorizing mentions of concepts,clustering,False,False
"Smart cities offer services to their inhabitants which make everyday life easier beyond providing a feedback channel to the city administration. For instance, a live timetable service for public transportation or real-time traffic jam notification can increase the efficiency of travel planning substantially. Traditionally, the implementation of these smart city services require the deployment of some costly sensing and tracking infrastructure. As an alternative, the crowd of inhabitants can be involved in data collection via their mobile devices. This emerging paradigm is called mobile crowd-sensing or participatory sensing. In this paper, we present our generic framework built upon XMPP (Extensible Messaging and Presence Protocol) for mobile participatory sensing based smart city applications. After giving a short description of this framework we show three use-case smart city application scenarios, namely a live transit feed service, a soccer intelligence agency service and a smart campus application, which are currently under development on top of our framework.",what has research problem ?,Participatory Sensing,smart cities,False,False
Software testing is a crucial measure used to assure the quality of software. Path testing can detect bugs earlier because of it performs higher error coverage. This paper presents a model of generating test data based on an improved ant colony optimization and path coverage criteria. Experiments show that the algorithm has a better performance than other two algorithms and improve the efficiency of test data generation notably.,what has research problem ?,Ant Colony Optimization,ant colony optimization,True,True
"While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https://github.com/artetxem/monoses.",what has research problem ?,Machine Translation,neural machine translation,False,True
"Trace chemical detection is important for a wide range of practical applications. Recently emerged two-dimensional (2D) crystals offer unique advantages as potential sensing materials with high sensitivity, owing to their very high surface-to-bulk atom ratios and semiconducting properties. Here, we report the first use of Schottky-contacted chemical vapor deposition grown monolayer MoS2 as high-performance room temperature chemical sensors. The Schottky-contacted MoS2 transistors show current changes by 2-3 orders of magnitude upon exposure to very low concentrations of NO2 and NH3. Specifically, the MoS2 sensors show clear detection of NO2 and NH3 down to 20 ppb and 1 ppm, respectively. We attribute the observed high sensitivity to both well-known charger transfer mechanism and, more importantly, the Schottky barrier modulation upon analyte molecule adsorption, the latter of which is made possible by the Schottky contacts in the transistors and is not reported previously for MoS2 sensors. This study shows the potential of 2D semiconductors as high-performance sensors and also benefits the fundamental studies of interfacial phenomena and interactions between chemical species and monolayer 2D semiconductors.",what has research problem ?,Chemical sensors,chemical detection,False,False
"The growth of the Web in recent years has resulted in the development of various online platforms that provide healthcare information services. These platforms contain an enormous amount of information, which could be beneficial for a large number of people. However, navigating through such knowledgebases to answer specific queries of healthcare consumers is a challenging task. A majority of such queries might be non-factoid in nature, and hence, traditional keyword-based retrieval models do not work well for such cases. Furthermore, in many scenarios, it might be desirable to get a short answer that sufficiently answers the query, instead of a long document with only a small amount of useful information. In this paper, we propose a neural network model for ranking documents for question answering in the healthcare domain. The proposed model uses a deep attention mechanism at word, sentence, and document levels, for efficient retrieval for both factoid and non-factoid queries, on documents of varied lengths. Specifically, the word-level cross-attention allows the model to identify words that might be most relevant for a query, and the hierarchical attention at sentence and document levels allows it to do effective retrieval on both long and short documents. We also construct a new large-scale healthcare question-answering dataset, which we use to evaluate our model. Experimental evaluation results against several state-of-the-art baselines show that our model outperforms the existing retrieval techniques.",what has research problem ?,Question Answering ,question answering,True,True
"This paper provides a working definition of what the middle-income trap is. We start by defining four income groups of GDP per capita in 1990 PPP dollars: low-income below $2,000; lower-middle-income between $2,000 and $7,250; upper-middle-income between $7,250 and $11,750; and high-income above $11,750. We then classify 124 countries for which we have consistent data for 1950–2010. In 2010, there were 40 low-income countries in the world, 38 lower-middle-income, 14 upper-middle-income, and 32 high-income countries. Then we calculate the threshold number of years for a country to be in the middle-income trap: a country that becomes lower-middle-income (i.e., that reaches $2,000 per capita income) has to attain an average growth rate of per capita income of at least 4.7 percent per annum to avoid falling into the lower-middle-income trap (i.e., to reach $7,250, the upper-middle-income threshold); and a country that becomes upper-middle-income (i.e., that reaches $7,250 per capita income) has to attain an average growth rate of per capita income of at least 3.5 percent per annum to avoid falling into the upper-middle-income trap (i.e., to reach $11,750, the high-income level threshold). Avoiding the middle-income trap is, therefore, a question of how to grow fast enough so as to cross the lower-middle-income segment in at most 28 years, and the upper-middle-income segment in at most 14 years. Finally, the paper proposes and analyzes one possible reason why some countries get stuck in the middle-income trap: the role played by the changing structure of the economy (from low-productivity activities into high-productivity activities), the types of products exported (not all products have the same consequences for growth and development), and the diversification of the economy. We compare the exports of countries in the middle-income trap with those of countries that graduated from it, across eight dimensions that capture different aspects of a country’s capabilities to undergo structural transformation, and test whether they are different. Results indicate that, in general, they are different. We also compare Korea, Malaysia, and the Philippines according to the number of products that each exports with revealed comparative advantage. We find that while Korea was able to gain comparative advantage in a significant number of sophisticated products and was well connected, Malaysia and the Philippines were able to gain comparative advantage in electronics only.",what has research problem ?,Middle-Income Trap,middle - income trap,False,False
"Cities form the heart of a dynamic society. In an open space-economy cities have to mobilize all of their resources to remain attractive and competitive. Smart cities depend on creative and knowledge resources to maximize their innovation potential. This study offers a comparative analysis of nine European smart cities on the basis of an extensive database covering two time periods. After conducting a principal component analysis, a new approach, based on a self-organizing map analysis, is adopted to position the various cities under consideration according to their selected “smartness” performance indicators.",what has research problem ?,Smart cities,smart cities,True,True
"Debates about the future of urban development in many Western countries have been increasingly influenced by discussions of smart cities. Yet despite numerous examples of this ‘urban labelling’ phenomenon, we know surprisingly little about so‐called smart cities, particularly in terms of what the label ideologically reveals as well as hides. Due to its lack of definitional precision, not to mention an underlying self‐congratulatory tendency, the main thrust of this article is to provide a preliminary critical polemic against some of the more rhetorical aspects of smart cities. The primary focus is on the labelling process adopted by some designated smart cities, with a view to problematizing a range of elements that supposedly characterize this new urban form, as well as question some of the underlying assumptions/contradictions hidden within the concept. To aid this critique, the article explores to what extent labelled smart cities can be understood as a high‐tech variation of the ‘entrepreneurial city’, as well as speculates on some general principles which would make them more progressive and inclusive.",what has research problem ?,Smart cities,smart cities.,True,True
"A consumer-dependent (business-to-consumer) organization tends to present itself as possessing a set of human qualities, which is termed the brand personality of the company. The perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs, or magazines, produced by the organization. A consistent brand will generate trust and retain customers over time as they develop an affinity toward regularity and common patterns. However, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content that needs to be authored and pushed to the Internet to maintain an edge in the era of digital marketing. To understand the depth of the problem, we collect around 300K web page content from around 650 companies. We develop trait-specific classification models by considering the linguistic features of the content. The classifier automatically identifies the web articles that are not consistent with the mission and vision of a company and further helps us to discover the conditions under which the consistency cannot be maintained. To address the brand inconsistency issue, we then develop a sentence ranking system that outputs the top three sentences that need to be changed for making a web article more consistent with the company’s brand personality.",what has research problem ?,Sentence Ranking,"brand personality of the company. the perception is impressed upon the consumer through the content, be it in the form of advertisement, blogs, or magazines, produced by the organization. a consistent brand will generate trust and retain customers over time as they develop an affinity toward regularity and common patterns. however, maintaining a consistent messaging tone for a brand has become more challenging with the virtual explosion in the amount of content that needs to be authored and pushed to the internet to maintain an edge in the era of digital marketing.",False,False
"Instruments play an essential role in creating research data. Given the importance of instruments and associated metadata to the assessment of data quality and data reuse, globally unique, persistent and resolvable identification of instruments is crucial. The Research Data Alliance Working Group Persistent Identification of Instruments (PIDINST) developed a community-driven solution for persistent identification of instruments which we present and discuss in this paper. Based on an analysis of 10 use cases, PIDINST developed a metadata schema and prototyped schema implementation with DataCite and ePIC as representative persistent identifier infrastructures and with HZB (Helmholtz-Zentrum Berlin fur Materialien und Energie) and BODC (British Oceanographic Data Centre) as representative institutional instrument providers. These implementations demonstrate the viability of the proposed solution in practice. Moving forward, PIDINST will further catalyse adoption and consolidate the schema by addressing new stakeholder requirements.",what has research problem ?,Persistent Identification,persistent identification,True,True
"Goal-oriented dialogue in complex domains is an extremely challenging problem and there are relatively few datasets. This task provided two new resources that presented different challenges: one was focused but small, while the other was large but diverse. We also considered several new variations on the next utterance selection problem: (1) increasing the number of candidates, (2) including paraphrases, and (3) not including a correct option in the candidate set. Twenty teams participated, developing a range of neural network models, including some that successfully incorporated external data to boost performance. Both datasets have been publicly released, enabling future work to build on these results, working towards robust goal-oriented dialogue systems.",what has research problem ?,Goal-Oriented Dialogue Systems,goal - oriented dialogue,False,False
"General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris’ distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task’s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",what has research problem ?,build task agnostic relation representations solely from entity-linked text,information extraction.,False,False
"Image-based food calorie estimation is crucial to diverse mobile applications for recording everyday meal. However, some of them need human help for calorie estimation, and even if it is automatic, food categories are often limited or images from multiple viewpoints are required. Then, it is not yet achieved to estimate food calorie with practical accuracy and estimating food calories from a food photo is an unsolved problem. Therefore, in this paper, we propose estimating food calorie from a food photo by simultaneous learning of food calories, categories, ingredients and cooking directions using deep learning. Since there exists a strong correlation between food calories and food categories, ingredients and cooking directions information in general, we expect that simultaneous training of them brings performance boosting compared to independent single training. To this end, we use a multi-task CNN [1]. In addition, in this research, we construct two kinds of datasets that is a dataset of calorie-annotated recipe collected from Japanese recipe sites on the Web and a dataset collected from an American recipe site. In this experiment, we trained multi-task and single-task CNNs. As a result, the multi-task CNN achieved the better performance on both food category estimation and food calorie estimation than single-task CNNs. For the Japanese recipe dataset, by introducing a multi-task CNN, 0.039 were improved on the correlation coefficient, while for the American recipe dataset, 0.090 were raised compared to the result by the single-task CNN.",what has research problem ?,Food calorie estimation,food calorie estimation,True,True
"The Environmental Kuznets Curve (EKC) hypothesises that emissions first increase at low stages of development then decrease once a certain threshold has been reached. The EKC concept is usually used with per capita Gross Domestic Product as the explanatory variable. As others, we find mixed evidence, at best, of such a pattern for CO2 emissions with respect to per capita GDP. We also show that the share of manufacture in GDP and governance/institutions play a significant role in the CO2 emissions–income relationship. As GDP presents shortcomings in representing income, development in a broad perspective or human well-being, it is then replaced by the World Bank's Adjusted Net Savings (ANS, also known as Genuine Savings). Using the ANS as an explanatory variable, we show that the EKC is generally empirically supported for CO2 emissions. We also show that human capital and natural capital are the main drivers of the downward sloping part of the EKC.",what has research problem ?,CO2 emissions,co2 emissions,True,True
"Abstract Background The goal of the first BioCreAtIvE challenge (Critical Assessment of Information Extraction in Biology) was to provide a set of common evaluation tasks to assess the state of the art for text mining applied to biological problems. The results were presented in a workshop held in Granada, Spain March 28–31, 2004. The articles collected in this BMC Bioinformatics supplement entitled ""A critical assessment of text mining methods in molecular biology"" describe the BioCreAtIvE tasks, systems, results and their independent evaluation. Results BioCreAtIvE focused on two tasks. The first dealt with extraction of gene or protein names from text, and their mapping into standardized gene identifiers for three model organism databases (fly, mouse, yeast). The second task addressed issues of functional annotation, requiring systems to identify specific text passages that supported Gene Ontology annotations for specific proteins, given full text articles. Conclusion The first BioCreAtIvE assessment achieved a high level of international participation (27 groups from 10 countries). The assessment provided state-of-the-art performance results for a basic task (gene name finding and normalization), where the best systems achieved a balanced 80% precision / recall or better, which potentially makes them suitable for real applications in biology. The results for the advanced task (functional annotation from free text) were significantly lower, demonstrating the current limitations of text-mining approaches where knowledge extrapolation and interpretation are required. In addition, an important contribution of BioCreAtIvE has been the creation and release of training and test data sets for both tasks. There are 22 articles in this special issue, including six that provide analyses of results or data quality for the data sets, including a novel inter-annotator consistency assessment for the test set used in task 2.",what has research problem ?,gene name finding and normalization,critical assessment of information extraction in biology ),False,False
"Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches. Furthermore, our framework has the benefits of extensibility and transferability. We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods. As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3% increase in F1 score relative to previous SoTA. Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy.",what has research problem ?,Entity Linking,entity linking,True,True
"Wikidata is becoming an increasingly important knowledge base whose usage is spreading in the research community. However, most question answering systems evaluation datasets rely on Freebase or DBpedia. We present two new datasets in order to train and benchmark QA systems over Wikidata. The first is a translation of the popular SimpleQuestions dataset to Wikidata, the second is a dataset created by collecting user feedbacks.",what has research problem ?,Question Answering ,question answering,True,True
"We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.",what has research problem ?,Atari Games,atari games.,True,True
"Background: The surge in patients during the COVID-19 pandemic has exacerbated the looming problem of staﬀ shortage in German ICUs possibly leading to worse outcomes for patients. Methods: Within the German Evidence Ecosystem CEOsys network, we conducted an online national mixed-methods survey assessing the standard of care in German ICUs treating patients with COVID-19. Results: A total of 171 German ICUs reported a median ideal number of patients per intensivist of 8 (interquartile range, IQR = 3rd quartile - 1st quartile = 4.0) and per nurse of 2.0 (IQR = 1.0). For COVID-19 patients, the median target was a maximum of 6.0 (IQR = 2.0) patients per intensivist or 2.0 (IQR = 0.0) patients per nurse. Targets for intensivists were rarely met by 15.2% and never met by 3.5% of responding institutions. Targets for nursing staﬃng could rarely be met in 32.2% and never in 5.3% of responding institutions.Conclusions: Shortages of staﬃng in the critical care setting are eminent during the COVID-19 pandemic and might not only negatively aﬀect patient outcomes, but also staﬀ wellbeing and healthcare costs. A joint eﬀort that scrutinizes the demands and structures of our health care system seems fundamental to be prepared for the future.",what has research problem ?,COVID-19,covid - 19 pandemic,False,False
"In recent years, the development of recommender systems has attracted increased interest in several domains, especially in e-learning. Massive Open Online Courses have brought a revolution. However, deficiency in support and personalization in this context drive learners to lose their motivation and leave the learning process. To overcome this problem we focus on adapting learning activities to learners' needs using a recommender system.This paper attempts to provide an introduction to different recommender systems for e-learning settings, as well as to present our proposed recommender system for massive learning activities in order to provide learners with the suitable learning activities to follow the learning process and maintain their motivation. We propose a hybrid knowledge-based recommender system based on ontology for recommendation of e-learning activities to learners in the context of MOOCs. In the proposed recommendation approach, ontology is used to model and represent the knowledge about the domain model, learners and learning activities.",what has research problem ?,Recommender Systems,e - learning.,False,False
"BioNLP Open Shared Tasks (BioNLP-OST) is an international competition organized to facilitate development and sharing of computational tasks of biomedical text mining and solutions to them. For BioNLP-OST 2019, we introduced a new mental health informatics task called “RDoC Task”, which is composed of two subtasks: information retrieval and sentence extraction through National Institutes of Mental Health’s Research Domain Criteria framework. Five and four teams around the world participated in the two tasks, respectively. According to the performance on the two tasks, we observe that there is room for improvement for text mining on brain research and mental illness.",what has research problem ?,Mental Health Informatics,information retrieval and sentence extraction,False,False
"A new electron‐rich central building block, 5,5,12,12‐tetrakis(4‐hexylphenyl)‐indacenobis‐(dithieno[3,2‐b:2′,3′‐d]pyrrol) (INP), and two derivative nonfullerene acceptors (INPIC and INPIC‐4F) are designed and synthesized. The two molecules reveal broad (600–900 nm) and strong absorption due to the satisfactory electron‐donating ability of INP. Compared with its counterpart INPIC, fluorinated nonfullerene acceptor INPIC‐4F exhibits a stronger near‐infrared absorption with a narrower optical bandgap of 1.39 eV, an improved crystallinity with higher electron mobility, and down‐shifted highest occupied molecular orbital and lowest unoccupied molecular orbital energy levels. Organic solar cells (OSCs) based on INPIC‐4F exhibit a high power conversion efficiency (PCE) of 13.13% and a relatively low energy loss of 0.54 eV, which is among the highest efficiencies reported for binary OSCs in the literature. The results demonstrate the great potential of the new INP as an electron‐donating building block for constructing high‐performance nonfullerene acceptors for OSCs.",what has research problem ?,Organic solar cells,organic solar cells,True,True
"The fluorescent N-doped carbon dots (N-CDs) obtained from C3N4 emit strong blue fluorescence, which is stable with different ionic strengths and time. The fluorescence intensity of N-CDs decreases with the temperature increasing, while it can recover to the initial one with the temperature decreasing. It is an accurate linear response of fluorescence intensity to temperature, which may be attributed to the synergistic effect of abundant oxygen-containing functional groups and hydrogen bonds. Further experiments also demonstrate that N-CDs can serve as effective in vitro and in vivo fluorescence-based nanothermometer.",what has research problem ?,Nanothermometer,"fluorescence,",False,False
"Facial expression recognition is an active research field which accommodates the need of interaction between humans and machines in a broad field of subjects. This work investigates the performance of a multi-scale and multi-orientation Gabor Filter Bank constructed in such a way to avoid redundant information. A region based approach is employed using different neighbourhood size at the locations of 34 fiducial points. Furthermore, a reduced set of 19 fiducial points is used to model the face geometry. The use of Principal Component Analysis (PCA) is evaluated. The proposed methodology is evaluated for the classification of the 6 basic emotions proposed by Ekman considering neutral expression as the seventh emotion.",what has research problem ?,Facial Expression Recognition,facial expression recognition,True,True
"Cross-lingual document classification aims at training a document classifier on resources in one language and transferring it to a different language without any additional resources. Several approaches have been proposed in the literature and the current best practice is to evaluate them on a subset of the Reuters Corpus Volume 2. However, this subset covers only few languages (English, German, French and Spanish) and almost all published works focus on the the transfer between English and German. In addition, we have observed that the class prior distributions differ significantly between the languages. We argue that this complicates the evaluation of the multilinguality. In this paper, we propose a new subset of the Reuters corpus with balanced class priors for eight languages. By adding Italian, Russian, Japanese and Chinese, we cover languages which are very different with respect to syntax, morphology, etc. We provide strong baselines for all language transfer directions using multilingual word and sentence embeddings respectively. Our goal is to offer a freely available framework to evaluate cross-lingual document classification, and we hope to foster by these means, research in this important area.",what has research problem ?,Cross-Lingual Document Classification,cross - lingual document classification,False,False
"A concept guided by the ISO 37120 standard for city services and quality of life is suggested as unified framework for smart city dashboards. The slow (annual, quarterly, or monthly) ISO 37120 indicators are enhanced and complemented with more detailed and person-centric indicators that can further accelerate the transition toward smart cities. The architecture supports three tasks: acquire and manage data from heterogeneous sensors; process data originated from heterogeneous sources (sensors, OpenData, social data, blogs, news, and so on); and implement such collection and processing on the cloud. A prototype application based on the proposed architecture concept is developed for the city of Skopje, Macedonia. This article is part of a special issue on smart cities.",what has research problem ?,City dashboards,smart cities.,False,False
"A series of halogenated conjugated molecules, containing F, Cl, Br and I, were easily prepared via Knoevenagel condensation and applied in field-effect transistors and organic solar cells. Halogenated conjugated materials were found to possess deep frontier energy levels and high crystallinity compared to their non-halogenated analogues, which is due to the strong electronegativity and heavy atom effect of halogens. As a result, halogenated semiconductors provide high electron mobilities up to 1.3 cm2 V−1 s−1 in transistors and high efficiencies over 9% in non-fullerene solar cells.",what has research problem ?,Organic solar cells,organic solar cells.,True,True
"The intraspecific chemical variability of essential oils (50 samples) isolated from the aerial parts of Artemisia herba‐alba Asso growing wild in the arid zone of Southeastern Tunisia was investigated. Analysis by GC (RI) and GC/MS allowed the identification of 54 essential oil components. The main compounds were β‐thujone and α‐thujone, followed by 1,8‐cineole, camphor, chrysanthenone, trans‐sabinyl acetate, trans‐pinocarveol, and borneol. Chemometric analysis (k‐means clustering and PCA) led to the partitioning into three groups. The composition of two thirds of the samples was dominated by α‐thujone or β‐thujone. Therefore, it could be expected that wild plants of A. herba‐alba randomly harvested in the area of Kirchaou and transplanted by local farmers for the cultivation in arid zones of Southern Tunisia produce an essential oil belonging to the α‐thujone/β‐thujone chemotype and containing also 1,8‐cineole, camphor, and trans‐sabinyl acetate at appreciable amounts.",what has research problem ?,Oil,oil,True,True
"Maritime situation awareness is supported by a combination of satellite, airborne, and terrestrial sensor systems. This paper presents several solutions to process that sensor data into information that supports operator decisions. Examples are vessel detection algorithms based on multispectral image techniques in combination with background subtraction, feature extraction techniques that estimate the vessel length to support vessel classification, and data fusion techniques to combine image based information, detections from coastal radar, and reports from cooperative systems such as (satellite) AIS. Other processing solutions include persistent tracking techniques that go beyond kinematic tracking, and include environmental information from navigation charts, and if available, ELINT reports. And finally rule-based and statistical solutions for the behavioural analysis of anomalous vessels. With that, trends and future work will be presented.",what has research problem ?,Vessel detection,vessel detection,True,True
"Fault-based testing is often advocated to overcome limitations ofother testing approaches; however it is also recognized as beingexpensive. On the other hand, evolutionary algorithms have beenproved suitable for reducing the cost of data generation in the contextof coverage based testing. In this paper, we propose a newevolutionary approach based on ant colony optimization for automatictest input data generation in the context of mutation testingto reduce the cost of such a test strategy. In our approach the antcolony optimization algorithm is enhanced by a probability densityestimation technique. We compare our proposal with otherevolutionary algorithms, e.g., Genetic Algorithm. Our preliminaryresults on JAVA testbeds show that our approach performed significantlybetter than other alternatives.",what has research problem ?,Ant Colony Optimization,ant colony optimization,True,True
"In this paper, we present the virtual knowledge graph (VKG) paradigm for data integration and access, also known in the literature as Ontology-based Data Access. Instead of structuring the integration layer as a collection of relational tables, the VKG paradigm replaces the rigid structure of tables with the flexibility of graphs that are kept virtual and embed domain knowledge. We explain the main notions of this paradigm, its tooling ecosystem and significant use cases in a wide range of applications. Finally, we discuss future research directions.",what has research problem ?,virtual knowledge graph (VKG) paradigm for data integration,"data integration and access, also known in the literature as ontology - based data access.",False,False
"While the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions, keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult. The community could greatly benefit from an automatic system able to summarize scientific results, e.g., in the form of a leaderboard. In this paper we build two datasets and develop a framework (TDMS-IE) aimed at automatically extracting task, dataset, metric and score from NLP papers, towards the automatic construction of leaderboards. Experiments show that our model outperforms several baselines by a large margin. Our model is a first step towards automatic leaderboard construction, e.g., in the NLP domain.",what has research problem ?,Automatic leaderboard construction,automatic construction of leaderboards.,False,False
"<jats:p> Open educational resources are currently becoming increasingly available from a multitude of sources and are consequently annotated in many diverse ways. Interoperability concerns that naturally arise can often be resolved through the semantification of metadata descriptions, while at the same time strengthening the knowledge value of resources. SKOS can be a solid linking point offering a standard vocabulary for thematic descriptions, by referencing semantic thesauri. We propose the enhancement and maintenance of educational resources’ metadata in the form of learning object ontologies and introduce the notion of a learning object ontology repository that can help towards their publication, discovery and reuse. At the same time, linking to thesauri datasets and contextualized sources interrelates learning objects with linked data and exposes them to the Web of Data. We build a set of extensions and workflows on top of contemporary ontology management tools, such as WebProtégé, that can make it suitable as a learning object ontology repository. The proposed approach and implementation can help libraries and universities in discovering, managing and incorporating open educational resources and enhancing current curricula. </jats:p>",what has research problem ?,educational resources,learning object ontology,False,False
"Multi-factory production networks have increased in recent years. With the factories located in different geographic areas, companies can benefit from various advantages, such as closeness to their customers, and can respond faster to market changes. Products (jobs) in the network can usually be produced in more than one factory. However, each factory has its operations efficiency, capacity, and utilization level. Allocation of jobs inappropriately in a factory will produce high cost, long lead time, overloading or idling resources, etc. This makes distributed scheduling more complicated than classical production scheduling problems because it has to determine how to allocate the jobs into suitable factories, and simultaneously determine the production scheduling in each factory as well. The problem is even more complicated when alternative production routing is allowed in the factories. This paper proposed a genetic algorithm with dominant genes to deal with distributed scheduling problems, especially in a flexible manufacturing system (FMS) environment. The idea of dominant genes is to identify and record the critical genes in the chromosome and to enhance the performance of genetic search. To testify and benchmark the optimization reliability, the proposed algorithm has been compared with other approaches on several distributed scheduling problems. These comparisons demonstrate the importance of distributed scheduling and indicate the optimization reliability of the proposed algorithm.",what has research problem ?,Scheduling problems,distributed scheduling,False,False
"Purpose – The purpose of this paper is to identify, assess and explore potential risks that Chinese companies may encounter when using, maintaining and enhancing their enterprise resource planning (ERP) systems in the post‐implementation phase.Design/methodology/approach – The study adopts a deductive research design based on a cross‐sectional questionnaire survey. This survey is preceded by a political, economic, social and technological analysis and a set of strength, weakness, opportunity and threat analyses, from which the researchers refine the research context and select state‐owned enterprises (SOEs) in the electronic and telecommunications industry in Guangdong province as target companies to carry out the research. The questionnaire design is based on a theoretical risk ontology drawn from a critical literature review process. The questionnaire is sent to 118 selected Chinese SOEs, from which 42 (84 questionnaires) valid and usable responses are received and analysed.Findings – The findings ident...",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"In a commonly-used version of the Simple Assembly Line Balancing Problem (SALBP-1) tasks are assigned to stations along an assembly line with a fixed cycle time in order to minimize the required number of stations. It has traditionally been assumed that the total work needed for each product unit has been partitioned into economically indivisible tasks. However, in practice, it is sometimes possible to divide particular tasks in limited ways at additional time penalty cost. Despite the penalties, task division where possible, now and then leads to a reduction in the minimum number of stations. Deciding which allowable tasks to divide creates a new assembly line balancing problem, TDALBP (Task Division Assembly Line Balancing Problem). We propose a mathematical model of the TDALBP, an exact solution procedure for it and present promising computational results for the adaptation of some classical SALBP instances from the research literature. The results demonstrate that the TDALBP sometimes has the potential to significantly improve assembly line performance.",what has research problem ?,Task division assembly line balancing problem,,False,False
"In this paper we describe the SemEval-2010 Cross-Lingual Lexical Substitution task, where given an English target word in context, participating systems had to find an alternative substitute word or phrase in Spanish. The task is based on the English Lexical Substitution task run at SemEval-2007. In this paper we provide background and motivation for the task, we describe the data annotation process and the scoring system, and present the results of the participating systems.",what has research problem ?,Cross-Lingual Lexical Substitution,"cross - lingual lexical substitution task,",False,False
"Errors are prevalent in time series data, which is particularly common in the industrial field. Data with errors could not be stored in the database, which results in the loss of data assets. At present, to deal with these time series containing errors, besides keeping original erroneous data, discarding erroneous data and manually checking erroneous data, we can also use the cleaning algorithm widely used in the database to automatically clean the time series data. This survey provides a classification of time series data cleaning techniques and comprehensively reviews the state-of-the-art methods of each type. Besides we summarize data cleaning tools, systems and evaluation criteria from research and industry. Finally, we highlight possible directions time series data cleaning.",what has research problem ?,Time Series Data Cleaning,"time series data,",False,False
"We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components.",what has research problem ?,Relation Extraction,"relation extraction via entity pretraining and scheduled sampling. our model improves over the state - of - the - art feature - based model on end - to - end relation extraction,",False,True
"Extraction of relevant features from high-dimensional multi-way functional MRI (fMRI) data is essential for the classification of a cognitive task. In general, fMRI records a combination of neural activation signals and several other noisy components. Alternatively, fMRI data is represented as a high dimensional array using a number of voxels, time instants, and snapshots. The organisation of fMRI data includes a number of Region Of Interests (ROI), snapshots, and thousand of voxels. The crucial step in cognitive task classification is a reduction of feature size through feature selection. Extraction of a specific pattern of interest within the noisy components is a challenging task. Tensor decomposition techniques have found several applications in the scientific fields. In this paper, a novel tensor gradient-based feature extraction technique for cognitive task classification is proposed. The technique has efficiently been applied on StarPlus fMRI data. Also, the technique has been used to discriminate the ROIs in fMRI data in terms of cognitive state classification. The method has been achieved a better average accuracy when compared to other existing feature extraction methods.",what has research problem ?,Cognitive state classification,cognitive state classification.,True,True
"Knowledge graph embedding is an important task and it will benefit lots of downstream applications. Currently, deep neural networks based methods achieve state-of-the-art performance. However, most of these existing methods are very complex and need much time for training and inference. To address this issue, we propose a simple but effective atrous convolution based knowledge graph embedding method. Compared with existing state-of-the-art methods, our method has following main characteristics. First, it effectively increases feature interactions by using atrous convolutions. Second, to address the original information forgotten issue and vanishing/exploding gradient issue, it uses the residual learning method. Third, it has simpler structure but much higher parameter efficiency. We evaluate our method on six benchmark datasets with different evaluation metrics. Extensive experiments show that our model is very effective. On these diverse datasets, it achieves better results than the compared state-of-the-art methods on most of evaluation metrics. The source codes of our model could be found at https://github.com/neukg/AcrE.",what has research problem ?,Knowledge Graph Embedding,knowledge graph embedding,True,True
"This paper discusses smart cities and raises critical questions about the faith being placed in technology to reduce carbon dioxide emissions. Given increasingly challenging carbon reduction targets, the role of information and communication technology and the digital economy are increasingly championed as offering potential to contribute to meeting these targets within cities and buildings. This paper questions the faith being placed in smart or intelligent solutions through asking, what role then for the ordinary citizen? The smart approach often appears to have a narrow view of how technology and user-engagement can sit together, viewing the behaviour of users as a hurdle to overcome rather than a resource to be utilised. This paper suggests lessons can be learnt from other disciplines and wider sustainable development policy that champions the role of citizens and user-engagement to harness the co-creation of knowledge, collaboration and empowerment. Specifically, empirical findings and observations a...",what has research problem ?,Smart cities,carbon dioxide emissions.,False,False
"We present a novel 3‐step self‐training method for author name disambiguation—SAND (self‐training associative name disambiguator)—which requires no manual labeling, no parameterization (in real‐world scenarios) and is particularly suitable for the common situation in which only the most basic information about a citation record is available (i.e., author names, and work and venue titles). During the first step, real‐world heuristics on coauthors are able to produce highly pure (although fragmented) clusters. The most representative of these clusters are then selected to serve as training data for the third supervised author assignment step. The third step exploits a state‐of‐the‐art transductive disambiguation method capable of detecting unseen authors not included in any training example and incorporating reliable predictions to the training data. Experiments conducted with standard public collections, using the minimum set of attributes present in a citation, demonstrate that our proposed method outperforms all representative unsupervised author grouping disambiguation methods and is very competitive with fully supervised author assignment methods. Thus, different from other bootstrapping methods that explore privileged, hard to obtain information such as self‐citations and personal information, our proposed method produces topnotch performance with no (manual) training data or parameterization and in the presence of scarce information.",what has research problem ?,Author name disambiguation,,False,False
"The research question addressed in this article concerns whether unemployment persistency can be regarded as a phenomenon that increases employment difficulties for the less educated and, if so, whether their employment chances are reduced by an overly rapid reduction in the number of jobs with low educational requirements. The empirical case is Sweden and the data covers the period 1976-2000. The empirical analyses point towards a negative response to both questions. First, it is shown that jobs with low educational requirements have declined but still constitute a substantial share of all jobs. Secondly, educational attainment has changed at a faster rate than the job structure with increasing over-education in jobs with low educational requirements as a result. This, together with changed selection patterns into the low education group, are the main reasons for the poor employment chances of the less educated in periods with low general demand for labour.",what has research problem ?,Over-education,unemployment persistency,False,False
"Companies declare that quality or customer satisfaction is their top priority in order to keep and attract more business in an increasingly competitive marketplace. The cost of quality (COQ) is a tool which can help determine the optimal level of quality investment. COQ analysis enables organizations to identify measure and control the consequences of poor quality. This study attempts to identify the COQ elements across the enterprise resource planning (ERP) implementation phases for the ERP implementation services of consultancy companies. The findings provide guidance to project managers on how best to utilize their limited resources. In summary, we suggest that project teams should focus on “value-added” activities and minimize the cost of “non-value-added” activities at each phase of the ERP implementation project. Key words: Services, ERP implementation services, quality standard, service quality standard, cost of quality, project management, project quality management, project financial management.",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"paper attempts to explore and identify issues affecting Enterprise Resource Planning (ERP) implementation in context to Indian Small and Medium Enterprises (SMEs) and large enterprises. Issues which are considered more important for large scale enterprises may not be of equal importance for a small and medium scale enterprise and hence replicating the implementation experience which holds for large organizations will not a wise approach on the part of the implementation vendors targeting small scale enterprises. This paper attempts to highlight those specific issues where a different approach needs to be adopted. Pareto analysis has been applied to identify the issues for Indian SMEs and Large scale enterprises as available from the published literature. Also by doing comparative analysis between the identified issues for Indian large enterprises and SMEs four issues are proved to be crucial for SMEs in India but not for large enterprises such as proper system implementation strategy, clearly defined scope of implementation procedure, proper project planning and minimal customization of the system selected for implementation, because of some limitations faced by the Indian SMEs compared to large enterprises.",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"There are three issues that are crucial to advancing our academic understanding of smart cities: (1) contextual conditions, (2) governance models, and (3) the assessment of public value. A brief review of recent literature and the analysis of the included papers provide support for the assumption that cities cannot simply copy good practices but must develop approaches that fit their own situation ( contingency) and concord with their own organization in terms of broader strategies, human resource policies, information policies, and so on ( configuration). A variety of insights into the mechanisms and building blocks of smart city practices are presented, and issues for further research are identified.",what has research problem ?,Smart cities,smart cities :,True,True
"Abstract. We describe here the development and evaluation of an Earth system model suitable for centennial-scale climate prediction. The principal new components added to the physical climate model are the terrestrial and ocean ecosystems and gas-phase tropospheric chemistry, along with their coupled interactions. The individual Earth system components are described briefly and the relevant interactions between the components are explained. Because the multiple interactions could lead to unstable feedbacks, we go through a careful process of model spin up to ensure that all components are stable and the interactions balanced. This spun-up configuration is evaluated against observed data for the Earth system components and is generally found to perform very satisfactorily. The reason for the evaluation phase is that the model is to be used for the core climate simulations carried out by the Met Office Hadley Centre for the Coupled Model Intercomparison Project (CMIP5), so it is essential that addition of the extra complexity does not detract substantially from its climate performance. Localised changes in some specific meteorological variables can be identified, but the impacts on the overall simulation of present day climate are slight. This model is proving valuable both for climate predictions, and for investigating the strengths of biogeochemical feedbacks.",what has research problem ?,CMIP5,climate prediction.,False,False
"We propose a static relation extraction task to complement biomedical information extraction approaches. We argue that static relations such as part-whole are implicitly involved in many common extraction settings, define a task setting making them explicit, and discuss their integration into previously proposed tasks and extraction methods. We further identify a specific static relation extraction task motivated by the BioNLP'09 shared task on event extraction, introduce an annotated corpus for the task, and demonstrate the feasibility of the task by experiments showing that the defined relations can be reliably extracted. The task setting and corpus can serve to support several forms of domain information extraction.",what has research problem ?,Relation Extraction,static relation extraction,False,True
"Abstract Dual functional fluorescence nanosensors have many potential applications in biology and medicine. Monitoring temperature with higher precision at localized small length scales or in a nanocavity is a necessity in various applications. As well as the detection of biologically interesting metal ions using low-cost and sensitive approach is of great importance in bioanalysis. In this paper, we describe the preparation of dual-function highly fluorescent B, N-co-doped carbon nanodots (CDs) that work as chemical and thermal sensors. The CDs emit blue fluorescence peaked at 450 nm and exhibit up to 70% photoluminescence quantum yield with showing excitation-independent fluorescence. We also show that water-soluble CDs display temperature-dependent fluorescence and can serve as highly sensitive and reliable nanothermometers with a thermo-sensitivity 1.8% °C −1 , and wide range thermo-sensing between 0–90 °C with excellent recovery. Moreover, the fluorescence emission of CDs are selectively quenched after the addition of Fe 2+ and Fe 3+ ions while show no quenching with adding other common metal cations and anions. The fluorescence emission shows a good linear correlation with concentration of Fe 2+ and Fe 3+ (R 2 = 0.9908 for Fe 2+ and R 2 = 0.9892 for Fe 3+ ) with a detection limit of of 80.0 ± 0.5 nM for Fe 2+ and 110.0 ± 0.5 nM for Fe 3+ . Considering the high quantum yield and selectivity, CDs are exploited to design a nanoprobe towards iron detection in a biological sample. The fluorimetric assay is used to detect Fe 2+ in iron capsules and total iron in serum samples successfully.",what has research problem ?,Nanothermometer,bioanalysis.,False,False
"Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.",what has research problem ?,Atari Games,atari games.,True,True
"The purpose of this paper is to shed the light on the critical success factors that lead to high supply chain performance outcomes in a Malaysian manufacturing company. The critical success factors consist of relationship with customer and supplier, information communication and technology (ICT), material flow management, corporate culture and performance measurement. Questionnaire was the main instrument for the study and it was distributed to 84 staff from departments of purchasing, planning, logistics and operation. Data analysis was conducted by employing descriptive analysis (mean and standard deviation), reliability analysis, Pearson correlation analysis and multiple regression. The findings show that there are relationships exist between relationship with customer and supplier, ICT, material flow management, performance measurement and supply chain management (SCM) performance, but not for corporate culture. Forming a good customer and supplier relationship is the main predictor of SCM performance, followed by performance measurement, material flow management and ICT. It is recommended that future study to determine additional success factors that are pertinent to firms’ current SCM strategies and directions, competitive advantages and missions. Logic suggests that further study to include more geographical data coverage, other nature of businesses and research instruments. Key words: Supply chain management, critical success factor.",what has research problem ?,Supply chain management,supply chain performance,False,False
"Abstract One determining characteristic of contemporary sociopolitical systems is their power over increasingly large and diverse populations. This raises questions about power relations between heterogeneous individuals and increasingly dominant and homogenizing system objectives. This article crosses epistemic boundaries by integrating computer engineering and a historicalphilosophical approach making the general organization of individuals within large-scale systems and corresponding individual homogenization intelligible. From a versatile archeological-genealogical perspective, an analysis of computer and social architectures is conducted that reinterprets Foucault’s disciplines and political anatomy to establish the notion of politics for a purely technical system. This permits an understanding of system organization as modern technology with application to technical and social systems alike. Connecting to Heidegger’s notions of the enframing ( Gestell ) and a more primal truth ( anfänglicheren Wahrheit) , the recognition of politics in differently developing systems then challenges the immutability of contemporary organization. Following this critique of modernity and within the conceptualization of system organization, Derrida’s democracy to come (à venir) is then reformulated more abstractly as organizations to come . Through the integration of the discussed concepts, the framework of Large-Scale Systems Composed of Homogeneous Individuals (LSSCHI) is proposed, problematizing the relationships between individuals, structure, activity, and power within large-scale systems. The LSSCHI framework highlights the conflict of homogenizing system-level objectives and individual heterogeneity, and outlines power relations and mechanisms of control shared across different social and technical systems.",what has research problem ?,Heterogeneity,system organization,False,False
"Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.",what has research problem ?,Representation Learning,"classification and recommendation,",False,False
"In today's fierce business competition, companies face the tremendous challenge of expanding markets, improving their products, services and processes and exploiting their intellectual capital in a dynamic network of knowledge-intensive relations inside and outside their borders. In order to accomplish these objectives, more and more companies are turning to the Enterprise Resource Planning systems (ERP). On the other hand, Knowledge Management (KM) has received considerable attention in the last decade and is continuously gaining interest by industry, enterprises and academia. As we are moving into an era of “knowledge capitalism”, knowledge management will play a fundamental role in the success of today's businesses. This paper aims at throwing light on the role of KM in the ERP success first and on their possible integration second. A wide range of academic and practitioner literature related to KM and ERP is reviewed. On the basis of this review, the paper gives answers to specific research questions and analyses future research directions.",what has research problem ?,Enterprise resource planning,"enterprise resource planning systems ( erp ). on the other hand, knowledge management",False,True
"Artemisia herba-alba Asso., Asteraceae, is widely used in Morrocan folk medicine for the treatment of different health disorders. However, no scientific or medical studies were carried out to assess the cytotoxicity of A. herba-alba essential oil against cancer cell lines. In this study, eighteen volatile compounds were identified by GC-MS analysis of the essential oil obtained from the plant's aerial parts. The main volatile constituent in A. herba-alba was found to be a monoterpene, Verbenol, contributing to about 22% of the total volatile components. The essential oil showed significant antiproliferative activity against the acute lymphoblastic leukaemia (CEM) cell line, with 3 µg/mL as IC50 value. The anticancer bioactivity of Moroccan A. herba-alba essential oil is described here for the first time.",what has research problem ?,Oil,"oil against cancer cell lines. in this study, eighteen volatile compounds were identified by gc - ms analysis of the essential oil obtained from the plant's aerial parts. the main volatile constituent in a. herba - alba was found to be a monoterpene, verbenol, contributing to about 22 % of the total volatile components. the essential oil showed significant antiproliferative activity against the acute lymphoblastic leukaemia",False,True
"Abstract The composition of the essential oil hydrodistilled from the aerial parts of Artemisia herba-alba Asso. growing in Jordan was determined by GC and GC/MS. The oil yield was 1.3% (v/w) from dried tops (leaves, stems and fowers). Forty components corresponding to 95.3% of the oil were identifed, of which oxygenated monoterpenes were the main oil fraction (39.3% of the oil), with α- and β-thujones as the principal components (24.7%). The other major identifed components were: santolina alcohol (13.0%), artemisia ketone (12.4%), trans-sabinyl acetate (5.4%), germacrene D (4.6%), α-eudesmol (4.2%) and caryophyllene acetate (5.7%). The high oil yield and the substantial levels of potentially active components, in particular thujones and santolina alcohol, in the oil of this Jordanian species make the plant and the oil thereof promising candidates as natural herbal constituents of antimicrobial drug combinations.",what has research problem ?,Oil,oil,True,True
"Abstract A retrospective study of 30 patients hospitalized with a diagnosis of uncomplicated retrobulbar neuritis was carried out. The follow‐up period was 2–11 years; 57% developed multiple sclerosis. When the initial examination revealed oligoclonal bands in the cerebrospinal fluid, the risk of developing multiple sclerosis increased to 79%. With normal cerebrospinal fluid the risk decreased to only 10%. In the majority of cases, the diagnosis of MS was made during the first 3 years after retrobulbar neuritis.",what has research problem ?,Multiple sclerosis,retrobulbar neuritis,False,False
"We present the design, preparation, results and analysis of the Cancer Genetics (CG) event extraction task, a main task of the BioNLP Shared Task (ST) 2013. The CG task is an information extraction task targeting the recognition of events in text, represented as structured n-ary associations of given physical entities. In addition to addressing the cancer domain, the CG task is differentiated from previous event extraction tasks in the BioNLP ST series in addressing a wide range of pathological processes and multiple levels of biological organization, ranging from the molecular through the cellular and organ levels up to whole organisms. Final test set submissions were accepted from six teams. The highest-performing system achieved an Fscore of 55.4%. This level of performance is broadly comparable with the state of the art for established molecular-level extraction tasks, demonstrating that event extraction resources and methods generalize well to higher levels of biological organization and are applicable to the analysis of scientific texts on cancer. The CG task continues as an open challenge to all interested parties, with tools and resources available from http://2013. bionlp-st.org/.",what has research problem ?,Cancer genetics (CG) event extraction,cancer genetics,False,False
"Abstract Seedlings of Artemisia herba-alba Asso collected from Kirchaou area were transplanted in an experimental garden near the Institut des Régions Arides of Médenine (Tunisia). During three years, the aerials parts were harvested (three levels of cutting, 25%, 50% and 75% of the plant), at full blossom and during the vegetative stage. The essential oil was isolated by hydrodistillation and its chemical composition was determined by GC(RI) and 13C-NMR. With respect to the quantity of vegetable material and the yield of hydrodistillation, it appears that the best results were obtained for plants cut at 50% of their height and during the full blossom. The chemical composition of the essential oil was dominated by β-thujone, α-thujone, 1,8-cineole, camphor and trans-sabinyl acetate, irrespective of the level of cutting and the period of harvest. It remains similar to that of plants growing wild in the same area.",what has research problem ?,Oil,oil,True,True
"Abstract Since its inception in 2007, DBpedia has been constantly releasing open data in RDF, extracted from various Wikimedia projects using a complex software system called the DBpedia Information Extraction Framework (DIEF). For the past 12 years, the software received a plethora of extensions by the community, which positively affected the size and data quality. Due to the increase in size and complexity, the release process was facing huge delays (from 12 to 17 months cycle), thus impacting the agility of the development. In this paper, we describe the new DBpedia release cycle including our innovative release workflow, which allows development teams (in particular those who publish large, open data) to implement agile, cost-efficient processes and scale up productivity. The DBpedia release workflow has been re-engineered, its new primary focus is on productivity and agility , to address the challenges of size and complexity. At the same time, quality is assured by implementing a comprehensive testing methodology. We run an experimental evaluation and argue that the implemented measures increase agility and allow for cost-effective quality-control and debugging and thus achieve a higher level of maintainability. As a result, DBpedia now publishes regular (i.e. monthly) releases with over 21 billion triples with minimal publishing effort .",what has research problem ?,DBPedia,dbpedia information extraction framework,False,True
"Abstract Despite rapid progress, most of the educational technologies today lack a strong instructional design knowledge basis leading to questionable quality of instruction. In addition, a major challenge is to customize these educational technologies for a wide range of customizable instructional designs. Ontologies are one of the pertinent mechanisms to represent instructional design in the literature. However, existing approaches do not support modeling of flexible instructional designs. To address this problem, in this paper, we propose an ontology based framework for systematic modeling of different aspects of instructional design knowledge based on domain patterns. As part of the framework, we present ontologies for modeling goals , instructional processes and instructional material . We demonstrate the ontology framework by presenting instances of the ontology for the large scale case study of adult literacy in India (287 million learners spread across 22 Indian Languages), which requires creation of hundreds of similar but varied e Learning Systems based on flexible instructional designs. The implemented framework is available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://rice.iiit.ac.in"">http://rice.iiit.ac.in</jats:ext-link> and is transferred to National Literacy Mission Authority of Government of India . The proposed framework could be potentially used for modeling instructional design knowledge for school education, vocational skills and beyond.",what has research problem ?,goals,adult literacy,False,False
"The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a “shallow” dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.",what has research problem ?,Environmental Sound Classification,environmental sound classification.,True,True
"Abstract. Vessel monitoring and surveillance is important for maritime safety and security, environment protection and border control. Ship monitoring systems based on Synthetic-aperture Radar (SAR) satellite images are operational. On SAR images the ships made of metal with sharp edges appear as bright dots and edges, therefore they can be well distinguished from the water. Since the radar is independent from the sun light and can acquire images also by cloudy weather and rain, it provides a reliable service. Vessel detection from spaceborne optical images (VDSOI) can extend the SAR based systems by providing more frequent revisit times and overcoming some drawbacks of the SAR images (e.g. lower spatial resolution, difficult human interpretation). Optical satellite images (OSI) can have a higher spatial resolution thus enabling the detection of smaller vessels and enhancing the vessel type classification. The human interpretation of an optical image is also easier than as of SAR image. In this paper I present a rapid automatic vessel detection method which uses pattern recognition methods, originally developed in the computer vision field. In the first step I train a binary classifier from image samples of vessels and background. The classifier uses simple features which can be calculated very fast. For the detection the classifier is slided along the image in various directions and scales. The detector has a cascade structure which rejects most of the background in the early stages which leads to faster execution. The detections are grouped together to avoid multiple detections. Finally the position, size(i.e. length and width) and heading of the vessels is extracted from the contours of the vessel. The presented method is parallelized, thus it runs fast (in minutes for 16000 × 16000 pixels image) on a multicore computer, enabling near real-time applications, e.g. one hour from image acquisition to end user.",what has research problem ?,Vessel detection,vessel monitoring,False,False
"We introduce SpERT, an attention model for span-based joint entity and relation extraction. Our key contribution is a light-weight reasoning on BERT embeddings, which features entity recognition and filtering, as well as relation classification with a localized, marker-free context representation. The model is trained using strong within-sentence negative samples, which are efficiently extracted in a single BERT pass. These aspects facilitate a search over all spans in the sentence. In ablation studies, we demonstrate the benefits of pre-training, strong negative sampling and localized context. Our model outperforms prior work by up to 2.6% F1 score on several datasets for joint entity and relation extraction.",what has research problem ?,Relation Extraction,relation extraction.,True,True
"We present the findings and results of theSecond Nuanced Arabic Dialect IdentificationShared Task (NADI 2021). This Shared Taskincludes four subtasks: country-level ModernStandard Arabic (MSA) identification (Subtask1.1), country-level dialect identification (Subtask1.2), province-level MSA identification (Subtask2.1), and province-level sub-dialect identifica-tion (Subtask 2.2). The shared task dataset cov-ers a total of 100 provinces from 21 Arab coun-tries, collected from the Twitter domain. A totalof 53 teams from 23 countries registered to par-ticipate in the tasks, thus reflecting the interestof the community in this area. We received 16submissions for Subtask 1.1 from five teams, 27submissions for Subtask 1.2 from eight teams,12 submissions for Subtask 2.1 from four teams,and 13 Submissions for subtask 2.2 from fourteams.",what has research problem ?,Arabic Dialect Identification,country - level dialect identification,False,False
"Abstract Background The task of recognizing and identifying species names in biomedical literature has recently been regarded as critical for a number of applications in text and data mining, including gene name recognition, species-specific document retrieval, and semantic enrichment of biomedical articles. Results In this paper we describe an open-source species name recognition and normalization software system, LINNAEUS, and evaluate its performance relative to several automatically generated biomedical corpora, as well as a novel corpus of full-text documents manually annotated for species mentions. LINNAEUS uses a dictionary-based approach (implemented as an efficient deterministic finite-state automaton) to identify species names and a set of heuristics to resolve ambiguous mentions. When compared against our manually annotated corpus, LINNAEUS performs with 94% recall and 97% precision at the mention level, and 98% recall and 90% precision at the document level. Our system successfully solves the problem of disambiguating uncertain species mentions, with 97% of all mentions in PubMed Central full-text documents resolved to unambiguous NCBI taxonomy identifiers. Conclusions LINNAEUS is an open source, stand-alone software system capable of recognizing and normalizing species name mentions with speed and accuracy, and can therefore be integrated into a range of bioinformatics and text-mining applications. The software and manually annotated corpus can be downloaded freely at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""http://linnaeus.sourceforge.net/"" ext-link-type=""uri"">http://linnaeus.sourceforge.net/</jats:ext-link>.",what has research problem ?,Species name recognition and normalization,"gene name recognition, species - specific document retrieval, and semantic enrichment of biomedical articles. results in this paper we describe an open - source species name recognition",False,False
"Many NLP applications require information about locations of objects referenced in text, or relations between them in space. For example, the phrase a book on the desk contains information about the location of the object book, as trajector, with respect to another object desk, as landmark. Spatial Role Labeling (SpRL) is an evaluation task in the information extraction domain which sets a goal to automatically process text and identify objects of spatial scenes and relations between them. This paper describes the task in Semantic Evaluations 2013, annotation schema, corpora, participants, methods and results obtained by the participants.",what has research problem ?,Spatial Role Labeling,,False,False
"As analysts still grapple with understanding core damage accident progression at Three Mile Island and Fukushima that caught the nuclear industry off-guard once too many times, one notices the very limited detail with which the large reactor cores of these subject reactors have been modelled in their severe accident simulation code packages. At the same time, modelling of CANDU severe accidents have largely borrowed from and suffered from the limitations of the same LWR codes (see IAEA TECDOC 1727) whose applications to PHWRs have poorly caught critical PHWR design specifics and vulnerabilities. As a result, accident management measures that have been instituted at CANDU PHWRs, while meeting the important industry objective of publically seeming to be doing something about lessons learnt from say Fukushima and showing that the reactor designs are oh so close to perfect and the off-site consequences of severe accidents happily benign. Integrated PHWR severe accident progression and consequence assessment code ROSHNI can make a significant contribution to actual, practical understanding of severe accident progression in CANDU PHWRs, improving significantly on the other PHWR specific computer codes developed three decades ago when modeling decisions were constrained by limited computing power and poor understanding of and interest in severe core damage accidents. These codes force gross simplifications in reactor core modelling and do not adequately represent all the right CANDU core details, materials, fluids, vessels or phenomena. But they produce results that are familiar and palatable. They do, however to their credit, also excel in their computational speed, largely because they model and compute so little and with such un-necessary simplifications. ROSHNI sheds most previous modelling simplifications and represents each of the 380 channels, 4560 bundle, 37 elements in four concentric ring, Zircaloy clad fuel geometry, materials and fluids more faithfully in a 2000 MW(Th) CANDU6 reactor. It can be used easily for other PHWRs with different number of fuel channels and bundles per each channel. Each of horizontal PHWR reactor channels with all their bundles, fuel rings, sheaths, appendages, end fittings and feeders are modelled and in detail that reflects large across core differences. While other codes model at best a few hundred core fuel entities, thermo-chemical transient behaviour of about 73,000 different fuel channel entities within the core is considered by ROSHNI simultaneously along with other 15,000 or so other flow path segments. At each location all known thermo-chemical and hydraulic phenomena are computed. With such detail, ROSHNI is able to provide information on their progressive and parallel thermo-chemical contribution to accident progression and a more realistic fission product release source term that would belie the miniscule one (100 TBq of Cs-137 or 0.15% of core inventory) used by EMOs now in Canada on recommendation of our national regulator CNSC. ROSHNI has an advanced, more CANDU specific consideration of each bundle transitioning to a solid debris behaviour in the Calandria vessel without reverting to a simplified molten corium formulation that happily ignores interaction of debris with vessel welds, further vessel failures and energetic interactions. The code is able to follow behaviour of each fuel bundle following its disassembly from the fuel channel and thus demonstrate that the gross assumption of a core collapse made in some analyses is wrong and misleading. It is able to thus demonstrate that PHWR core disassembly is not only gradual, it will be also be incomplete with a large number of low power, peripheral fuel channels never disassembling under most credible scenarios. The code is designed to grow into and use its voluminous results in a severe accident simulator for operator training. It’s phenomenological models are able to examine design inadequacies / issues that affect accident progression and several simple to implement design improvements that have a profound effect on results. For example, an early pressure boundary failure due to inadequacy of heat sinks in a station blackout scenario can be examined along with the effect of improved and adequate over pressure protection. A best effort code such as ROSHNI can be instrumental in identifying the risk reduction benefits of undertaking certain design, operational and accidental management improvements for PHWRs, with some of the multi-unit ones handicapped by poor pressurizer placement and leaky containments with vulnerable materials, poor overpressure protection, ad-hoc mitigation measures and limited instrumentation common to all CANDUs. Case in point is the PSA supported design and installed number of Hydrogen recombiners that are neither for the right gas (designed mysteriously for H2 instead of D2) or its potential release quantity (they are sparse and will cause explosions). The paper presents ROSHNI results of simulations of a postulated station blackout scenario and sheds a light on the challenges ahead in minimizing risk from operation of these otherwise unique power reactors.",what has research problem ?,Fission product release,core damage,False,False
"Migrating existing enterprise software to cloud platforms involves the comparison of competing cloud deployment options (CDOs). A CDO comprises a combination of a specific cloud environment, deployment architecture, and runtime reconfiguration rules for dynamic resource scaling. Our simulator CDOSim can evaluate CDOs, e.g., regarding response times and costs. However, the design space to be searched for well-suited solutions is extremely huge. In this paper, we approach this optimization problem with the novel genetic algorithm CDOXplorer. It uses techniques of the search-based software engineering field and CDOSim to assess the fitness of CDOs. An experimental evaluation that employs, among others, the cloud environments Amazon EC2 and Microsoft Windows Azure, shows that CDOXplorer can find solutions that surpass those of other state-of-the-art techniques by up to 60%. Our experiment code and data and an implementation of CDOXplorer are available as open source software.",what has research problem ?,Search-Based Software Engineering,cdoxplorer.,False,False
"Building predictive models for information extraction from text, such as named entity recognition or the extraction of semantic relationships between named entities in text, requires a large corpus of annotated text. Wikipedia is often used as a corpus for these tasks where the annotation is a named entity linked by a hyperlink to its article. However, editors on Wikipedia are only expected to link these mentions in order to help the reader to understand the content, but are discouraged from adding links that do not add any benefit for understanding an article. Therefore, many mentions of popular entities (such as countries or popular events in history), or previously linked articles, as well as the article’s entity itself, are not linked. In this paper, we discuss WEXEA, a Wikipedia EXhaustive Entity Annotation system, to create a text corpus based on Wikipedia with exhaustive annotations of entity mentions, i.e. linking all mentions of entities to their corresponding articles. This results in a huge potential for additional annotations that can be used for downstream NLP tasks, such as Relation Extraction. We show that our annotations are useful for creating distantly supervised datasets for this task. Furthermore, we publish all code necessary to derive a corpus from a raw Wikipedia dump, so that it can be reproduced by everyone.",what has research problem ?,Named Entity Recognition,named entity recognition,True,True
"1. IntroductionEconomy is the main determinant of smart city proposals, and a city with a significant level of economic competitiveness (Popescu, 2015a, b, c, d, e) has one of the features of a smart city. The economic consequences of the smart city proposals are business production, job generation, personnel development, and enhancement in the productivity. The enforcement of an ICT infrastructure is essential to a smart city's advancement and is contingent on several elements associated with its attainability and operation. (Chourabi et al., 2012) Smart city involvements are the end results of, and uncomfortably incorporated into, present social and spatial configurations of urban governa nce (Br a tu, 2015) a nd the built setting: the s ma rt city is put together gradually, integrated awkwardly into current arrangements of city administration and the reinforced environment. Smart cities are intrinsically distinguished, being geographically asymmetrical at a diversity of scales. Not all places of the city will be similarly smart: smart cities will favor some spaces, individuals, and undertakings over others. An essential component of the smart city is its capacity to further economic growth. (Shelton et al., 2015)2. The Assemblage of Participants, Tenets and Technologies Related to Smart City InterventionsThe ""smart city"" notion has arisen from long-persisting opinions regarding urban technological idealistic schemes (Lazaroiu, 2013) and the absolutely competitive city. Smart cities are where novel technologies may be produced and the receptacles for technology, i.e. the goal of its utilizations. The contest to join this movement and become a smart city has stimulated city policymakers to endogenize the performance of technology-led growth (Lazaroiu, 2014a, b, c), leading municipal budgets toward financings that present smart city standing. The boundaries of the smart city are generated both by the lack of data utilizations that can handle shared and not separate solutions and by the incapacity to aim at indefinit e features of cities that both enhance and blemish from the standard of urban existence for city inhabitants. Smart city technology fina ncings are chiefly composed of a meliorations inst ea d of genuine innovations, on the cit izen consumer side. (Glasmeier and Christopherson, 2015) The notion of smart city as a method to improve the life standard of individuals has been achieving rising relevance in the calendars of policymakers. The amount of ""smart"" proposals initiated by a municipality can lead to an intermediate final product that indicates the endeavors made to augment the quality of existence of the citizens. The probabilities of a city raising its degree of smartness are contingent on several country-specific variables that outweigh its economic, technological and green advancement rate. Public administrations dema nd backing to organize the notion of the smartness of a city (Nica, 2015a, b, c, d), to encapsulate its ramifications, to establish standards at the global level, and to observe enhancement chances. (Neir otti et a l., 2014) T he gr owth of smart cit ies is assisting the rise of government employment of ITCs to enhance political involvement, enforce public schemes or supply public spher e ser vices. Ther e is no one wa y to becoming smart, and diverse cities ha ve embraced distinct adva nces that indicate their specific circumstances. The administration of smart cities is dependent on elaborate arrangements of interdependent entities. (Rodriguez Bolivar, 2015)The association of smart (technology-enabled) solutions to satisfy the leading societal difficult tasks and the concentration on the city as the chief determinant of alteration bring about the notion of the ""smart city."" The rise of novel technologies to assess and interlink various facets of ordinary exis- tence (""the internet of things"") is relevant in the progression towards a smart city. The latter is attempt ing to encourage a nd adjust innovations to the demands of their citizens (Pera, 2015a, b) by urging synergetic advancement of inventions with various stakeholders. …",what has research problem ?,Smart cities,introductioneconomy,False,False
"Today when many practitioners run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs. Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF). Though expressive and accurate, these models fail to fully exploit GPU parallelism, limiting their computational efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER: Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are more accurate than Bi-LSTM-CRFs while attaining 8x faster test time speeds.",what has research problem ?,NER,neural networks,False,False
"A novel low-computation discriminative feature space is introduced for facial expression recognition capable of robust performance over a rang of image resolutions. Our approach is based on the simple local binary patterns (LBP) for representing salient micro-patterns of face images. Compared to Gabor wavelets, the LBP features can be extracted faster in a single scan through the raw image and lie in a lower dimensional space, whilst still retaining facial information efficiently. Template matching with weighted Chi square statistic and support vector machine are adopted to classify facial expressions. Extensive experiments on the Cohn-Kanade Database illustrate that the LBP features are effective and efficient for facial expression discrimination. Additionally, experiments on face images with different resolutions show that the LBP features are robust to low-resolution images, which is critical in real-world applications where only low-resolution video input is available.",what has research problem ?,Facial Expression Recognition,facial expression recognition,True,True
"Abstract Semantic embedding of knowledge graphs has been widely studied and used for prediction and statistical analysis tasks across various domains such as Natural Language Processing and the Semantic Web. However, less attention has been paid to developing robust methods for embedding OWL (Web Ontology Language) ontologies, which contain richer semantic information than plain knowledge graphs, and have been widely adopted in domains such as bioinformatics. In this paper, we propose a random walk and word embedding based ontology embedding method named , which encodes the semantics of an OWL ontology by taking into account its graph structure, lexical information and logical constructors. Our empirical evaluation with three real world datasets suggests that benefits from these three different aspects of an ontology in class membership prediction and class subsumption prediction tasks. Furthermore, often significantly outperforms the state-of-the-art methods in our experiments.",what has research problem ?,Ontology Embedding,natural language processing,False,False
"Today game engines are popular in commercial game development, as they lower the threshold of game production by providing common technologies and convenient content-creation tools. Game engine based development is therefore the mainstream methodology in the game industry. Model-Driven Game Development (MDGD) is an emerging game development methodology, which applies the Model-Driven Software Development (MDSD) method in the game development domain. This simplifies game development by reducing the gap between game design and implementation. MDGD has to take advantage of the existing game engines in order to be useful in commercial game development practice. However, none of the existing MDGD approaches in literature has convincingly demonstrated good integration of its tools with the game engine tool-chain. In this paper, we propose a hybrid approach named ECGM to address the integration challenges of two methodologies with a focus on the technical aspects. The approach makes a run-time engine the base of the domain framework, and uses the game engine tool-chain together with the MDGD tool-chain. ECGM minimizes the change to the existing workflow and technology, thus reducing the cost and risk of adopting MDGD in commercial game development. Our contribution is one important step towards MDGD industrialization.",what has research problem ?,Model-driven Game Development,model - driven game development,False,False
"Recent studies have reported that organizations are often unable to identify the key success factors of Sustainable Supply Chain Management (SSCM) and to understand their implications for management practice. For this reason, the implementation of SSCM often does not result in noticeable benefits. So far, research has failed to offer any explanations for this discrepancy. In view of this fact, our study aims at identifying and analyzing the factors that underlie successful SSCM. Success factors are identified by means of a systematic literature review and are then integrated into an explanatory model. Consequently, the proposed success factor model is tested on the basis of an empirical study focusing on recycling networks of the electrics and electronics industry. We found that signaling, information provision and the adoption of standards are crucial preconditions for strategy commitment, mutual learning, the establishment of ecological cycles and hence for the overall success of SSCM. Copyright © 2011 John Wiley & Sons, Ltd and ERP Environment.",what has research problem ?,Supply chain management,sustainable supply chain management,False,True
"Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.",what has research problem ?,Document Classification,"document classification,",True,True
"Abstract Smart cities are a modern administrative/ developmental concept that tries to combine the development of urban areas with a higher level of citizens’ participation. However, there is a lack of understanding of the concept’s potential, due possibly to an unwillingness to accept a new form of relationship with the citizens. In this article, the willingness to introduce the elements of smart cities into two Central and Eastern European cities is tested. The results show that people are reluctant to use technology above the level of their needs and show little interest in participating in matters of governance, which prevents smart cities from developing in reality.",what has research problem ?,Smart cities,smart cities,True,True
"Purpose – The purpose of this paper is to further build up the knowledge about reasons for small and mid‐sized enterprises (SMEs) to adopt open source enterprise resource planning (ERP) systems.Design/methodology/approach – The paper presents and analyses findings in articles about proprietary ERPs and open source ERPs. In addition, a limited investigation of the distribution channel SourceForge for open source is made.Findings – The cost perspective seems to receive a high attention regarding adoption of open source ERPs. This can be questioned and the main conclusion is that costs seem to have a secondary role in adoption or non adoption of open source ERPs.Research limitations/implications – The paper is mainly a conceptual paper written from a literature review. The ambition is to search support for the findings by doing more research in the area.Practical implications – The findings presented are of interest both for developers of proprietary ERPs as well as SMEs since it is shown that there are defi...",what has research problem ?,Enterprise resource planning,open source enterprise resource planning,False,True
"Open Educational Resources (OER) are a direct reaction to knowledge privatization; they foment their exchange to the entire world with the aim of increase the human intellectual capacity.In this document, we describe the committment of Universidad Técnica Particular de Loja (UTPL), Ecuador, in the promotion of open educational practices and resources and their impact in society and knowledge economy through the use of Social Software.",what has research problem ?,Open Education,knowledge privatization ;,False,False
"The paper is concerned with two-class active learning. While the common approach for collecting data in active learning is to select samples close to the classification boundary, better performance can be achieved by taking into account the prior data distribution. The main contribution of the paper is a formal framework that incorporates clustering into active learning. The algorithm first constructs a classifier on the set of the cluster representatives, and then propagates the classification decision to the other samples via a local noise model. The proposed model allows to select the most representative samples as well as to avoid repeatedly labeling samples in the same cluster. During the active learning process, the clustering is adjusted using the coarse-to-fine strategy in order to balance between the advantage of large clusters and the accuracy of the data representation. The results of experiments in image databases show a better performance of our algorithm compared to the current methods.",what has research problem ?,Active learning,active learning.,True,True
"We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers.As a result, we propose LeViT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT.",what has research problem ?,Image Classification,image classification,True,True
"Links build the backbone of the Linked Data Cloud. With the steady growth in size of datasets comes an increased need for end users to know which frameworks to use for deriving links between datasets. In this survey, we comparatively evaluate current Link Discovery tools and frameworks. For this purpose, we outline general requirements and derive a generic architecture of Link Discovery frameworks. Based on this generic architecture, we study and compare the features of state-ofthe-art linking frameworks. We also analyze reported performance evaluations for the different frameworks. Finally, we derive insights pertaining to possible future developments in the domain of Link Discovery.",what has research problem ?,Link Discovery,"link discovery tools and frameworks. for this purpose, we outline general requirements and derive a generic architecture of link discovery frameworks. based on this generic architecture, we study and compare the features of state - ofthe - art linking frameworks. we also analyze reported performance evaluations for the different frameworks. finally, we derive insights pertaining to possible future developments in the domain of link discovery.",False,True
A highly efficient C-C bond cleavage of unstrained aliphatic ketones bearing β-hydrogens with olefins was achieved using a chelation-assisted catalytic system consisting of (Ph 3 P) 3 RhCl and 2-amino-3-picoline by microwave irradiation under solvent-free conditions. The addition of cyclohexylamine catalyst accelerated the reaction rate dramatically under microwave irradiation compared with the classical heating method.,what has research problem ?,C-C bond cleavage,cleavage,False,False
"In recent years, ``smart cities'' have rapidly increased in discourses as well as in their real number, and raise various issues. While citizen engagement is a key element of most definitions of smart cities, information and communication technologies (ICTs) would also have great potential for facilitating public participation. However, scholars have highlighted that little research has focused on actual practices of citizen involvement in smart cities so far. In this respect, the authors analyse public participation in Japanese ``Smart Communities'', paying attention to both official discourses and actual practices. Smart Communities were selected in 2010 by the Japanese government which defines them as ``smart city'' projects and imposed criteria such as focus on energy issues, participation and lifestyle innovation. Drawing on analysis of official documents as well as on interviews with each of the four Smart Communities' stakeholders, the paper explains that very little input is expected from Japanese citizens. Instead, ICTs are used by municipalities and electric utilities to steer project participants and to change their behaviour. The objective of Smart Communities would not be to involve citizens in city governance, but rather to make them participate in the co-production of public services, mainly energy production and distribution.",what has research problem ?,Smart cities,smart cities ',False,True
"Hyperspectral sensors are devices that acquire images over hundreds of spectral bands, thereby enabling the extraction of spectral signatures for objects or materials observed. Hyperspectral remote sensing has been used over a wide range of applications, such as agriculture, forestry, geology, ecological monitoring and disaster monitoring. In this paper, the specific application of hyperspectral remote sensing to agriculture is examined. The technological development of agricultural methods is of critical importance as the world's population is anticipated to continuously rise much beyond the current number of 7 billion. One area upon which hyperspectral sensing can yield considerable impact is that of precision agriculture - the use of observations to optimize the use of resources and management of farming practices. For example, hyperspectral image processing is used in the monitoring of plant diseases, insect pests and invasive plant species; the estimation of crop yield; and the fine classification of crop distributions. This paper also presents a detailed overview of hyperspectral data processing techniques and suggestions for advancing the agricultural applications of hyperspectral technologies in Turkey.",what has research problem ?,Application of Hyperspectral remote sensing to Agriculture,hyperspectral remote sensing,False,False
"The business value generated by information and communication technologies (ICT) has been for long time a major research topic. Recently there is a growing research interest in the business value generated by particular types of information systems (IS). One of them is the enterprise resource planning (ERP) systems, which are increasingly adopted by organizations for supporting and integrating key business and management processes. The current paper initially presents a critical review of the existing empirical literature concerning the business value of the ERP systems, which investigates the impact of ERP systems adoption on various measures of organizational performance. Then is critically reviewed the literature concerning the related topic of critical success factors (CSFs) in ERP systems implementation, which aims at identifying and investigating factors that result in more successful ERP systems implementation that generate higher levels of value for organizations. Finally, future directions of research concerning ERP systems business value are proposed.",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"Diabetes Mellitus (DM) is a chronic, progressive and life-threatening disease. The ocular manifestations of DM, Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME), are the leading causes of blindness in the adult population throughout the world. Early diagnosis of DR and DM through screening tests and successive treatments can reduce the threat to visual acuity. In this context, we propose an encoder decoder based semantic segmentation network SOP-Net (Segmentation of Ocular Pathologies Using Deep Convolutional Neural Network) for simultaneous delineation of retinal pathologies (hard exudates, soft exudates, hemorrhages, microaneurysms). The proposed semantic segmentation framework is capable of providing segmentation results at pixel-level with good localization of object boundaries. SOP-Net has been trained and tested on IDRiD dataset which is publicly available with pixel level annotations of retinal pathologies. The network achieved average accuracies of 98.98%, 90.46%, 96.79%, and 96.70% for segmentation of hard exudates, soft exudates, hemorrhages, and microaneurysms. The proposed methodology has the capability to be used in developing a diagnostic system for organizing large scale ophthalmic screening programs.",what has research problem ?,"simultaneous delineation of retinal pathologies (hard exudates, soft exudates, hemorrhages, microaneurysms)",diabetic retinopathy,False,False
"Purpose – The purpose of this paper is to review the literature on supply chain management (SCM) practices in small and medium scale enterprises (SMEs) and outlines the key insights.Design/methodology/approach – The paper describes a literature‐based research that has sought understand the issues of SCM for SMEs. The methodology is based on critical review of 77 research papers from high‐quality, international refereed journals. Mainly, issues are explored under three categories – supply chain integration, strategy and planning and implementation. This has supported the development of key constructs and propositions.Findings – The research outcomes are three fold. Firstly, paper summarizes the reported literature and classifies it based on their nature of work and contributions. Second, paper demonstrates the overall approach towards the development of constructs, research questions, and investigative questions leading to key proposition for the further research. Lastly, paper outlines the key findings an...",what has research problem ?,Supply chain management,supply chain management,True,True
"A side‐chain conjugation strategy in the design of nonfullerene electron acceptors is proposed, with the design and synthesis of a side‐chain‐conjugated acceptor (ITIC2) based on a 4,8‐bis(5‐(2‐ethylhexyl)thiophen‐2‐yl)benzo[1,2‐b:4,5‐b′]di(cyclopenta‐dithiophene) electron‐donating core and 1,1‐dicyanomethylene‐3‐indanone electron‐withdrawing end groups. ITIC2 with the conjugated side chains exhibits an absorption peak at 714 nm, which redshifts 12 nm relative to ITIC1. The absorption extinction coefficient of ITIC2 is 2.7 × 105m−1 cm−1, higher than that of ITIC1 (1.5 × 105m−1 cm−1). ITIC2 exhibits slightly higher highest occupied molecular orbital (HOMO) (−5.43 eV) and lowest unoccupied molecular orbital (LUMO) (−3.80 eV) energy levels relative to ITIC1 (HOMO: −5.48 eV; LUMO: −3.84 eV), and higher electron mobility (1.3 × 10−3 cm2 V−1 s−1) than that of ITIC1 (9.6 × 10−4 cm2 V−1 s−1). The power conversion efficiency of ITIC2‐based organic solar cells is 11.0%, much higher than that of ITIC1‐based control devices (8.54%). Our results demonstrate that side‐chain conjugation can tune energy levels, enhance absorption, and electron mobility, and finally enhance photovoltaic performance of nonfullerene acceptors.",what has research problem ?,Organic solar cells,nonfullerene electron,False,False
"Multilingual Named Entity Recognition (NER) is a key intermediate task which is needed in many areas of NLP. In this paper, we address the well-known issue of data scarcity in NER, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of multilingual silver data for the task. We exploit the texts of Wikipedia and introduce a new methodology based on the effective combination of knowledge-based approaches and neural models, together with a novel domain adaptation technique, to produce high-quality training corpora for NER. We evaluate our datasets extensively on standard benchmarks for NER, yielding substantial improvements of up to 6 span-based F1-score points over previous state-of-the-art systems for data creation.",what has research problem ?,Multilingual named entity recognition,multilingual named entity recognition,True,True
"In this paper, we present a novel approach to estimate the relative depth of regions in monocular images. There are several contributions. First, the task of monocular depth estimation is considered as a learning-to-rank problem which offers several advantages compared to regression approaches. Second, monocular depth clues of human perception are modeled in a systematic manner. Third, we show that these depth clues can be modeled and integrated appropriately in a Rankboost framework. For this purpose, a space-efficient version of Rankboost is derived that makes it applicable to rank a large number of objects, as posed by the given problem. Finally, the monocular depth clues are combined with results from a deep learning approach. Experimental results show that the error rate is reduced by adding the monocular features while outperforming state-of-the-art systems.",what has research problem ?,Depth Estimation,,False,False
"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize.",what has research problem ?,Parsing,"unlexicalized pcfg can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.",False,False
"Enterprise resource planning (ERP) systems have emerged as the core of successful information management and the enterprise backbone of organizations. The difficulties of ERP implementations have been widely cited in the literature but research on the critical factors for initial and ongoing ERP implementation success is rare and fragmented. Through a comprehensive review of the literature, 11 factors were found to be critical to ERP implementation success – ERP teamwork and composition; change management program and culture; top management support; business plan and vision; business process reengineering with minimum customization; project management; monitoring and evaluation of performance; effective communication; software development, testing and troubleshooting; project champion; appropriate business and IT legacy systems. The classification of these factors into the respective phases (chartering, project, shakedown, onward and upward) in Markus and Tanis’ ERP life cycle model is presented and the importance of each factor is discussed.",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.",what has research problem ?,Named Entity Recognition,named entity recognition,True,True
"Evolutionary algorithms are methods, which imitate the natural evolution process. An artificial evolution process evaluates fitness of each individual, which are solution candidates. The next population of candidate solutions is formed by using the good properties of the current population by applying different mutation and crossover operations. Different kinds of evolutionary algorithm applications related to software engineering were searched in the literature. Because the entire book presents some interesting information chapter of some evolutionary computation techniques applied into the software engineering we consider necessary to present into this chapter a short survey of some techniques which are very useful in the future research of this field. The majority of evolutionary algorithm applications related to software engineering were about software design or testing. Software Engineering is the application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software, and the study of these approaches; that is, the application of engineering to software (Abran and Moore, 2004). The purpose of this book is to open a door in order to find out the optimization problems in different software engineering problems. The idea of putting together the application of evolutionary computation and evolutionary optimization techniques in software engineering problems provided to the researchers the possibility to study some existing abSTraCT",what has research problem ?,Evolutionary Computation Techniques,software engineering,False,False
"Abstract The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants’ systems had to identify and rank relevant articles in a collection of 5.2 M MEDLINE citations (task 1) or 530 000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search.",what has research problem ?,text-mining services for kinome curation,"kinome curation track, part of biocreative vi, proposed a competition to assess the effectiveness of text mining to perform literature triage. the track has exploited an unpublished curated data set from the nextprot database. this data set contained comprehensive annotations for 300 human protein kinases.",False,False
"Over the last ten years, there has been a dramatic growth in the acquisition of Enterprise Resource Planning (ERP) systems, where the market leader is the German company, SAP AG. However, more recently, there has been an increase in reported ERP failures, suggesting that the implementation issues are not just technical, but encompass wider behavioural factors.",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"We present SemEval-2019 Task 8 on Fact Checking in Community Question Answering Forums, which features two subtasks. Subtask A is about deciding whether a question asks for factual information vs. an opinion/advice vs. just socializing. Subtask B asks to predict whether an answer to a factual question is true, false or not a proper answer. We received 17 official submissions for subtask A and 11 official submissions for Subtask B. For subtask A, all systems improved over the majority class baseline. For Subtask B, all systems were below a majority class baseline, but several systems were very close to it. The leaderboard and the data from the competition can be found at http://competitions.codalab.org/competitions/20022.",what has research problem ?,Fact Checking in Community Question Answering Forums,"community question answering forums,",False,False
"The successful implementation of various enterprise resource planning (ERP) systems has provoked considerable interest over the last few years. Management has recently been enticed to look toward these new information technologies and philosophies of manufacturing for the key to survival or competitive edges. Although there is no shortage of glowing reports on the success of ERP installations, many companies have tossed millions of dollars in this direction with little to show for it. Since many of the ERP failures today can be attributed to inadequate planning prior to installation, we choose to analyze several critical planning issues including needs assessment and choosing a right ERP system, matching business process with the ERP system, understanding the organizational requirements, and economic and strategic justification. In addition, this study also identifies new windows of opportunity as well as challenges facing companies today as enterprise systems continue to evolve and expand.",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"ABSTRACTThe concept of the digital twin calls for virtual replicas of real world products. Achieving this requires a sophisticated network of models that have a level of interconnectivity. The authors attempted to improve model interconnectivity by enhancing the computer-aided design model with spatially related non-geometric data. A tool was created to store, visualize, and search for spatial data within the computer-aided design tool. This enables both model authors, and consumers to utilize information inside the CAD tool which traditionally would have existed in separate software.",what has research problem ?,digital twin,digital twin,True,True
"Existing literature on Question Answering (QA) mostly focuses on algorithmic novelty, data augmentation, or increasingly large pre-trained language models like XLNet and RoBERTa. Additionally, a lot of systems on the QA leaderboards do not have associated research documentation in order to successfully replicate their experiments. In this paper, we outline these algorithmic components such as Attention-over-Attention, coupled with data augmentation and ensembling strategies that have shown to yield state-of-the-art results on benchmark datasets like SQuAD, even achieving super-human performance. Contrary to these prior results, when we evaluate on the recently proposed Natural Questions benchmark dataset, we find that an incredibly simple approach of transfer learning from BERT outperforms the previous state-of-the-art system trained on 4 million more examples than ours by 1.9 F1 points. Adding ensembling strategies further improves that number by 2.3 F1 points.",what has research problem ?,Question Answering,question answering,True,True
"E-learning recommender systems are gaining significance nowadays due to its ability to enhance the learning experience by providing tailor-made services based on learner preferences. A Personalized Learning Environment (PLE) that automatically adapts to learner characteristics such as learning styles and knowledge level can recommend appropriate learning resources that would favor the learning process and improve learning outcomes. The pure cold-start problem is a relevant issue in PLEs, which arises due to the lack of prior information about the new learner in the PLE to create appropriate recommendations. This article introduces a semantic framework based on ontology to address the pure cold-start problem in content recommenders. The ontology encapsulates the domain knowledge about the learners as well as Learning Objects (LOs). The semantic model that we built has been experimented with different combinations of the key learner parameters such as learning style, knowledge level, and background knowledge. The proposed framework utilizes these parameters to build natural learner groups from the learner ontology using SPARQL queries. The ontology holds 480 learners’ data, 468 annotated learning objects with 5,600 learner ratings. A multivariate k-means clustering algorithm, an unsupervised machine learning technique for grouping similar data, is used to evaluate the learner similarity computation accuracy. The learner satisfaction achieved with the proposed model is measured based on the ratings given by the 40 participants of the experiments. From the evaluation perspective, it is evident that 79% of the learners are satisfied with the recommendations generated by the proposed model in pure cold-start condition.",what has research problem ?,Recommender Systems,cold - start problem,False,False
"This paper presents the Graded Word Similarity in Context (GWSC) task which asked participants to predict the effects of context on human perception of similarity in English, Croatian, Slovene and Finnish. We received 15 submissions and 11 system description papers. A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two different contexts. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect.",what has research problem ?,Graded Word Similarity in Context,graded word similarity,False,False
"Business architecture became a well-known tool for business transformations. According to a recent study by Forrester, 50 percent of the companies polled claimed to have an active business architecture initiative, whereas 20 percent were planning to engage in business architecture work in the near future. However, despite the high interest in BA, there is not yet a common understanding of the main concepts. There is a lack for the business architecture framework which provides a complete metamodel, suggests methodology for business architecture development and enables tool support for it. The ORG- Master framework is designed to solve this problem using the ontology as a core of the metamodel. This paper describes the ORG-Master framework, its implementation and dissemination.",what has research problem ?,Business architecture development,business architecture,False,False
"The technological development of quantum dots has ushered in a new era in fluorescence bioimaging, which was propelled with the advent of novel multiphoton fluorescence microscopes. Here, the potential use of CdSe quantum dots has been evaluated as fluorescent nanothermometers for two-photon fluorescence microscopy. In addition to the enhancement in spatial resolution inherent to any multiphoton excitation processes, two-photon (near-infrared) excitation leads to a temperature sensitivity of the emission intensity much higher than that achieved under one-photon (visible) excitation. The peak emission wavelength is also temperature sensitive, providing an additional approach for thermal imaging, which is particularly interesting for systems where nanoparticles are not homogeneously dispersed. On the basis of these superior thermal sensitivity properties of the two-photon excited fluorescence, we have demonstrated the ability of CdSe quantum dots to image a temperature gradient artificially created in a biocompatible fluid (phosphate-buffered saline) and also their ability to measure an intracellular temperature increase externally induced in a single living cell.",what has research problem ?,Nanothermometer,"fluorescence bioimaging,",False,False
"This paper presents a method that conbines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dictionary (MRD). Our aim is to profit from conventional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic extraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE.",what has research problem ?,automatic extraction of taxonomic links from MRD entries,taxonomy building,False,False
"– Critical success factors (CSFs) have been widely used in the context of commercial supply chains. However, in the context of humanitarian aid (HA) this is a poorly addressed area and this paper therefore aims to set out the key areas for research., – This paper is based on a conceptual discussion of CSFs as applied to the HA sector. A detailed literature review is undertaken to identify CSFs in a commercial context and to consider their applicability to the HA sector., – CSFs have not previously been identified for the HA sector, an issue addressed in this paper., – The main constraint on this paper is that CSFs have not been previously considered in the literature as applied to HA. The relevance of CSFs will therefore need to be tested in the HA environment and qualitative research is needed to inform further work., – This paper informs the HA community of key areas of activity which have not been fully addressed and offers., – This paper contributes to the understanding of supply chain management in an HA context.",what has research problem ?,Supply chain management,,False,False
"Eighty‐six patients with monosymptomatic optic neuritis of unknown cause were followed prospectively for a median period of 12.9 years. At onset, cerebrospinal fluid (CSF) pleocytosis was present in 46 patients (53%) but oligoclonal immunoglobulin in only 40 (47%) of the patients. The human leukocyte antigen (HLA)‐DR2 was present in 45 (52%). Clinically definite multiple sclerosis (MS) was established in 33 patients. Actuarial analysis showed that the cumulative probability of developing MS within 15 years was 45%. Three risk factors were identified: low age and abnormal CSF at onset, and early recurrence of optic neuritis. Female gender, onset in the winter season, and the presence of HLA‐DR2 antigen increased the risk for MS, but not significantly. Magnetic resonance imaging detected bilateral discrete white matter lesions, similar to those in MS, in 11 of 25 patients, 7 to 18 years after the isolated attack of optic neuritis. Nine were among the 13 with abnormal CSF and only 2 belonged to the group of 12 with normal CSF (p = 0.01). Normal CSF at the onset of optic neuritis conferred better prognosis but did not preclude the development of MS.",what has research problem ?,Multiple sclerosis,multiple sclerosis,True,True
"We present two related tasks of the BioNLP Shared Tasks 2011: Bacteria Gene Renaming (Rename) and Bacteria Gene Interactions (GI). We detail the objectives, the corpus specification, the evaluation metrics, and we summarize the participants' results. Both issued from PubMed scientific literature abstracts, the Rename task aims at extracting gene name synonyms, and the GI task aims at extracting genic interaction events, mainly about gene transcriptional regulations in bacteria.",what has research problem ?,Bacteria gene renaming,bacteria gene renaming,True,True
"Enterprise resource planning (ERP) is one the most common applications implemented by firms around the world. These systems cannot remain static after their implementation, they need maintenance. This process is required by the rapidly-changing business environment and the usual software maintenance needs. However, these projects are highly complex and risky. So, the risks management associated with ERP maintenance projects is crucial to attain a satisfactory performance. Unfortunately, ERP maintenance risks have not been studied in depth. For this reason, this paper presents a framework, which gathers together the risks affecting the performance of ERP maintenance.",what has research problem ?,Enterprise resource planning,enterprise resource planning,True,True
"Abstract Despite rapid progress, most of the educational technologies today lack a strong instructional design knowledge basis leading to questionable quality of instruction. In addition, a major challenge is to customize these educational technologies for a wide range of customizable instructional designs. Ontologies are one of the pertinent mechanisms to represent instructional design in the literature. However, existing approaches do not support modeling of flexible instructional designs. To address this problem, in this paper, we propose an ontology based framework for systematic modeling of different aspects of instructional design knowledge based on domain patterns. As part of the framework, we present ontologies for modeling goals , instructional processes and instructional material . We demonstrate the ontology framework by presenting instances of the ontology for the large scale case study of adult literacy in India (287 million learners spread across 22 Indian Languages), which requires creation of hundreds of similar but varied e Learning Systems based on flexible instructional designs. The implemented framework is available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://rice.iiit.ac.in"">http://rice.iiit.ac.in</jats:ext-link> and is transferred to National Literacy Mission Authority of Government of India . The proposed framework could be potentially used for modeling instructional design knowledge for school education, vocational skills and beyond.",what has research problem ?,framework,adult literacy,False,False
"Acute lymphoblastic leukemia (ALL) is the most common childhood malignancy, and implementation of risk-adapted therapy has been instrumental in the dramatic improvements in clinical outcomes. A key to risk-adapted therapies includes the identification of genomic features of individual tumors, including chromosome number (for hyper- and hypodiploidy) and gene fusions, notably ETV6-RUNX1, TCF3-PBX1, and BCR-ABL1 in B-cell ALL (B-ALL). RNA-sequencing (RNA-seq) of large ALL cohorts has expanded the number of recurrent gene fusions recognized as drivers in ALL, and identification of these new entities will contribute to refining ALL risk stratification. We used RNA-seq on 126 ALL patients from our clinical service to test the utility of including RNA-seq in standard-of-care diagnostic pipelines to detect gene rearrangements and IKZF1 deletions. RNA-seq identified 86% of rearrangements detected by standard-of-care diagnostics. KMT2A (MLL) rearrangements, although usually identified, were the most commonly missed by RNA-seq as a result of low expression. RNA-seq identified rearrangements that were not detected by standard-of-care testing in 9 patients. These were found in patients who were not classifiable using standard molecular assessment. We developed an approach to detect the most common IKZF1 deletion from RNA-seq data and validated this using an RQ-PCR assay. We applied an expression classifier to identify Philadelphia chromosome-like B-ALL patients. T-ALL proved a rich source of novel gene fusions, which have clinical implications or provide insights into disease biology. Our experience shows that RNA-seq can be implemented within an individual clinical service to enhance the current molecular diagnostic risk classification of ALL.",what has research problem ?,acute lymphoblastic leukemia (ALL),acute lymphoblastic leukemia,False,False
"Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\it MCTest} benchmark. Partly because of its limited size, prior work on {\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\it MCTest}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15\% absolute).",what has research problem ?,Understanding unstructured text,machine comprehension,False,False
"Coastal safety may be influenced by climate change, as changes in extreme surge levels and wave extremes may increase the vulnerability of dunes and other coastal defenses. In the North Sea, an area already prone to severe flooding, these high surge levels and waves are generated by low atmospheric pressure and severe wind speeds during storm events. As a result of the geometry of the North Sea, not only the maximum wind speed is relevant, but also wind direction. Climate change could change maximum wind conditions, with potentially negative effects for coastal safety. Here, we use an ensemble of 12 Coupled Model Intercomparison Project Phase 5 (CMIP5) General Circulation Models (GCMs) and diagnose the effect of two climate scenarios (rcp4.5 and rcp8.5) on annual maximum wind speed, wind speeds with lower return frequencies, and the direction of these annual maximum wind speeds. The 12 selected CMIP5 models do not project changes in annual maximum wind speed and in wind speeds with lower return frequencies; however, we do find an indication that the annual extreme wind events are coming more often from western directions. Our results are in line with the studies based on CMIP3 models and do not confirm the statement based on some reanalysis studies that there is a climate‐change‐related upward trend in storminess in the North Sea area.",what has research problem ?,CMIP5,coastal safety,False,False
"Purpose – The purpose of this paper is to explore critical factors for implementing green supply chain management (GSCM) practice in the Taiwanese electrical and electronics industries relative to European Union directives.Design/methodology/approach – A tentative list of critical factors of GSCM was developed based on a thorough and detailed analysis of the pertinent literature. The survey questionnaire contained 25 items, developed based on the literature and interviews with three industry experts, specifically quality and product assurance representatives. A total of 300 questionnaires were mailed out, and 87 were returned, of which 84 were valid, representing a response rate of 28 percent. Using the data collected, the identified critical factors were performed via factor analysis to establish reliability and validity.Findings – The results show that 20 critical factors were extracted into four dimensions, which denominated supplier management, product recycling, organization involvement and life cycl...",what has research problem ?,Supply chain management,supply chain management,True,True
"Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a two-channel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task.",what has research problem ?,sentence similarity,,False,False
"A simple small molecule acceptor named DICTF, with fluorene as the central block and 2-(2,3-dihydro-3-oxo-1H-inden-1-ylidene)propanedinitrile as the end-capping groups, has been designed for fullerene-free organic solar cells. The new molecule was synthesized from widely available and inexpensive commercial materials in only three steps with a high overall yield of ∼60%. Fullerene-free organic solar cells with DICTF as the acceptor material provide a high PCE of 7.93%.",what has research problem ?,Organic solar cells,organic solar cells.,True,True
"Our purpose is to identify the relevance of participative governance in urban areas characterized by smart cities projects, especially those implementing Living Labs initiatives as real-life settings to develop services innovation and enhance engagement of all urban stakeholders. A research on the three top smart cities in Europe – i.e. Amsterdam, Barcelona and Helsinki – is proposed through a content analysis with NVivo on the offi cial documents issued by the project partners (2012-2015) to investigate their Living Lab initiatives. The results show the increasing usefulness of Living Labs for the development of more inclusive smart cities projects in which public and private actors, and people, collaborate in innovation processes and governance for the co-creation of new services, underlining the importance of the open and ecosystem-oriented approach for smart cities.",what has research problem ?,Smart cities,smart cities,True,True
"This paper presents the preparation, resources, results and analysis of the Epigenetics and Post-translational Modifications (EPI) task, a main task of the BioNLP Shared Task 2011. The task concerns the extraction of detailed representations of 14 protein and DNA modification events, the catalysis of these reactions, and the identification of instances of negated or speculatively stated event instances. Seven teams submitted final results to the EPI task in the shared task, with the highest-performing system achieving 53% F-score in the full task and 69% F-score in the extraction of a simplified set of core event arguments.",what has research problem ?,The Epigenetics and Post-translational Modifications (EPI) task,epigenetics and post - translational modifications,False,False
"Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step. Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages. Similarly, pre-processing introduces an additional source of error. To address these limitations, we introduce TRE, a Transformer for Relation Extraction, extending the OpenAI Generative Pre-trained Transformer [Radford et al., 2018]. Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions. TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task. TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively. Furthermore, we observe a significant increase in sample efficiency. With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset. We open-source our trained models, experiments, and source code.",what has research problem ?,"learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task",relation extraction,False,False
"Academic attention to smart cities and their governance is growing rapidly, but the fragmentation in approaches makes for a confusing debate. This article brings some structure to the debate by analyzing a corpus of 51 publications and mapping their variation. The analysis shows that publications differ in their emphasis on (1) smart technology, smart people or smart collaboration as the defining features of smart cities, (2) a transformative or incremental perspective on changes in urban governance, (3) better outcomes or a more open process as the legitimacy claim for smart city governance. We argue for a comprehensive perspective: smart city governance is about crafting new forms of human collaboration through the use of ICTs to obtain better outcomes and more open governance processes. Research into smart city governance could benefit from previous studies into success and failure factors for e-government and build upon sophisticated theories of socio-technical change. This article highlights that smart city governance is not a technological issue: we should study smart city governance as a complex process of institutional change and acknowledge the political nature of appealing visions of socio-technical governance. Points for practitioners The study provides practitioners with an in-depth understanding of current debates about smart city governance. The article highlights that governing a smart city is about crafting new forms of human collaboration through the use of information and communication technologies. City managers should realize that technology by itself will not make a city smarter: building a smart city requires a political understanding of technology, a process approach to manage the emerging smart city and a focus on both economic gains and other public values.",what has research problem ?,Smart cities,smart cities,True,True
"(1) Background: Although bullying victimization is a phenomenon that is increasingly being recognized as a public health and mental health concern in many countries, research attention on this aspect of youth violence in low- and middle-income countries, especially sub-Saharan Africa, is minimal. The current study examined the national prevalence of bullying victimization and its correlates among in-school adolescents in Ghana. (2) Methods: A sample of 1342 in-school adolescents in Ghana (55.2% males; 44.8% females) aged 12–18 was drawn from the 2012 Global School-based Health Survey (GSHS) for the analysis. Self-reported bullying victimization “during the last 30 days, on how many days were you bullied?” was used as the central criterion variable. Three-level analyses using descriptive, Pearson chi-square, and binary logistic regression were performed. Results of the regression analysis were presented as adjusted odds ratios (aOR) at 95% confidence intervals (CIs), with a statistical significance pegged at p < 0.05. (3) Results: Bullying victimization was prevalent among 41.3% of the in-school adolescents. Pattern of results indicates that adolescents in SHS 3 [aOR = 0.34, 95% CI = 0.25, 0.47] and SHS 4 [aOR = 0.30, 95% CI = 0.21, 0.44] were less likely to be victims of bullying. Adolescents who had sustained injury [aOR = 2.11, 95% CI = 1.63, 2.73] were more likely to be bullied compared to those who had not sustained any injury. The odds of bullying victimization were higher among adolescents who had engaged in physical fight [aOR = 1.90, 95% CI = 1.42, 2.25] and those who had been physically attacked [aOR = 1.73, 95% CI = 1.32, 2.27]. Similarly, adolescents who felt lonely were more likely to report being bullied [aOR = 1.50, 95% CI = 1.08, 2.08] as against those who did not feel lonely. Additionally, adolescents with a history of suicide attempts were more likely to be bullied [aOR = 1.63, 95% CI = 1.11, 2.38] and those who used marijuana had higher odds of bullying victimization [aOR = 3.36, 95% CI = 1.10, 10.24]. (4) Conclusions: Current findings require the need for policy makers and school authorities in Ghana to design and implement policies and anti-bullying interventions (e.g., Social Emotional Learning (SEL), Emotive Behavioral Education (REBE), Marijuana Cessation Therapy (MCT)) focused on addressing behavioral issues, mental health and substance abuse among in-school adolescents.",what has research problem ?,bullying,bullying victimization,False,True
"The FAIR principles were received with broad acceptance in several scientific communities. However, there is still some degree of uncertainty on how they should be implemented. Several self-report questionnaires have been proposed to assess the implementation of the FAIR principles. Moreover, the FAIRmetrics group released 14, general-purpose maturity for representing FAIRness. Initially, these metrics were conducted as open-answer questionnaires. Recently, these metrics have been implemented into a software that can automatically harvest metadata from metadata providers and generate a principle-specific FAIRness evaluation. With so many different approaches for FAIRness evaluations, we believe that further clarification on their limitations and advantages, as well as on their interpretation and interplay should be considered.",what has research problem ?,Fairness,fairness evaluation.,False,True
"The widespread adoption of agile methodologies raises the question of their continued and effective usage in organizations. An agile usage model consisting of innovation, sociological, technological, team, and organizational factors is used to inform an analysis of post-adoptive usage of agile practices in two major organizations. Analysis of the two case studies found that a methodology champion and top management support were the most important factors influencing continued usage, while innovation factors such as compatibility seemed less influential. Both horizontal and vertical usage was found to have significant impact on the effectiveness of agile usage.",what has research problem ?,Agile Usage,agile,False,False
"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",what has research problem ?,Image Classification,image classification,True,True
"Universities and higher education institutes continually create knowledge. Hence it is necessary to keep a record of the academic and administrative information generated. Considering the vast amount of information managed by higher education institutions and the diversity of heterogeneous systems that can coexist within the same institution, it becomes necessary to use technologies for knowledge representation. Ontologies facilitate access to knowledge allowing the adequate exchange of information between people and between heterogeneous systems. This paper aims to identify existing research on the use and application of ontologies in higher education. From a set of 2792 papers, a study based on systematic mapping was conducted. A total of 52 research papers were reviewed and analyzed. Our results contribute key findings regarding how ontologies are used in higher education institutes, what technologies and tools are applied for the development of ontologies and what are the main vocabularies reused in the application of ontologies.",what has research problem ?,Knowledge Representation,systematic mapping,False,False
"End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16× speedup with a slight reduction in accuracy.",what has research problem ?,Relation Extraction,relation extraction,True,True
"Many of the challenges to be faced by smart cities surpass the capacities, capabilities, and reaches of their traditional institutions and their classical processes of governing, and therefore new and innovative forms of governance are needed to meet these challenges. According to the network governance literature, governance models in public administrations can be categorized through the identification and analysis of some main dimensions that govern in the way of managing the city by governments. Based on prior research and on the perception of city practitioners in European smart cities, this paper seeks to analyze the relevance of main dimensions of governance models in smart cities. Results could shed some light regarding new future research on efficient patterns of governance models within smart cities.",what has research problem ?,Smart cities,smart cities,True,True
"Game developers are facing an increasing demand for new games every year. Game development tools can be of great help, but require highly specialized professionals. Also, just as any software development effort, game development has some challenges. Model-Driven Game Development (MDGD) is suggested as a means to solve some of these challenges, but with a loss in flexibility. We propose a MDGD approach that combines multiple domain-specific languages (DSLs) with design patterns to provide flexibility and allow generated code to be integrated with manual code. After experimentation, we observed that, with the approach, less experienced developers can create games faster and more easily, and the product of code generation can be customized with manually written code, providing flexibility. However, with MDGD, developers become less familiar with the code, making manual codification more difficult.",what has research problem ?,Model-driven Game Development,model - driven game development,False,False
"Environmental sustainability is a critical global issue that requires comprehensive intervention policies. Viewed as localized intervention policy implementations, smart cities leverage information infrastructures and distributed renewable energy smart micro-grids, smart meters, and home/building energy management systems to reduce city-wide carbon emissions. However, theory-driven smart city implementation research is critically lacking. This theory-building case study identifies antecedent conditions necessary for implementing smart cities. We integrated resource dependence, social embeddedness, and citizen-centric e-governance theories to develop a citizen-centric social governance framework. We apply the framework to a field-based case study of Japan’s Kitakyushu smart community project to examine the validity and utility of the framework’s antecedent conditions: resource-dependent leadership network, cross-sector collaboration based on social ties, and citizen-centric e-governance. We conclude that complex smart community implementation processes require shared vision of social innovation owned by diverse stakeholders with conflicting values and adaptive use of informal social governance mechanisms for effective smart city implementation.",what has research problem ?,Smart cities,environmental sustainability,False,False
"The accuracy of face alignment affects the performance of a face recognition system. Since face alignment is usually conducted using eye positions, an accurate eye localization algorithm is therefore essential for accurate face recognition. In this paper, we first study the impact of eye locations on face recognition accuracy, and then introduce an automatic technique for eye detection. The performance of our automatic eye detection technique is subsequently validated using FRGC 1.0 database. The validation shows that our eye detector has an overall 94.5% eye detection rate, with the detected eyes very close to the manually provided eye positions. In addition, the face recognition performance based on the automatic eye detection is shown to be comparable to that of using manually given eye positions.",what has research problem ?,Eye localization,face recognition system.,False,False
"As part of the BioNLP Open Shared Tasks 2019, the CRAFT Shared Tasks 2019 provides a platform to gauge the state of the art for three fundamental language processing tasks — dependency parse construction, coreference resolution, and ontology concept identification — over full-text biomedical articles. The structural annotation task requires the automatic generation of dependency parses for each sentence of an article given only the article text. The coreference resolution task focuses on linking coreferring base noun phrase mentions into chains using the symmetrical and transitive identity relation. The ontology concept annotation task involves the identification of concept mentions within text using the classes of ten distinct ontologies in the biomedical domain, both unmodified and augmented with extension classes. This paper provides an overview of each task, including descriptions of the data provided to participants and the evaluation metrics used, and discusses participant results relative to baseline performances for each of the three tasks.",what has research problem ?,Coreference Resolution,ontology concept identification —,False,False
"Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder--decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to English German, and English->Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, BLEU and CHRF3. An open-source implementation of our neural MT system is available, as are sample files and configurations.",what has research problem ?,Machine Translation,neural machine translation,False,True
"This study examines the dynamic relationship among carbon dioxide (CO2) emissions, economic growth, energy consumption and foreign trade based on the environmental Kuznets curve (EKC) hypothesis in Indonesia for the period 1971–2007, using the Auto Regressive Distributed Lag (ARDL) methodology. The results do not support the EKC hypothesis, which assumes an inverted U-shaped relationship between income and environmental degradation. The long-run results indicate that foreign trade is the most significant variable in explaining CO2 emissions in Indonesia followed by Energy consumption and economic growth. The stability of the variables in estimated model is also examined. The result suggests that the estimated model is stable over the study period.",what has research problem ?,CO2 emissions,,False,False
"The aim of this study was to identify and analyse the key success factors behind successful achievement of environment sustainability in Indian automobile industry supply chains. Here, critical success factors (CSFs) and performance measures of green supply chain management (GSCM) have been identified through extensive literature review and discussions with experts from Indian automobile industry. Based on the literature review, a questionnaire was designed and 123 final responses were considered. Six CSFs to implement GSCM for achieving sustainability and four expected performance measures of GSCM practices implementation were extracted using factor analysis. interpretive ranking process (IRP) modelling approach is employed to examine the contextual relationships among CSFs and to rank them with respect to performance measures. The developed IRP model shows that the CSF ‘Competitiveness’ is the most important CSF for achieving sustainability in Indian automobile industry through GSCM practices. This study is one of the few that have considered the environmental sustainability practices in the automobile industry in India and their implications on sectoral economy. The results of this study may help the mangers/SC practitioners/Governments/Customers in making strategic and tactical decisions regarding successful implementation of GSCM practices in Indian automobile industry with a sustainability focus. The developed framework provides a comprehensive perspective for assessing the synergistic impact of CSFs on GSCM performances and can act as ready reckoner for the practitioners. As there is very limited work presented in literature using IRP, this piece of work would provide a better understanding of this relatively new ranking methodology.",what has research problem ?,Supply chain management,environment sustainability,False,False
"This paper describes the first task on semantic relation extraction and classification in scientific paper abstracts at SemEval 2018. The challenge focuses on domain-specific semantic relations and includes three different subtasks. The subtasks were designed so as to compare and quantify the effect of different pre-processing steps on the relation classification results. We expect the task to be relevant for a broad range of researchers working on extracting specialized knowledge from domain corpora, for example but not limited to scientific or bio-medical information extraction. The task attracted a total of 32 participants, with 158 submissions across different scenarios.",what has research problem ?,Semantic Relation Extraction and Classification,semantic relation extraction,False,False
"Various new national advanced manufacturing strategies, such as Industry 4.0, Industrial Internet, and Made in China 2025, are issued to achieve smart manufacturing, resulting in the increasing number of newly designed production lines in both developed and developing countries. Under the individualized designing demands, more realistic virtual models mirroring the real worlds of production lines are essential to bridge the gap between design and operation. This paper presents a digital twin-based approach for rapid individualized designing of the hollow glass production line. The digital twin merges physics-based system modeling and distributed real-time process data to generate an authoritative digital design of the system at pre-production phase. A digital twin-based analytical decoupling framework is also developed to provide engineering analysis capabilities and support the decision-making over the system designing and solution evaluation. Three key enabling techniques as well as a case study in hollow glass production line are addressed to validate the proposed approach.",what has research problem ?,digital twin,hollow glass production line.,False,False
"A large community of research has been developed in recent years to analyze social media and social networks, with the aim of understanding, discovering insights, and exploiting the available information. The focus has shifted from conventional polarity classification to contemporary application-oriented fine-grained aspects such as, emotions, sarcasm, stance, rumor, and hate speech detection in the user-generated content. Detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. The majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. In this paper, we propose a deep learning model called sAtt-BLSTM convNet that is based on the hybrid of soft attention-based bidirectional long short-term memory (sAtt-BLSTM) and convolution neural network (convNet) applying global vectors for word representation (GLoVe) for building semantic word embeddings. In addition to the feature maps generated by the sAtt-BLSTM, punctuation-based auxiliary features are also merged into the convNet. The robustness of the proposed model is investigated using balanced (tweets from benchmark SemEval 2015 Task 11) and unbalanced (approximately 40000 random tweets using the Sarcasm Detector tool with 15000 sarcastic and 25000 non-sarcastic messages) datasets. An experimental study using the training- and test-set accuracy metrics is performed to compare the proposed deep neural model with convNet, LSTM, and bidirectional LSTM with/without attention and it is observed that the novel sAtt-BLSTM convNet model outperforms others with a superior sarcasm-classification accuracy of 97.87% for the Twitter dataset and 93.71% for the random-tweet dataset.",what Material ?,"words, emoticons, and exclamation marks",social media,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,our relational process model,modern information and communication technologies,False,False
"The molecular chaperone Hsp90-dependent proteome represents a complex protein network of critical biological and medical relevance. Known to associate with proteins with a broad variety of functions termed clients, Hsp90 maintains key essential and oncogenic signalling pathways. Consequently, Hsp90 inhibitors are being tested as anti-cancer drugs. Using an integrated systematic approach to analyse the effects of Hsp90 inhibition in T-cells, we quantified differential changes in the Hsp90-dependent proteome, Hsp90 interactome, and a selection of the transcriptome. Kinetic behaviours in the Hsp90-dependent proteome were assessed using a novel pulse-chase strategy (Fierro-Monti et al., accompanying article), detecting effects on both protein stability and synthesis. Global and specific dynamic impacts, including proteostatic responses, are due to direct inhibition of Hsp90 as well as indirect effects. As a result, a decrease was detected in most proteins that changed their levels, including known Hsp90 clients. Most likely, consequences of the role of Hsp90 in gene expression determined a global reduction in net de novo protein synthesis. This decrease appeared to be greater in magnitude than a concomitantly observed global increase in protein decay rates. Several novel putative Hsp90 clients were validated, and interestingly, protein families with critical functions, particularly the Hsp90 family and cofactors themselves as well as protein kinases, displayed strongly increased decay rates due to Hsp90 inhibitor treatment. Remarkably, an upsurge in survival pathways, involving molecular chaperones and several oncoproteins, and decreased levels of some tumour suppressors, have implications for anti-cancer therapy with Hsp90 inhibitors. The diversity of global effects may represent a paradigm of mechanisms that are operating to shield cells from proteotoxic stress, by promoting pro-survival and anti-proliferative functions. Data are available via ProteomeXchange with identifier PXD000537.",what Material ?,Several novel putative Hsp90 clients,"molecular chaperone hsp90 - dependent proteome represents a complex protein network of critical biological and medical relevance. known to associate with proteins with a broad variety of functions termed clients, hsp90 maintains key essential and oncogenic signalling pathways. consequently, hsp90 inhibitors are being tested as anti - cancer drugs. using an integrated systematic approach to analyse the effects of hsp90 inhibition in t - cells,",False,False
"Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.",what Material ?,CoNLL-2003 dataset,text,False,False
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Material ?,high fat fed non-diabetic rats,,False,False
"To improve designs of e-learning materials, it is necessary to know which word or figure a learner felt ""difficult"" in the materials. In this pilot study, we measured electroencephalography (EEG) and eye gaze data of learners and analyzed to estimate which area they had difficulty to learn. The developed system realized simultaneous measurements of physiological data and subjective evaluations during learning. Using this system, we observed specific EEG activity in difficult pages. Integrating of eye gaze and EEG measurements raised a possibility to determine where a learner felt ""difficult"" in a page of learning materials. From these results, we could suggest that the multimodal measurements of EEG and eye gaze would lead to effective improvement of learning materials. For future study, more data collection using various materials and learners with different backgrounds is necessary. This study could lead to establishing a method to improve e-learning materials based on learners' mental states.",what Material ?,various materials and learners with different backgrounds,difficult pages.,False,False
"Colloidal nanocrystals (NCs) of APbX3-type lead halide perovskites [A = Cs+, CH3NH3+ (methylammonium or MA+) or CH(NH2)2+ (formamidinium or FA+); X = Cl–, Br–, I–] have recently emerged as highly versatile photonic sources for applications ranging from simple photoluminescence down-conversion (e.g., for display backlighting) to light-emitting diodes. From the perspective of spectral coverage, a formidable challenge facing the use of these materials is how to obtain stable emissions in the red and infrared spectral regions covered by the iodide-based compositions. So far, red-emissive CsPbI3 NCs have been shown to suffer from a delayed phase transformation into a nonluminescent, wide-band-gap 1D polymorph, and MAPbI3 exhibits very limited chemical durability. In this work, we report a facile colloidal synthesis method for obtaining FAPbI3 and FA-doped CsPbI3 NCs that are uniform in size (10–15 nm) and nearly cubic in shape and exhibit drastically higher robustness than their MA- or Cs-only cousins with similar sizes and morphologies. Detailed structural analysis indicated that the FAPbI3 NCs had a cubic crystal structure, while the FA0.1Cs0.9PbI3 NCs had a 3D orthorhombic structure that was isostructural to the structure of CsPbBr3 NCs. Bright photoluminescence (PL) with high quantum yield (QY > 70%) spanning red (690 nm, FA0.1Cs0.9PbI3 NCs) and near-infrared (near-IR, ca. 780 nm, FAPbI3 NCs) regions was sustained for several months or more in both the colloidal state and in films. The peak PL wavelengths can be fine-tuned by using postsynthetic cation- and anion-exchange reactions. Amplified spontaneous emissions with low thresholds of 28 and 7.5 μJ cm–2 were obtained from the films deposited from FA0.1Cs0.9PbI3 and FAPbI3 NCs, respectively. Furthermore, light-emitting diodes with a high external quantum efficiency of 2.3% were obtained by using FAPbI3 NCs.",what Material ?,materials,colloidal nanocrystals,False,False
"Over the last years, the Web of Data has grown significantly. Various interfaces such as LOD Stats, LOD Laudromat, SPARQL endpoints provide access to the hundered of thousands of RDF datasets, representing billions of facts. These datasets are available in different formats such as raw data dumps and HDT files or directly accessible via SPARQL endpoints. Querying such large amount of distributed data is particularly challenging and many of these datasets cannot be directly queried using the SPARQL query language. In order to tackle these problems, we present WimuQ, an integrated query engine to execute SPARQL queries and retrieve results from large amount of heterogeneous RDF data sources. Presently, WimuQ is able to execute both federated and non-federated SPARQL queries over a total of 668,166 datasets from LOD Stats and LOD Laudromat as well as 559 active SPARQL endpoints. These data sources represent a total of 221.7 billion triples from more than 5 terabytes of information from datasets retrieved using the service ""Where is My URI"" (WIMU). Our evaluation on state-of-the-art real-data benchmarks shows that WimuQ retrieves more complete results for the benchmark queries.",what Material ?,state-of-the-art real-data benchmarks,,False,False
"Over the last years, the Web of Data has grown significantly. Various interfaces such as LOD Stats, LOD Laudromat, SPARQL endpoints provide access to the hundered of thousands of RDF datasets, representing billions of facts. These datasets are available in different formats such as raw data dumps and HDT files or directly accessible via SPARQL endpoints. Querying such large amount of distributed data is particularly challenging and many of these datasets cannot be directly queried using the SPARQL query language. In order to tackle these problems, we present WimuQ, an integrated query engine to execute SPARQL queries and retrieve results from large amount of heterogeneous RDF data sources. Presently, WimuQ is able to execute both federated and non-federated SPARQL queries over a total of 668,166 datasets from LOD Stats and LOD Laudromat as well as 559 active SPARQL endpoints. These data sources represent a total of 221.7 billion triples from more than 5 terabytes of information from datasets retrieved using the service ""Where is My URI"" (WIMU). Our evaluation on state-of-the-art real-data benchmarks shows that WimuQ retrieves more complete results for the benchmark queries.",what Material ?,These data sources,,False,False
"The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster.",what Material ?,data,data sets,False,True
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,these technologies,modern information and communication technologies,False,False
"It was recently reported that men self-cite >50% more often than women across a wide variety of disciplines in the bibliographic database JSTOR. Here, we replicate this finding in a sample of 1.6 million papers from Author-ity, a version of PubMed with computationally disambiguated author names. More importantly, we show that the gender effect largely disappears when accounting for prior publication count in a multidimensional statistical model. Gender has the weakest effect on the probability of self-citation among an extensive set of features tested, including byline position, affiliation, ethnicity, collaboration size, time lag, subject-matter novelty, reference/citation counts, publication type, language, and venue. We find that self-citation is the hallmark of productive authors, of any gender, who cite their novel journal publications early and in similar venues, and more often cross citation-barriers such as language and indexing. As a result, papers by authors with short, disrupted, or diverse careers miss out on the initial boost in visibility gained from self-citations. Our data further suggest that this disproportionately affects women because of attrition and not because of disciplinary under-specialization.",what Material ?,a multidimensional statistical model,novel journal publications,False,False
"Background The COVID-19 outbreak has affected the lives of millions of people by causing a dramatic impact on many health care systems and the global economy. This devastating pandemic has brought together communities across the globe to work on this issue in an unprecedented manner. Objective This case study describes the steps and methods employed in the conduction of a remote online health hackathon centered on challenges posed by the COVID-19 pandemic. It aims to deliver a clear implementation road map for other organizations to follow. Methods This 4-day hackathon was conducted in April 2020, based on six COVID-19–related challenges defined by frontline clinicians and researchers from various disciplines. An online survey was structured to assess: (1) individual experience satisfaction, (2) level of interprofessional skills exchange, (3) maturity of the projects realized, and (4) overall quality of the event. At the end of the event, participants were invited to take part in an online survey with 17 (+5 optional) items, including multiple-choice and open-ended questions that assessed their experience regarding the remote nature of the event and their individual project, interprofessional skills exchange, and their confidence in working on a digital health project before and after the hackathon. Mentors, who guided the participants through the event, also provided feedback to the organizers through an online survey. Results A total of 48 participants and 52 mentors based in 8 different countries participated and developed 14 projects. A total of 75 mentorship video sessions were held. Participants reported increased confidence in starting a digital health venture or a research project after successfully participating in the hackathon, and stated that they were likely to continue working on their projects. Of the participants who provided feedback, 60% (n=18) would not have started their project without this particular hackathon and indicated that the hackathon encouraged and enabled them to progress faster, for example, by building interdisciplinary teams, gaining new insights and feedback provided by their mentors, and creating a functional prototype. Conclusions This study provides insights into how online hackathons can contribute to solving the challenges and effects of a pandemic in several regions of the world. The online format fosters team diversity, increases cross-regional collaboration, and can be executed much faster and at lower costs compared to in-person events. Results on preparation, organization, and evaluation of this online hackathon are useful for other institutions and initiatives that are willing to introduce similar event formats in the fight against COVID-19.",what Material ?,frontline clinicians and researchers,,False,False
"Abstract Background Data papers have emerged as a powerful instrument for open data publishing, obtaining credit, and establishing priority for datasets generated in scientific experiments. Academic publishing improves data and metadata quality through peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. Objective We aimed to establish a new type of article structure and template for omics studies: the omics data paper. To improve data interoperability and further incentivize researchers to publish well-described datasets, we created a prototype workflow for streamlined import of genomics metadata from the European Nucleotide Archive directly into a data paper manuscript. Methods An omics data paper template was designed by defining key article sections that encourage the description of omics datasets and methodologies. A metadata import workflow, based on REpresentational State Transfer services and Xpath, was prototyped to extract information from the European Nucleotide Archive, ArrayExpress, and BioSamples databases. Findings The template and workflow for automatic import of standard-compliant metadata into an omics data paper manuscript provide a mechanism for enhancing existing metadata through publishing. Conclusion The omics data paper structure and workflow for import of genomics metadata will help to bring genomic and other omics datasets into the spotlight. Promoting enhanced metadata descriptions and enforcing manuscript peer review and data auditing of the underlying datasets brings additional quality to datasets. We hope that streamlined metadata reuse for scholarly publishing encourages authors to create enhanced metadata descriptions in the form of data papers to improve both the quality of their metadata and its findability and accessibility.",what Material ?,Data papers,genomics metadata,False,False
"Over the last years, the Web of Data has grown significantly. Various interfaces such as LOD Stats, LOD Laudromat, SPARQL endpoints provide access to the hundered of thousands of RDF datasets, representing billions of facts. These datasets are available in different formats such as raw data dumps and HDT files or directly accessible via SPARQL endpoints. Querying such large amount of distributed data is particularly challenging and many of these datasets cannot be directly queried using the SPARQL query language. In order to tackle these problems, we present WimuQ, an integrated query engine to execute SPARQL queries and retrieve results from large amount of heterogeneous RDF data sources. Presently, WimuQ is able to execute both federated and non-federated SPARQL queries over a total of 668,166 datasets from LOD Stats and LOD Laudromat as well as 559 active SPARQL endpoints. These data sources represent a total of 221.7 billion triples from more than 5 terabytes of information from datasets retrieved using the service ""Where is My URI"" (WIMU). Our evaluation on state-of-the-art real-data benchmarks shows that WimuQ retrieves more complete results for the benchmark queries.",what Material ?,SPARQL endpoints,,False,False
"Abstract Presently, analytics degree programs exhibit a growing trend to meet a strong market demand. To explore the skill sets required for analytics positions, the authors examined a sample of online job postings related to professions such as business analyst (BA), business intelligence analyst (BIA), data analyst (DA), and data scientist (DS) using content analysis. They present a ranked list of relevant skills belonging to specific skills categories for the studied positions. Also, they conducted a pairwise comparison between DA and DS as well as BA and BIA. Overall, the authors observed that decision making, organization, communication, and structured data management are key to all job categories. The analysis shows that technical skills like statistics and programming skills are in most demand for DAs. The analysis is useful for creating clear definitions with respect to required skills for job categories in the business and data analytics domain and for designing course curricula for this domain.",what Material ?,sample of online job postings,technical skills like statistics,False,False
"Hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. The need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the Open Biodiversity Knowledge Management System (OBKMS). This paper presents OpenBiodiv: An OBKMS that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. It is presented as a Linked Open Dataset generated from scientific literature. OpenBiodiv encompasses data extracted from more than 5000 scholarly articles published by Pensoft and many more taxonomic treatments extracted by Plazi from journals of other publishers. The data from both sources are converted to Resource Description Framework (RDF) and integrated in a graph database using the OpenBiodiv-O ontology and an RDF version of the Global Biodiversity Information Facility (GBIF) taxonomic backbone. Through the application of semantic technologies, the project showcases the value of open publishing of Findable, Accessible, Interoperable, Reusable (FAIR) data towards the establishment of open science practices in the biodiversity domain.",what Material ?,taxonomic treatments,"published articles or monographs. the need for a system to store and manage collective biodiversity knowledge in a community - agreed and interoperable open format has evolved into the concept of the open biodiversity knowledge management system ( obkms ). this paper presents openbiodiv : an obkms that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. it is presented as a linked open dataset generated from scientific literature. openbiodiv encompasses data extracted from more than 5000 scholarly articles published by pensoft and many more taxonomic treatments",False,True
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,modern information and communication technologies,modern information and communication technologies,True,True
"Over the last years, the Web of Data has grown significantly. Various interfaces such as LOD Stats, LOD Laudromat, SPARQL endpoints provide access to the hundered of thousands of RDF datasets, representing billions of facts. These datasets are available in different formats such as raw data dumps and HDT files or directly accessible via SPARQL endpoints. Querying such large amount of distributed data is particularly challenging and many of these datasets cannot be directly queried using the SPARQL query language. In order to tackle these problems, we present WimuQ, an integrated query engine to execute SPARQL queries and retrieve results from large amount of heterogeneous RDF data sources. Presently, WimuQ is able to execute both federated and non-federated SPARQL queries over a total of 668,166 datasets from LOD Stats and LOD Laudromat as well as 559 active SPARQL endpoints. These data sources represent a total of 221.7 billion triples from more than 5 terabytes of information from datasets retrieved using the service ""Where is My URI"" (WIMU). Our evaluation on state-of-the-art real-data benchmarks shows that WimuQ retrieves more complete results for the benchmark queries.",what Material ?,the SPARQL query language,,False,False
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Material ?,rats,,False,False
"With the rapid growth of online social media content, and the impact these have made on people’s behavior, many researchers have been interested in studying these media platforms. A major part of their work focused on sentiment analysis and opinion mining. These refer to the automatic identification of opinions of people toward specific topics by analyzing their posts and publications. Multi-class sentiment analysis, in particular, addresses the identification of the exact sentiment conveyed by the user rather than the overall sentiment polarity of his text message or post. That being the case, we introduce a task different from the conventional multi-class classification, which we run on a data set collected from Twitter. We refer to this task as “quantification.” By the term “quantification,” we mean the identification of all the existing sentiments within an online post (i.e., tweet) instead of attributing a single sentiment label to it. For this sake, we propose an approach that automatically attributes different scores to each sentiment in a tweet, and selects the sentiments with the highest scores which we judge as conveyed in the text. To reach this target, we added to our previously introduced tool SENTA the necessary components to run and perform such a task. Throughout this work, we present the added components; we study the feasibility of quantification, and propose an approach to perform it on a data set made of tweets for 11 different sentiment classes. The data set was manually labeled and the results of the automatic analysis were checked against the human annotation. Our experiments show the feasibility of this task and reach an F1 score equal to 45.9%.",what Material ?,necessary components,"social media content,",False,False
"With the rapid growth of online social media content, and the impact these have made on people’s behavior, many researchers have been interested in studying these media platforms. A major part of their work focused on sentiment analysis and opinion mining. These refer to the automatic identification of opinions of people toward specific topics by analyzing their posts and publications. Multi-class sentiment analysis, in particular, addresses the identification of the exact sentiment conveyed by the user rather than the overall sentiment polarity of his text message or post. That being the case, we introduce a task different from the conventional multi-class classification, which we run on a data set collected from Twitter. We refer to this task as “quantification.” By the term “quantification,” we mean the identification of all the existing sentiments within an online post (i.e., tweet) instead of attributing a single sentiment label to it. For this sake, we propose an approach that automatically attributes different scores to each sentiment in a tweet, and selects the sentiments with the highest scores which we judge as conveyed in the text. To reach this target, we added to our previously introduced tool SENTA the necessary components to run and perform such a task. Throughout this work, we present the added components; we study the feasibility of quantification, and propose an approach to perform it on a data set made of tweets for 11 different sentiment classes. The data set was manually labeled and the results of the automatic analysis were checked against the human annotation. Our experiments show the feasibility of this task and reach an F1 score equal to 45.9%.",what Material ?,text,"social media content,",False,False
"<jats:p>This Editorial describes the rationale, focus, scope and technology behind the newly launched, open access, innovative Food Modelling Journal (FMJ). The Journal is designed to publish those outputs of the research cycle that usually precede the publication of the research article, but have their own value and re-usability potential. Such outputs are methods, models, software and data. The Food Modelling Journal is launched by the AGINFRA+ community and is integrated with the AGINFRA+ Virtual Research Environment (VRE) to facilitate and streamline the authoring, peer review and publication of the manuscripts via the ARPHA Publishing Platform.</jats:p>",what Material ?,software,"models, software and data.",False,True
"Colloidal nanocrystals (NCs) of APbX3-type lead halide perovskites [A = Cs+, CH3NH3+ (methylammonium or MA+) or CH(NH2)2+ (formamidinium or FA+); X = Cl–, Br–, I–] have recently emerged as highly versatile photonic sources for applications ranging from simple photoluminescence down-conversion (e.g., for display backlighting) to light-emitting diodes. From the perspective of spectral coverage, a formidable challenge facing the use of these materials is how to obtain stable emissions in the red and infrared spectral regions covered by the iodide-based compositions. So far, red-emissive CsPbI3 NCs have been shown to suffer from a delayed phase transformation into a nonluminescent, wide-band-gap 1D polymorph, and MAPbI3 exhibits very limited chemical durability. In this work, we report a facile colloidal synthesis method for obtaining FAPbI3 and FA-doped CsPbI3 NCs that are uniform in size (10–15 nm) and nearly cubic in shape and exhibit drastically higher robustness than their MA- or Cs-only cousins with similar sizes and morphologies. Detailed structural analysis indicated that the FAPbI3 NCs had a cubic crystal structure, while the FA0.1Cs0.9PbI3 NCs had a 3D orthorhombic structure that was isostructural to the structure of CsPbBr3 NCs. Bright photoluminescence (PL) with high quantum yield (QY > 70%) spanning red (690 nm, FA0.1Cs0.9PbI3 NCs) and near-infrared (near-IR, ca. 780 nm, FAPbI3 NCs) regions was sustained for several months or more in both the colloidal state and in films. The peak PL wavelengths can be fine-tuned by using postsynthetic cation- and anion-exchange reactions. Amplified spontaneous emissions with low thresholds of 28 and 7.5 μJ cm–2 were obtained from the films deposited from FA0.1Cs0.9PbI3 and FAPbI3 NCs, respectively. Furthermore, light-emitting diodes with a high external quantum efficiency of 2.3% were obtained by using FAPbI3 NCs.",what Material ?,Colloidal nanocrystals (NCs) of APbX3-type lead halide perovskites,colloidal nanocrystals,False,False
"<jats:p>Despite being a challenging research field with many unresolved problems, recommender systems are getting more popular in recent years. These systems rely on the personal preferences of users on items given in the form of ratings and return the preferable items based on choices of like-minded users. In this study, a graph-based recommender system using link prediction techniques incorporating similarity metrics is proposed. A graph-based recommender system that has ratings of users on items can be represented as a bipartite graph, where vertices correspond to users and items and edges to ratings. Recommendation generation in a bipartite graph is a link prediction problem. In current literature, modified link prediction approaches are used to distinguish between fundamental relational dualities of like vs. dislike and similar vs. dissimilar. However, the similarity relationship between users/items is mostly disregarded in the complex domain. The proposed model utilizes user-user and item-item cosine similarity value with the relational dualities in order to improve coverage and hits rate of the system by carefully incorporating similarities. On the standard MovieLens Hetrec and MovieLens datasets, the proposed similarity-inclusive link prediction method performed empirically well compared to other methods operating in the complex domain. The experimental results show that the proposed recommender system can be a plausible alternative to overcome the deficiencies in recommender systems.</jats:p>",what Material ?,MovieLens,personal preferences of users,False,False
<p>We introduce a solution-processed copper tin sulfide (CTS) thin film to realize high-performance of thin-film transistors (TFT) by optimizing the CTS precursor solution concentration.</p>,what Material ?,Copper tin sulfide (CTS) thin film,copper tin sulfide ( cts ) thin film,False,False
"Hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. The need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the Open Biodiversity Knowledge Management System (OBKMS). This paper presents OpenBiodiv: An OBKMS that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. It is presented as a Linked Open Dataset generated from scientific literature. OpenBiodiv encompasses data extracted from more than 5000 scholarly articles published by Pensoft and many more taxonomic treatments extracted by Plazi from journals of other publishers. The data from both sources are converted to Resource Description Framework (RDF) and integrated in a graph database using the OpenBiodiv-O ontology and an RDF version of the Global Biodiversity Information Facility (GBIF) taxonomic backbone. Through the application of semantic technologies, the project showcases the value of open publishing of Findable, Accessible, Interoperable, Reusable (FAIR) data towards the establishment of open science practices in the biodiversity domain.",what Material ?,communal knowledge,"published articles or monographs. the need for a system to store and manage collective biodiversity knowledge in a community - agreed and interoperable open format has evolved into the concept of the open biodiversity knowledge management system ( obkms ). this paper presents openbiodiv : an obkms that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. it is presented as a linked open dataset generated from scientific literature. openbiodiv encompasses data extracted from more than 5000 scholarly articles published by pensoft and many more taxonomic treatments",False,False
"The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster.",what Material ?,RDF data,data sets,False,False
"Colloidal nanocrystals (NCs) of APbX3-type lead halide perovskites [A = Cs+, CH3NH3+ (methylammonium or MA+) or CH(NH2)2+ (formamidinium or FA+); X = Cl–, Br–, I–] have recently emerged as highly versatile photonic sources for applications ranging from simple photoluminescence down-conversion (e.g., for display backlighting) to light-emitting diodes. From the perspective of spectral coverage, a formidable challenge facing the use of these materials is how to obtain stable emissions in the red and infrared spectral regions covered by the iodide-based compositions. So far, red-emissive CsPbI3 NCs have been shown to suffer from a delayed phase transformation into a nonluminescent, wide-band-gap 1D polymorph, and MAPbI3 exhibits very limited chemical durability. In this work, we report a facile colloidal synthesis method for obtaining FAPbI3 and FA-doped CsPbI3 NCs that are uniform in size (10–15 nm) and nearly cubic in shape and exhibit drastically higher robustness than their MA- or Cs-only cousins with similar sizes and morphologies. Detailed structural analysis indicated that the FAPbI3 NCs had a cubic crystal structure, while the FA0.1Cs0.9PbI3 NCs had a 3D orthorhombic structure that was isostructural to the structure of CsPbBr3 NCs. Bright photoluminescence (PL) with high quantum yield (QY > 70%) spanning red (690 nm, FA0.1Cs0.9PbI3 NCs) and near-infrared (near-IR, ca. 780 nm, FAPbI3 NCs) regions was sustained for several months or more in both the colloidal state and in films. The peak PL wavelengths can be fine-tuned by using postsynthetic cation- and anion-exchange reactions. Amplified spontaneous emissions with low thresholds of 28 and 7.5 μJ cm–2 were obtained from the films deposited from FA0.1Cs0.9PbI3 and FAPbI3 NCs, respectively. Furthermore, light-emitting diodes with a high external quantum efficiency of 2.3% were obtained by using FAPbI3 NCs.",what Material ?,light-emitting diodes,colloidal nanocrystals,False,False
"Most question answering (QA) systems over Linked Data, i.e. Knowledge Graphs, approach the question answering task as a conversion from a natural language question to its corresponding SPARQL query. A common approach is to use query templates to generate SPARQL queries with slots that need to be filled. Using templates instead of running an extensive NLP pipeline or end-to-end model shifts the QA problem into a classification task, where the system needs to match the input question to the appropriate template. This paper presents an approach to automatically learn and classify natural language questions into corresponding templates using recursive neural networks. Our model was trained on 5000 questions and their respective SPARQL queries from the preexisting LC-QuAD dataset grounded in DBpedia, spanning 5042 entities and 615 predicates. The resulting model was evaluated using the FAIR GERBIL QA framework resulting in 0.419 macro f-measure on LC-QuAD and 0.417 macro f-measure on QALD-7.",what Material ?,LC-QuAD dataset,,False,False
"Entity linking has recently been the subject of a significant body of research. Currently, the best performing approaches rely on trained mono-lingual models. Porting these approaches to other languages is consequently a difficult endeavor as it requires corresponding training data and retraining of the models. We address this drawback by presenting a novel multilingual, knowledge-base agnostic and deterministic approach to entity linking, dubbed MAG. MAG is based on a combination of context-based retrieval on structured knowledge bases and graph algorithms. We evaluate MAG on 23 data sets and in 7 languages. Our results show that the best approach trained on English datasets (PBOH) achieves a micro F-measure that is up to 4 times worse on datasets in other languages. MAG on the other hand achieves state-of-the-art performance on English datasets and reaches a micro F-measure that is up to 0.6 higher than that of PBOH on non-English languages.",what Material ?,7 languages,structured knowledge bases and graph algorithms. we evaluate mag on 23 data sets,False,False
"The integration of different datasets in the Linked Data Cloud is a key aspect to the success of the Web of Data. To tackle this problem most of existent solutions have been supported by the task of entity resolution. However, many challenges still prevail specially when considering different types, structures and vocabularies used in the Web. Another common problem is that data usually are incomplete, inconsistent and contain outliers. To overcome these limitations, some works have applied machine learning algorithms since they are typically robust to both noise and data inconsistencies and are able to efficiently utilize nondeterministic dependencies in the data. In this paper we propose an approach based in a relational learning algorithm that addresses the problem by statistical approximation method. Modeling the problem as a relational machine learning task allows exploit contextual information that might be too distant in the relational graph. The joint application of relationship patterns between entities and evidences of similarity between their descriptions can improve the effectiveness of results. Furthermore, it is based on a sparse structure that scales well to large datasets. We present initial experiments based on BTC2012 datasets.",what Material ?,Linked Data Cloud,sparse structure,False,False
"Colloidal nanocrystals (NCs) of APbX3-type lead halide perovskites [A = Cs+, CH3NH3+ (methylammonium or MA+) or CH(NH2)2+ (formamidinium or FA+); X = Cl–, Br–, I–] have recently emerged as highly versatile photonic sources for applications ranging from simple photoluminescence down-conversion (e.g., for display backlighting) to light-emitting diodes. From the perspective of spectral coverage, a formidable challenge facing the use of these materials is how to obtain stable emissions in the red and infrared spectral regions covered by the iodide-based compositions. So far, red-emissive CsPbI3 NCs have been shown to suffer from a delayed phase transformation into a nonluminescent, wide-band-gap 1D polymorph, and MAPbI3 exhibits very limited chemical durability. In this work, we report a facile colloidal synthesis method for obtaining FAPbI3 and FA-doped CsPbI3 NCs that are uniform in size (10–15 nm) and nearly cubic in shape and exhibit drastically higher robustness than their MA- or Cs-only cousins with similar sizes and morphologies. Detailed structural analysis indicated that the FAPbI3 NCs had a cubic crystal structure, while the FA0.1Cs0.9PbI3 NCs had a 3D orthorhombic structure that was isostructural to the structure of CsPbBr3 NCs. Bright photoluminescence (PL) with high quantum yield (QY > 70%) spanning red (690 nm, FA0.1Cs0.9PbI3 NCs) and near-infrared (near-IR, ca. 780 nm, FAPbI3 NCs) regions was sustained for several months or more in both the colloidal state and in films. The peak PL wavelengths can be fine-tuned by using postsynthetic cation- and anion-exchange reactions. Amplified spontaneous emissions with low thresholds of 28 and 7.5 μJ cm–2 were obtained from the films deposited from FA0.1Cs0.9PbI3 and FAPbI3 NCs, respectively. Furthermore, light-emitting diodes with a high external quantum efficiency of 2.3% were obtained by using FAPbI3 NCs.",what Material ?,"780 nm, FAPbI3 NCs",colloidal nanocrystals,False,False
"Interpreting observational data is a fundamental task in the sciences, specifically in earth and environmental science where observational data are increasingly acquired, curated, and published systematically by environmental research infrastructures. Typically subject to substantial processing, observational data are used by research communities, their research groups and individual scientists, who interpret such primary data for their meaning in the context of research investigations. The result of interpretation is information – meaningful secondary or derived data – about the observed environment. Research infrastructures and research communities are thus essential to evolving uninterpreted observational data to information. In digital form, the classical bearer of information are the commonly known “(elaborated) data products,” for instance maps. In such form, meaning is generally implicit e.g., in map colour coding, and thus largely inaccessible to machines. The systematic acquisition, curation, possible publishing and further processing of information gained in observational data interpretation – as machine readable data and their machine-readable meaning – is not common practice among environmental research infrastructures. For a use case in aerosol science, we elucidate these problems and present a Jupyter based prototype infrastructure that exploits a machine learning approach to interpretation and could support a research community in interpreting observational data and, more importantly, in curating and further using resulting information about a studied natural phenomenon.",what Material ?,Research infrastructures and research communities,primary data for their meaning in the context of research investigations. the result of interpretation is information – meaningful secondary or derived data –,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,our relational process model,modern information and communication technologies,False,False
"Knowledge bases (KBs), pragmatic collections of knowledge about notable entities, are an important asset in applications such as search, question answering and dialogue. Rooted in a long tradition in knowledge representation, all popular KBs only store positive information, but abstain from taking any stance towards statements not contained in them. In this paper, we make the case for explicitly stating interesting statements which are not true. Negative statements would be important to overcome current limitations of question answering, yet due to their potential abundance, any effort towards compiling them needs a tight coupling with ranking. We introduce two approaches towards automatically compiling negative statements. (i) In peer-based statistical inferences, we compare entities with highly related entities in order to derive potential negative statements, which we then rank using supervised and unsupervised features. (ii) In pattern-based query log extraction, we use a pattern-based approach for harvesting search engine query logs. Experimental results show that both approaches hold promising and complementary potential. Along with this paper, we publish the first datasets on interesting negative information, containing over 1.4M statements for 130K popular Wikidata entities.",what Material ?,Knowledge bases (KBs),"notable entities,",False,False
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Material ?,Group A (control),,False,False
"Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.",what Material ?,OntoNotes 5.0 dataset,text,False,False
"While Wikipedia exists in 287 languages, its content is unevenly distributed among them. In this work, we investigate the generation of open domain Wikipedia summaries in underserved languages using structured data from Wikidata. To this end, we propose a neural network architecture equipped with copy actions that learns to generate single-sentence and comprehensible textual summaries from Wikidata triples. We demonstrate the effectiveness of the proposed approach by evaluating it against a set of baselines on two languages of different natures: Arabic, a morphological rich language with a larger vocabulary than English, and Esperanto, a constructed language known for its easy acquisition.",what Material ?,a neural network architecture,structured data from wikidata.,False,False
"Science communication only reaches certain segments of society. Various underserved audiences are detached from it and feel left out, which is a challenge for democratic societies that build on informed participation in deliberative processes. While only recently researchers and practitioners have addressed the question on the detailed composition of the not reached groups, even less is known about the emotional impact on underserved audiences: feelings and emotions can play an important role in how science communication is received, and “feeling left out” can be an important aspect of exclusion. In this exploratory study, we provide insights from interviews and focus groups with three different underserved audiences in Germany. We found that on the one hand, material exclusion factors such as available infrastructure or financial means as well as specifically attributable factors such as language skills, are influencing the audience composition of science communication. On the other hand, emotional exclusion factors such as fear, habitual distance, and self- as well as outside-perception also play an important role. Therefore, simply addressing material aspects can only be part of establishing more inclusive science communication practices. Rather, being aware of emotions and feelings can serve as a point of leverage for science communication in reaching out to underserved audiences.",what Material ?,underserved audiences,feelings and emotions,False,False
"<jats:p>This paper discusses the potential of current advancements in Information Communication Technologies (ICT) for cultural heritage preservation, valorization and management within contemporary cities. The paper highlights the potential of virtual environments to assess the impacts of heritage policies on urban development. It does so by discussing the implications of virtual globes and crowdsourcing to support the participatory valuation and management of cultural heritage assets. To this purpose, a review of available valuation techniques is here presented together with a discussion on how these techniques might be coupled with ICT tools to promote inclusive governance. </jats:p>",what Material ?,contemporary cities,virtual environments,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,a building,modern information and communication technologies,False,False
"Background Visual atypicalities in autism spectrum disorder (ASD) are a well documented phenomenon, beginning as early as 2–6 months of age and manifesting in a significantly decreased attention to the eyes, direct gaze and socially salient information. Early emerging neurobiological deficits in perceiving social stimuli as rewarding or its active avoidance due to the anxiety it entails have been widely purported as potential reasons for this atypicality. Parallel research evidence also points to the significant benefits of animal presence for reducing social anxiety and enhancing social interaction in children with autism. While atypicality in social attention in ASD has been widely substantiated, whether this atypicality persists equally across species types or is confined to humans has not been a key focus of research insofar. Methods We attempted a comprehensive examination of the differences in visual attention to static images of human and animal faces (40 images; 20 human faces and 20 animal faces) among children with ASD using an eye tracking paradigm. 44 children (ASD n = 21; TD n = 23) participated in the study (10,362 valid observations) across five regions of interest (left eye, right eye, eye region, face and screen). Results Results obtained revealed significantly greater social attention across human and animal stimuli in typical controls when compared to children with ASD. However in children with ASD, a significantly greater attention allocation was seen to animal faces and eye region and lesser attention to the animal mouth when compared to human faces, indicative of a clear attentional preference to socially salient regions of animal stimuli. The positive attentional bias toward animals was also seen in terms of a significantly greater visual attention to direct gaze in animal images. Conclusion Our results suggest the possibility that atypicalities in social attention in ASD may not be uniform across species. It adds to the current neural and biomarker evidence base of the potentially greater social reward processing and lesser social anxiety underlying animal stimuli as compared to human stimuli in children with ASD.",what Material ?,"five regions of interest (left eye, right eye, eye region, face and screen)",static images of human and animal faces,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,different technical disciplines,modern information and communication technologies,False,False
"ABSTRACT ‘Heritage Interpretation’ has always been considered as an effective learning, communication and management tool that increases visitors’ awareness of and empathy to heritage sites or artefacts. Yet the definition of ‘digital heritage interpretation’ is still wide and so far, no significant method and objective are evident within the domain of ‘digital heritage’ theory and discourse. Considering ‘digital heritage interpretation’ as a process rather than as a tool to present or communicate with end-users, this paper presents a critical application of a theoretical construct ascertained from multiple disciplines and explicates four objectives for a comprehensive interpretive process. A conceptual model is proposed and further developed into a conceptual framework with fifteen considerations. This framework is then implemented and tested on an online platform to assess its impact on end-users’ interpretation level. We believe the presented interpretive framework (PrEDiC) will help heritage professionals and media designers to develop interpretive heritage project.",what Material ?,tool,,False,False
"While Wikipedia exists in 287 languages, its content is unevenly distributed among them. In this work, we investigate the generation of open domain Wikipedia summaries in underserved languages using structured data from Wikidata. To this end, we propose a neural network architecture equipped with copy actions that learns to generate single-sentence and comprehensible textual summaries from Wikidata triples. We demonstrate the effectiveness of the proposed approach by evaluating it against a set of baselines on two languages of different natures: Arabic, a morphological rich language with a larger vocabulary than English, and Esperanto, a constructed language known for its easy acquisition.",what Material ?,underserved languages,structured data from wikidata.,False,False
"Abstract Objective To evaluate viral loads at different stages of disease progression in patients infected with the 2019 severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) during the first four months of the epidemic in Zhejiang province, China. Design Retrospective cohort study. Setting A designated hospital for patients with covid-19 in Zhejiang province, China. Participants 96 consecutively admitted patients with laboratory confirmed SARS-CoV-2 infection: 22 with mild disease and 74 with severe disease. Data were collected from 19 January 2020 to 20 March 2020. Main outcome measures Ribonucleic acid (RNA) viral load measured in respiratory, stool, serum, and urine samples. Cycle threshold values, a measure of nucleic acid concentration, were plotted onto the standard curve constructed on the basis of the standard product. Epidemiological, clinical, and laboratory characteristics and treatment and outcomes data were obtained through data collection forms from electronic medical records, and the relation between clinical data and disease severity was analysed. Results 3497 respiratory, stool, serum, and urine samples were collected from patients after admission and evaluated for SARS-CoV-2 RNA viral load. Infection was confirmed in all patients by testing sputum and saliva samples. RNA was detected in the stool of 55 (59%) patients and in the serum of 39 (41%) patients. The urine sample from one patient was positive for SARS-CoV-2. The median duration of virus in stool (22 days, interquartile range 17-31 days) was significantly longer than in respiratory (18 days, 13-29 days; P=0.02) and serum samples (16 days, 11-21 days; P<0.001). The median duration of virus in the respiratory samples of patients with severe disease (21 days, 14-30 days) was significantly longer than in patients with mild disease (14 days, 10-21 days; P=0.04). In the mild group, the viral loads peaked in respiratory samples in the second week from disease onset, whereas viral load continued to be high during the third week in the severe group. Virus duration was longer in patients older than 60 years and in male patients. Conclusion The duration of SARS-CoV-2 is significantly longer in stool samples than in respiratory and serum samples, highlighting the need to strengthen the management of stool samples in the prevention and control of the epidemic, and the virus persists longer with higher load and peaks later in the respiratory tissue of patients with severe disease.",what Material ?,serum sample,,False,False
"<jats:p>This Editorial describes the rationale, focus, scope and technology behind the newly launched, open access, innovative Food Modelling Journal (FMJ). The Journal is designed to publish those outputs of the research cycle that usually precede the publication of the research article, but have their own value and re-usability potential. Such outputs are methods, models, software and data. The Food Modelling Journal is launched by the AGINFRA+ community and is integrated with the AGINFRA+ Virtual Research Environment (VRE) to facilitate and streamline the authoring, peer review and publication of the manuscripts via the ARPHA Publishing Platform.</jats:p>",what Material ?,data,"models, software and data.",False,True
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,a consistent mathematical process model,modern information and communication technologies,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,these technologies,modern information and communication technologies,False,False
"Hybrid halide perovskites that are currently intensively studied for photovoltaic applications, also present outstanding properties for light emission. Here, we report on the preparation of bright solid state light emitting diodes (LEDs) based on a solution-processed hybrid lead halide perovskite (Pe). In particular, we have utilized the perovskite generally described with the formula CH3NH3PbI(3-x)Cl(x) and exploited a configuration without electron or hole blocking layer in addition to the injecting layers. Compact TiO2 and Spiro-OMeTAD were used as electron and hole injecting layers, respectively. We have demonstrated a bright combined visible-infrared radiance of 7.1 W·sr(-1)·m(-2) at a current density of 232 mA·cm(-2), and a maximum external quantum efficiency (EQE) of 0.48%. The devices prepared surpass the EQE values achieved in previous reports, considering devices with just an injecting layer without any additional blocking layer. Significantly, the maximum EQE value of our devices is obtained at applied voltages as low as 2 V, with a turn-on voltage as low as the Pe band gap (V(turn-on) = 1.45 ± 0.06 V). This outstanding performance, despite the simplicity of the approach, highlights the enormous potentiality of Pe-LEDs. In addition, we present a stability study of unsealed Pe-LEDs, which demonstrates a dramatic influence of the measurement atmosphere on the performance of the devices. The decrease of the electroluminescence (EL) under continuous operation can be attributed to an increase of the non-radiative recombination pathways, rather than a degradation of the perovskite material itself.",what Material ?,non-radiative recombination pathways,"unsealed pe - leds,",False,False
"The molecular chaperone Hsp90-dependent proteome represents a complex protein network of critical biological and medical relevance. Known to associate with proteins with a broad variety of functions termed clients, Hsp90 maintains key essential and oncogenic signalling pathways. Consequently, Hsp90 inhibitors are being tested as anti-cancer drugs. Using an integrated systematic approach to analyse the effects of Hsp90 inhibition in T-cells, we quantified differential changes in the Hsp90-dependent proteome, Hsp90 interactome, and a selection of the transcriptome. Kinetic behaviours in the Hsp90-dependent proteome were assessed using a novel pulse-chase strategy (Fierro-Monti et al., accompanying article), detecting effects on both protein stability and synthesis. Global and specific dynamic impacts, including proteostatic responses, are due to direct inhibition of Hsp90 as well as indirect effects. As a result, a decrease was detected in most proteins that changed their levels, including known Hsp90 clients. Most likely, consequences of the role of Hsp90 in gene expression determined a global reduction in net de novo protein synthesis. This decrease appeared to be greater in magnitude than a concomitantly observed global increase in protein decay rates. Several novel putative Hsp90 clients were validated, and interestingly, protein families with critical functions, particularly the Hsp90 family and cofactors themselves as well as protein kinases, displayed strongly increased decay rates due to Hsp90 inhibitor treatment. Remarkably, an upsurge in survival pathways, involving molecular chaperones and several oncoproteins, and decreased levels of some tumour suppressors, have implications for anti-cancer therapy with Hsp90 inhibitors. The diversity of global effects may represent a paradigm of mechanisms that are operating to shield cells from proteotoxic stress, by promoting pro-survival and anti-proliferative functions. Data are available via ProteomeXchange with identifier PXD000537.",what Material ?,protein families,"molecular chaperone hsp90 - dependent proteome represents a complex protein network of critical biological and medical relevance. known to associate with proteins with a broad variety of functions termed clients, hsp90 maintains key essential and oncogenic signalling pathways. consequently, hsp90 inhibitors are being tested as anti - cancer drugs. using an integrated systematic approach to analyse the effects of hsp90 inhibition in t - cells,",False,False
"To improve designs of e-learning materials, it is necessary to know which word or figure a learner felt ""difficult"" in the materials. In this pilot study, we measured electroencephalography (EEG) and eye gaze data of learners and analyzed to estimate which area they had difficulty to learn. The developed system realized simultaneous measurements of physiological data and subjective evaluations during learning. Using this system, we observed specific EEG activity in difficult pages. Integrating of eye gaze and EEG measurements raised a possibility to determine where a learner felt ""difficult"" in a page of learning materials. From these results, we could suggest that the multimodal measurements of EEG and eye gaze would lead to effective improvement of learning materials. For future study, more data collection using various materials and learners with different backgrounds is necessary. This study could lead to establishing a method to improve e-learning materials based on learners' mental states.",what Material ?,which word or figure a learner,difficult pages.,False,False
"A large community of research has been developed in recent years to analyze social media and social networks, with the aim of understanding, discovering insights, and exploiting the available information. The focus has shifted from conventional polarity classification to contemporary application-oriented fine-grained aspects such as, emotions, sarcasm, stance, rumor, and hate speech detection in the user-generated content. Detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. The majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. In this paper, we propose a deep learning model called sAtt-BLSTM convNet that is based on the hybrid of soft attention-based bidirectional long short-term memory (sAtt-BLSTM) and convolution neural network (convNet) applying global vectors for word representation (GLoVe) for building semantic word embeddings. In addition to the feature maps generated by the sAtt-BLSTM, punctuation-based auxiliary features are also merged into the convNet. The robustness of the proposed model is investigated using balanced (tweets from benchmark SemEval 2015 Task 11) and unbalanced (approximately 40000 random tweets using the Sarcasm Detector tool with 15000 sarcastic and 25000 non-sarcastic messages) datasets. An experimental study using the training- and test-set accuracy metrics is performed to compare the proposed deep neural model with convNet, LSTM, and bidirectional LSTM with/without attention and it is observed that the novel sAtt-BLSTM convNet model outperforms others with a superior sarcasm-classification accuracy of 97.87% for the Twitter dataset and 93.71% for the random-tweet dataset.",what Material ?,social media and social networks,social media,False,False
"The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster.",what Material ?,popular multi-node RDF data management systems,data sets,False,False
"It was recently reported that men self-cite >50% more often than women across a wide variety of disciplines in the bibliographic database JSTOR. Here, we replicate this finding in a sample of 1.6 million papers from Author-ity, a version of PubMed with computationally disambiguated author names. More importantly, we show that the gender effect largely disappears when accounting for prior publication count in a multidimensional statistical model. Gender has the weakest effect on the probability of self-citation among an extensive set of features tested, including byline position, affiliation, ethnicity, collaboration size, time lag, subject-matter novelty, reference/citation counts, publication type, language, and venue. We find that self-citation is the hallmark of productive authors, of any gender, who cite their novel journal publications early and in similar venues, and more often cross citation-barriers such as language and indexing. As a result, papers by authors with short, disrupted, or diverse careers miss out on the initial boost in visibility gained from self-citations. Our data further suggest that this disproportionately affects women because of attrition and not because of disciplinary under-specialization.",what Material ?,similar venues,novel journal publications,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,the project leader,modern information and communication technologies,False,False
"<jats:p>Neural-symbolic systems combine the strengths of neural networks and symbolic formalisms. In this paper, we introduce a neural-symbolic system which combines restricted Boltzmann machines and probabilistic semi-abstract argumentation. We propose to train networks on argument labellings explaining the data, so that any sampled data outcome is associated with an argument labelling. Argument labellings are integrated as constraints within restricted Boltzmann machines, so that the neural networks are used to learn probabilistic dependencies amongst argument labels. Given a dataset and an argumentation graph as prior knowledge, for every example/case K in the dataset, we use a so-called K-maxconsistent labelling of the graph, and an explanation of case K refers to a K-maxconsistent labelling of the given argumentation graph. The abilities of the proposed system to predict correct labellings were evaluated and compared with standard machine learning techniques. Experiments revealed that such argumentation Boltzmann machines can outperform other classification models, especially in noisy settings.</jats:p>",what Material ?,restricted boltzmann machines,"neural networks and symbolic formalisms. in this paper, we introduce a neural - symbolic system which combines restricted boltzmann machines and probabilistic semi - abstract argumentation. we propose to train networks on argument labellings explaining the data, so that any sampled data outcome is associated with an argument labelling. argument labellings are integrated as constraints within restricted boltzmann machines, so that the neural networks are used to learn probabilistic dependencies amongst argument labels. given a dataset",False,True
"Two-dimensional (2D) layered materials, such as MoS2, are greatly attractive for flexible devices due to their unique layered structures, novel physical and electronic properties, and high mechanical strength. However, their limited mechanical strains (<2%) can hardly meet the demands of loading conditions for most flexible and stretchable device applications. In this Article, inspired from Kirigami, the ancient Japanese art of paper cutting, we design and fabricate nanoscale Kirigami architectures of 2D layered MoS2 on a soft substrate of polydimethylsiloxane (PDMS) using a top-down fabrication process. Results show that the Kirigami structures significantly improve the reversible stretchability of flexible 2D MoS2 electronic devices, which is increased from 0.75% to ∼15%. This increase in flexibility is originated from a combination of multidimensional deformation capabilities from the nanoscale Kirigami architectures consisting of in-plane stretching and out-of-plane deformation. We further discover a ...",what Material ?,MoS2,,False,False
"Purpose – The purpose of this paper is to investigate what employers seek when recruiting library and information professionals in the UK and whether professional skills, generic skills or personal qualities are most in demand.Design/methodology/approach – A content analysis of a sample of 180 advertisements requiring a professional library or information qualification from Chartered Institute of Library and Information Professional's Library + Information Gazette over the period May 2006‐2007.Findings – The findings reveal that a multitude of skills and qualities are required in the profession. When the results were compared with Information National Training Organisation and Library and Information Management Employability Skills research, customer service, interpersonal and communication skills, and general computing skills emerged as the requirements most frequently sought by employers. Overall, requirements from the generic skills area were most important to employers, but the research also demonstra...",what Material ?,sample of 180 advertisements,180 advertisements,False,False
"Interpreting observational data is a fundamental task in the sciences, specifically in earth and environmental science where observational data are increasingly acquired, curated, and published systematically by environmental research infrastructures. Typically subject to substantial processing, observational data are used by research communities, their research groups and individual scientists, who interpret such primary data for their meaning in the context of research investigations. The result of interpretation is information – meaningful secondary or derived data – about the observed environment. Research infrastructures and research communities are thus essential to evolving uninterpreted observational data to information. In digital form, the classical bearer of information are the commonly known “(elaborated) data products,” for instance maps. In such form, meaning is generally implicit e.g., in map colour coding, and thus largely inaccessible to machines. The systematic acquisition, curation, possible publishing and further processing of information gained in observational data interpretation – as machine readable data and their machine-readable meaning – is not common practice among environmental research infrastructures. For a use case in aerosol science, we elucidate these problems and present a Jupyter based prototype infrastructure that exploits a machine learning approach to interpretation and could support a research community in interpreting observational data and, more importantly, in curating and further using resulting information about a studied natural phenomenon.",what Material ?,(elaborated) data products,primary data for their meaning in the context of research investigations. the result of interpretation is information – meaningful secondary or derived data –,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Material ?,the project leader,modern information and communication technologies,False,False
"Two-dimensional (2D) layered materials are ideal for micro- and nanoelectromechanical systems (MEMS/NEMS) due to their ultimate thinness. Platinum diselenide (PtSe2), an exciting and unexplored 2D transition metal dichalcogenide material, is particularly interesting because its low temperature growth process is scalable and compatible with silicon technology. Here, we report the potential of thin PtSe2 films as electromechanical piezoresistive sensors. All experiments have been conducted with semimetallic PtSe2 films grown by thermally assisted conversion of platinum at a complementary metal–oxide–semiconductor (CMOS)-compatible temperature of 400 °C. We report high negative gauge factors of up to −85 obtained experimentally from PtSe2 strain gauges in a bending cantilever beam setup. Integrated NEMS piezoresistive pressure sensors with freestanding PMMA/PtSe2 membranes confirm the negative gauge factor and exhibit very high sensitivity, outperforming previously reported values by orders of magnitude. We employ density functional theory calculations to understand the origin of the measured negative gauge factor. Our results suggest PtSe2 as a very promising candidate for future NEMS applications, including integration into CMOS production lines.",what Material ?,PtSe2,ptse2 films,False,True
"With the rapid growth of online social media content, and the impact these have made on people’s behavior, many researchers have been interested in studying these media platforms. A major part of their work focused on sentiment analysis and opinion mining. These refer to the automatic identification of opinions of people toward specific topics by analyzing their posts and publications. Multi-class sentiment analysis, in particular, addresses the identification of the exact sentiment conveyed by the user rather than the overall sentiment polarity of his text message or post. That being the case, we introduce a task different from the conventional multi-class classification, which we run on a data set collected from Twitter. We refer to this task as “quantification.” By the term “quantification,” we mean the identification of all the existing sentiments within an online post (i.e., tweet) instead of attributing a single sentiment label to it. For this sake, we propose an approach that automatically attributes different scores to each sentiment in a tweet, and selects the sentiments with the highest scores which we judge as conveyed in the text. To reach this target, we added to our previously introduced tool SENTA the necessary components to run and perform such a task. Throughout this work, we present the added components; we study the feasibility of quantification, and propose an approach to perform it on a data set made of tweets for 11 different sentiment classes. The data set was manually labeled and the results of the automatic analysis were checked against the human annotation. Our experiments show the feasibility of this task and reach an F1 score equal to 45.9%.",what Material ?,added components,"social media content,",False,False
"The method of phylogenetic ancestral sequence reconstruction is a powerful approach for studying evolutionary relationships among protein sequence, structure, and function. In particular, this approach allows investigators to (1) reconstruct and “resurrect” (that is, synthesize in vivo or in vitro) extinct proteins to study how they differ from modern proteins, (2) identify key amino acid changes that, over evolutionary timescales, have altered the function of the protein, and (3) order historical events in the evolution of protein function. Widespread use of this approach has been slow among molecular biologists, in part because the methods require significant computational expertise. Here we present PhyloBot, a web-based software tool that makes ancestral sequence reconstruction easy. Designed for non-experts, it integrates all the necessary software into a single user interface. Additionally, PhyloBot provides interactive tools to explore evolutionary trajectories between ancestors, enabling the rapid generation of hypotheses that can be tested using genetic or biochemical approaches. Early versions of this software were used in previous studies to discover genetic mechanisms underlying the functions of diverse protein families, including V-ATPase ion pumps, DNA-binding transcription regulators, and serine/threonine protein kinases. PhyloBot runs in a web browser, and is available at the following URL: http://www.phylobot.com. The software is implemented in Python using the Django web framework, and runs on elastic cloud computing resources from Amazon Web Services. Users can create and submit jobs on our free server (at the URL listed above), or use our open-source code to launch their own PhyloBot server.",what creates ?,PhyloBot,"phylobot,",True,True
"The rapidly expanding body of available genomic and protein structural data provides a rich resource for understanding protein dynamics with biomolecular simulation. While computational infrastructure has grown rapidly, simulations on an omics scale are not yet widespread, primarily because software infrastructure to enable simulations at this scale has not kept pace. It should now be possible to study protein dynamics across entire (super)families, exploiting both available structural biology data and conformational similarities across homologous proteins. Here, we present a new tool for enabling high-throughput simulation in the genomics era. Ensembler takes any set of sequences - from a single sequence to an entire superfamily - and shepherds them through various stages of modeling and refinement to produce simulation-ready structures. This includes comparative modeling to all relevant PDB structures (which may span multiple conformational states of interest), reconstruction of missing loops, addition of missing atoms, culling of nearly identical structures, assignment of appropriate protonation states, solvation in explicit solvent, and refinement and filtering with molecular simulation to ensure stable simulation. The output of this pipeline is an ensemble of structures ready for subsequent molecular simulations using computer clusters, supercomputers, or distributed computing projects like Folding@home. Ensembler thus automates much of the time-consuming process of preparing protein models suitable for simulation, while allowing scalability up to entire superfamilies. A particular advantage of this approach can be found in the construction of kinetic models of conformational dynamics - such as Markov state models (MSMs) - which benefit from a diverse array of initial configurations that span the accessible conformational states to aid sampling. We demonstrate the power of this approach by constructing models for all catalytic domains in the human tyrosine kinase family, using all available kinase catalytic domain structures from any organism as structural templates. Ensembler is free and open source software licensed under the GNU General Public License (GPL) v2. It is compatible with Linux and OS X. The latest release can be installed via the conda package manager, and the latest source can be downloaded from https://github.com/choderalab/ensembler.",what creates ?,Ensembler,ensembler,True,True
"The quantification of cell shape, cell migration, and cell rearrangements is important for addressing classical questions in developmental biology such as patterning and tissue morphogenesis. Time-lapse microscopic imaging of transgenic embryos expressing fluorescent reporters is the method of choice for tracking morphogenetic changes and establishing cell lineages and fate maps in vivo. However, the manual steps involved in curating thousands of putative cell segmentations have been a major bottleneck in the application of these technologies especially for cell membranes. Segmentation of cell membranes while more difficult than nuclear segmentation is necessary for quantifying the relations between changes in cell morphology and morphogenesis. We present a novel and fully automated method to first reconstruct membrane signals and then segment out cells from 3D membrane images even in dense tissues. The approach has three stages: 1) detection of local membrane planes, 2) voting to fill structural gaps, and 3) region segmentation. We demonstrate the superior performance of the algorithms quantitatively on time-lapse confocal and two-photon images of zebrafish neuroectoderm and paraxial mesoderm by comparing its results with those derived from human inspection. We also compared with synthetic microscopic images generated by simulating the process of imaging with fluorescent reporters under varying conditions of noise. Both the over-segmentation and under-segmentation percentages of our method are around 5%. The volume overlap of individual cells, compared to expert manual segmentation, is consistently over 84%. By using our software (ACME) to study somite formation, we were able to segment touching cells with high accuracy and reliably quantify changes in morphogenetic parameters such as cell shape and size, and the arrangement of epithelial and mesenchymal cells. Our software has been developed and tested on Windows, Mac, and Linux platforms and is available publicly under an open source BSD license (https://github.com/krm15/ACME).",what creates ?,ACME,linux,False,False
"<jats:title>Abstract</jats:title><jats:p>Somatic copy number variations (CNVs) play a crucial role in development of many human cancers. The broad availability of next-generation sequencing data has enabled the development of algorithms to computationally infer CNV profiles from a variety of data types including exome and targeted sequence data; currently the most prevalent types of cancer genomics data. However, systemic evaluation and comparison of these tools remains challenging due to a lack of ground truth reference sets. To address this need, we have developed Bamgineer, a tool written in Python to introduce user-defined haplotype-phased allele-specific copy number events into an existing Binary Alignment Mapping (BAM) file, with a focus on targeted and exome sequencing experiments. As input, this tool requires a read alignment file (BAM format), lists of non-overlapping genome coordinates for introduction of gains and losses (bed file), and an optional file defining known haplotypes (vcf format). To improve runtime performance, Bamgineer introduces the desired CNVs in parallel using queuing and parallel processing on a local machine or on a high-performance computing cluster. As proof-of-principle, we applied Bamgineer to a single high-coverage (mean: 220X) exome sequence file from a blood sample to simulate copy number profiles of 3 exemplar tumors from each of 10 tumor types at 5 tumor cellularity levels (20-100%, 150 BAM files in total). To demonstrate feasibility beyond exome data, we introduced read alignments to a targeted 5-gene cell-free DNA sequencing library to simulate <jats:italic>EGFR</jats:italic> amplifications at frequencies consistent with circulating tumor DNA (10, 1, 0.1 and 0.01%) while retaining the multimodal insert size distribution of the original data. We expect Bamgineer to be of use for development and systematic benchmarking of CNV calling algorithms by users using locally-generated data for a variety of applications. The source code is freely available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://github.com/pughlab/bamgineer"">http://github.com/pughlab/bamgineer</jats:ext-link>.</jats:p><jats:sec><jats:title>Author summary</jats:title><jats:p>We present Bamgineer, a software program to introduce user-defined, haplotype-specific copy number variants (CNVs) at any frequency into standard Binary Alignment Mapping (BAM) files. Copy number gains are simulated by introducing new DNA sequencing read pairs sampled from existing reads and modified to contain SNPs of the haplotype of interest. This approach retains biases of the original data such as local coverage, strand bias, and insert size. Deletions are simulated by removing reads corresponding to one or both haplotypes. In our proof-of-principle study, we simulated copy number profiles from 10 cancer types at varying cellularity levels typically encountered in clinical samples. We also demonstrated introduction of low frequency CNVs into cell-free DNA sequencing data that retained the bimodal fragment size distribution characteristic of these data. Bamgineer is flexible and enables users to simulate CNVs that reflect characteristics of locally-generated sequence files and can be used for many applications including development and benchmarking of CNV inference tools for a variety of data types.</jats:p></jats:sec>",what creates ?,Bamgineer,"bamgineer,",True,True
"Characterization of Human Endogenous Retrovirus (HERV) expression within the transcriptomic landscape using RNA-seq is complicated by uncertainty in fragment assignment because of sequence similarity. We present Telescope, a computational software tool that provides accurate estimation of transposable element expression (retrotranscriptome) resolved to specific genomic locations. Telescope directly addresses uncertainty in fragment assignment by reassigning ambiguously mapped fragments to the most probable source transcript as determined within a Bayesian statistical model. We demonstrate the utility of our approach through single locus analysis of HERV expression in 13 ENCODE cell types. When examined at this resolution, we find that the magnitude and breadth of the retrotranscriptome can be vastly different among cell types. Furthermore, our approach is robust to differences in sequencing technology and demonstrates that the retrotranscriptome has potential to be used for cell type identification. We compared our tool with other approaches for quantifying transposable element (TE) expression, and found that Telescope has the greatest resolution, as it estimates expression at specific TE insertions rather than at the TE subfamily level. Telescope performs highly accurate quantification of the retrotranscriptomic landscape in RNA-seq experiments, revealing a differential complexity in the transposable element biology of complex systems not previously observed. Telescope is available at https://github.com/mlbendall/telescope.",what creates ?,Telescope,"telescope,",True,True
"PathVisio is a commonly used pathway editor, visualization and analysis software. Biological pathways have been used by biologists for many years to describe the detailed steps in biological processes. Those powerful, visual representations help researchers to better understand, share and discuss knowledge. Since the first publication of PathVisio in 2008, the original paper was cited more than 170 times and PathVisio was used in many different biological studies. As an online editor PathVisio is also integrated in the community curated pathway database WikiPathways. Here we present the third version of PathVisio with the newest additions and improvements of the application. The core features of PathVisio are pathway drawing, advanced data visualization and pathway statistics. Additionally, PathVisio 3 introduces a new powerful extension systems that allows other developers to contribute additional functionality in form of plugins without changing the core application. PathVisio can be downloaded from http://www.pathvisio.org and in 2014 PathVisio 3 has been downloaded over 5,500 times. There are already more than 15 plugins available in the central plugin repository. PathVisio is a freely available, open-source tool published under the Apache 2.0 license (http://www.apache.org/licenses/LICENSE-2.0). It is implemented in Java and thus runs on all major operating systems. The code repository is available at http://svn.bigcat.unimaas.nl/pathvisio. The support mailing list for users is available on https://groups.google.com/forum/#!forum/wikipathways-discuss and for developers on https://groups.google.com/forum/#!forum/wikipathways-devel.",what creates ?,PathVisio,pathvisio,True,True
"The analysis of the mutational landscape of cancer, including mutual exclusivity and co-occurrence of mutations, has been instrumental in studying the disease. We hypothesized that exploring the interplay between co-occurrence, mutual exclusivity, and functional interactions between genes will further improve our understanding of the disease and help to uncover new relations between cancer driving genes and pathways. To this end, we designed a general framework, BeWith, for identifying modules with different combinations of mutation and interaction patterns. We focused on three different settings of the BeWith schema: (i) BeME-WithFun, in which the relations between modules are enriched with mutual exclusivity, while genes within each module are functionally related; (ii) BeME-WithCo, which combines mutual exclusivity between modules with co-occurrence within modules; and (iii) BeCo-WithMEFun, which ensures co-occurrence between modules, while the within module relations combine mutual exclusivity and functional interactions. We formulated the BeWith framework using Integer Linear Programming (ILP), enabling us to find optimally scoring sets of modules. Our results demonstrate the utility of BeWith in providing novel information about mutational patterns, driver genes, and pathways. In particular, BeME-WithFun helped identify functionally coherent modules that might be relevant for cancer progression. In addition to finding previously well-known drivers, the identified modules pointed to other novel findings such as the interaction between NCOR2 and NCOA3 in breast cancer. Additionally, an application of the BeME-WithCo setting revealed that gene groups differ with respect to their vulnerability to different mutagenic processes, and helped us to uncover pairs of genes with potentially synergistic effects, including a potential synergy between mutations in TP53 and the metastasis related DCC gene. Overall, BeWith not only helped us uncover relations between potential driver genes and pathways, but also provided additional insights on patterns of the mutational landscape, going beyond cancer driving mutations. Implementation is available at https://www.ncbi.nlm.nih.gov/CBBresearch/Przytycka/software/bewith.html",what creates ?,BeWith,"bewith,",True,True
"Nonribosomally and ribosomally synthesized bioactive peptides constitute a source of molecules of great biomedical importance, including antibiotics such as penicillin, immunosuppressants such as cyclosporine, and cytostatics such as bleomycin. Recently, an innovative mass-spectrometry-based strategy, peptidogenomics, has been pioneered to effectively mine microbial strains for novel peptidic metabolites. Even though mass-spectrometric peptide detection can be performed quite fast, true high-throughput natural product discovery approaches have still been limited by the inability to rapidly match the identified tandem mass spectra to the gene clusters responsible for the biosynthesis of the corresponding compounds. With Pep2Path, we introduce a software package to fully automate the peptidogenomics approach through the rapid Bayesian probabilistic matching of mass spectra to their corresponding biosynthetic gene clusters. Detailed benchmarking of the method shows that the approach is powerful enough to correctly identify gene clusters even in data sets that consist of hundreds of genomes, which also makes it possible to match compounds from unsequenced organisms to closely related biosynthetic gene clusters in other genomes. Applying Pep2Path to a data set of compounds without known biosynthesis routes, we were able to identify candidate gene clusters for the biosynthesis of five important compounds. Notably, one of these clusters was detected in a genome from a different subphylum of Proteobacteria than that in which the molecule had first been identified. All in all, our approach paves the way towards high-throughput discovery of novel peptidic natural products. Pep2Path is freely available from http://pep2path.sourceforge.net/, implemented in Python, licensed under the GNU General Public License v3 and supported on MS Windows, Linux and Mac OS X.",what creates ?,Pep2Path,"pep2path,",True,True
"We present ggsashimi, a command-line tool for the visualization of splicing events across multiple samples. Given a specified genomic region, ggsashimi creates sashimi plots for individual RNA-seq experiments as well as aggregated plots for groups of experiments, a feature unique to this software. Compared to the existing versions of programs generating sashimi plots, it uses popular bioinformatics file formats, it is annotation-independent, and allows the visualization of splicing events even for large genomic regions by scaling down the genomic segments between splice sites. ggsashimi is freely available at https://github.com/guigolab/ggsashimi. It is implemented in python, and internally generates R code for plotting.",what creates ?,ggsashimi,"ggsashimi,",True,True
"Detecting similarities between ligand binding sites in the absence of global homology between target proteins has been recognized as one of the critical components of modern drug discovery. Local binding site alignments can be constructed using sequence order-independent techniques, however, to achieve a high accuracy, many current algorithms for binding site comparison require high-quality experimental protein structures, preferably in the bound conformational state. This, in turn, complicates proteome scale applications, where only various quality structure models are available for the majority of gene products. To improve the state-of-the-art, we developed eMatchSite, a new method for constructing sequence order-independent alignments of ligand binding sites in protein models. Large-scale benchmarking calculations using adenine-binding pockets in crystal structures demonstrate that eMatchSite generates accurate alignments for almost three times more protein pairs than SOIPPA. More importantly, eMatchSite offers a high tolerance to structural distortions in ligand binding regions in protein models. For example, the percentage of correctly aligned pairs of adenine-binding sites in weakly homologous protein models is only 4–9% lower than those aligned using crystal structures. This represents a significant improvement over other algorithms, e.g. the performance of eMatchSite in recognizing similar binding sites is 6% and 13% higher than that of SiteEngine using high- and moderate-quality protein models, respectively. Constructing biologically correct alignments using predicted ligand binding sites in protein models opens up the possibility to investigate drug-protein interaction networks for complete proteomes with prospective systems-level applications in polypharmacology and rational drug repositioning. eMatchSite is freely available to the academic community as a web-server and a stand-alone software distribution at http://www.brylinski.org/ematchsite.",what creates ?,eMatchSite,"ematchsite,",True,True
"Imaging and analyzing the locomotion behavior of small animals such as Drosophila larvae or C. elegans worms has become an integral subject of biological research. In the past we have introduced FIM, a novel imaging system feasible to extract high contrast images. This system in combination with the associated tracking software FIMTrack is already used by many groups all over the world. However, so far there has not been an in-depth discussion of the technical aspects. Here we elaborate on the implementation details of FIMTrack and give an in-depth explanation of the used algorithms. Among others, the software offers several tracking strategies to cover a wide range of different model organisms, locomotion types, and camera properties. Furthermore, the software facilitates stimuli-based analysis in combination with built-in manual tracking and correction functionalities. All features are integrated in an easy-to-use graphical user interface. To demonstrate the potential of FIMTrack we provide an evaluation of its accuracy using manually labeled data. The source code is available under the GNU GPLv3 at https://github.com/i-git/FIMTrack and pre-compiled binaries for Windows and Mac are available at http://fim.uni-muenster.de.",what creates ?,FIMTrack,"fim,",False,False
"Advances in computational metabolic optimization are required to realize the full potential of new in vivo metabolic engineering technologies by bridging the gap between computational design and strain development. We present Redirector, a new Flux Balance Analysis-based framework for identifying engineering targets to optimize metabolite production in complex pathways. Previous optimization frameworks have modeled metabolic alterations as directly controlling fluxes by setting particular flux bounds. Redirector develops a more biologically relevant approach, modeling metabolic alterations as changes in the balance of metabolic objectives in the system. This framework iteratively selects enzyme targets, adds the associated reaction fluxes to the metabolic objective, thereby incentivizing flux towards the production of a metabolite of interest. These adjustments to the objective act in competition with cellular growth and represent up-regulation and down-regulation of enzyme mediated reactions. Using the iAF1260 E. coli metabolic network model for optimization of fatty acid production as a test case, Redirector generates designs with as many as 39 simultaneous and 111 unique engineering targets. These designs discover proven in vivo targets, novel supporting pathways and relevant interdependencies, many of which cannot be predicted by other methods. Redirector is available as open and free software, scalable to computational resources, and powerful enough to find all known enzyme targets for fatty acid production.",what creates ?,Redirector,"redirector,",True,True
"Since its identification in 1983, HIV-1 has been the focus of a research effort unprecedented in scope and difficulty, whose ultimate goals — a cure and a vaccine – remain elusive. One of the fundamental challenges in accomplishing these goals is the tremendous genetic variability of the virus, with some genes differing at as many as 40% of nucleotide positions among circulating strains. Because of this, the genetic bases of many viral phenotypes, most notably the susceptibility to neutralization by a particular antibody, are difficult to identify computationally. Drawing upon open-source general-purpose machine learning algorithms and libraries, we have developed a software package IDEPI (IDentify EPItopes) for learning genotype-to-phenotype predictive models from sequences with known phenotypes. IDEPI can apply learned models to classify sequences of unknown phenotypes, and also identify specific sequence features which contribute to a particular phenotype. We demonstrate that IDEPI achieves performance similar to or better than that of previously published approaches on four well-studied problems: finding the epitopes of broadly neutralizing antibodies (bNab), determining coreceptor tropism of the virus, identifying compartment-specific genetic signatures of the virus, and deducing drug-resistance associated mutations. The cross-platform Python source code (released under the GPL 3.0 license), documentation, issue tracking, and a pre-configured virtual machine for IDEPI can be found at https://github.com/veg/idepi.",what creates ?,IDEPI,idepi,True,True
"A metabolome-wide genome-wide association study (mGWAS) aims to discover the effects of genetic variants on metabolome phenotypes. Most mGWASes use as phenotypes concentrations of limited sets of metabolites that can be identified and quantified from spectral information. In contrast, in an untargeted mGWAS both identification and quantification are forgone and, instead, all measured metabolome features are tested for association with genetic variants. While the untargeted approach does not discard data that may have eluded identification, the interpretation of associated features remains a challenge. To address this issue, we developed metabomatching to identify the metabolites underlying significant associations observed in untargeted mGWASes on proton NMR metabolome data. Metabomatching capitalizes on genetic spiking, the concept that because metabolome features associated with a genetic variant tend to correspond to the peaks of the NMR spectrum of the underlying metabolite, genetic association can allow for identification. Applied to the untargeted mGWASes in the SHIP and CoLaus cohorts and using 180 reference NMR spectra of the urine metabolome database, metabomatching successfully identified the underlying metabolite in 14 of 19, and 8 of 9 associations, respectively. The accuracy and efficiency of our method make it a strong contender for facilitating or complementing metabolomics analyses in large cohorts, where the availability of genetic, or other data, enables our approach, but targeted quantification is limited.",what deposits ?,Metabomatching,metabomatching,True,True
"The use of 3C-based methods has revealed the importance of the 3D organization of the chromatin for key aspects of genome biology. However, the different caveats of the variants of 3C techniques have limited their scope and the range of scientific fields that could benefit from these approaches. To address these limitations, we present 4Cin, a method to generate 3D models and derive virtual Hi-C (vHi-C) heat maps of genomic loci based on 4C-seq or any kind of 4C-seq-like data, such as those derived from NG Capture-C. 3D genome organization is determined by integrative consideration of the spatial distances derived from as few as four 4C-seq experiments. The 3D models obtained from 4C-seq data, together with their associated vHi-C maps, allow the inference of all chromosomal contacts within a given genomic region, facilitating the identification of Topological Associating Domains (TAD) boundaries. Thus, 4Cin offers a much cheaper, accessible and versatile alternative to other available techniques while providing a comprehensive 3D topological profiling. By studying TAD modifications in genomic structural variants associated to disease phenotypes and performing cross-species evolutionary comparisons of 3D chromatin structures in a quantitative manner, we demonstrate the broad potential and novel range of applications of our method.",what deposits ?,4Cin,"4cin,",True,True
"The rapidly expanding body of available genomic and protein structural data provides a rich resource for understanding protein dynamics with biomolecular simulation. While computational infrastructure has grown rapidly, simulations on an omics scale are not yet widespread, primarily because software infrastructure to enable simulations at this scale has not kept pace. It should now be possible to study protein dynamics across entire (super)families, exploiting both available structural biology data and conformational similarities across homologous proteins. Here, we present a new tool for enabling high-throughput simulation in the genomics era. Ensembler takes any set of sequences - from a single sequence to an entire superfamily - and shepherds them through various stages of modeling and refinement to produce simulation-ready structures. This includes comparative modeling to all relevant PDB structures (which may span multiple conformational states of interest), reconstruction of missing loops, addition of missing atoms, culling of nearly identical structures, assignment of appropriate protonation states, solvation in explicit solvent, and refinement and filtering with molecular simulation to ensure stable simulation. The output of this pipeline is an ensemble of structures ready for subsequent molecular simulations using computer clusters, supercomputers, or distributed computing projects like Folding@home. Ensembler thus automates much of the time-consuming process of preparing protein models suitable for simulation, while allowing scalability up to entire superfamilies. A particular advantage of this approach can be found in the construction of kinetic models of conformational dynamics - such as Markov state models (MSMs) - which benefit from a diverse array of initial configurations that span the accessible conformational states to aid sampling. We demonstrate the power of this approach by constructing models for all catalytic domains in the human tyrosine kinase family, using all available kinase catalytic domain structures from any organism as structural templates. Ensembler is free and open source software licensed under the GNU General Public License (GPL) v2. It is compatible with Linux and OS X. The latest release can be installed via the conda package manager, and the latest source can be downloaded from https://github.com/choderalab/ensembler.",what deposits ?,Ensembler,linux,False,False
"Epigenetic regulation consists of a multitude of different modifications that determine active and inactive states of chromatin. Conditions such as cell differentiation or exposure to environmental stress require concerted changes in gene expression. To interpret epigenomics data, a spectrum of different interconnected datasets is needed, ranging from the genome sequence and positions of histones, together with their modifications and variants, to the transcriptional output of genomic regions. Here we present a tool, Podbat (Positioning database and analysis tool), that incorporates data from various sources and allows detailed dissection of the entire range of chromatin modifications simultaneously. Podbat can be used to analyze, visualize, store and share epigenomics data. Among other functions, Podbat allows data-driven determination of genome regions of differential protein occupancy or RNA expression using Hidden Markov Models. Comparisons between datasets are facilitated to enable the study of the comprehensive chromatin modification system simultaneously, irrespective of data-generating technique. Any organism with a sequenced genome can be accommodated. We exemplify the power of Podbat by reanalyzing all to-date published genome-wide data for the histone variant H2A.Z in fission yeast together with other histone marks and also phenotypic response data from several sources. This meta-analysis led to the unexpected finding of H2A.Z incorporation in the coding regions of genes encoding proteins involved in the regulation of meiosis and genotoxic stress responses. This incorporation was partly independent of the H2A.Z-incorporating remodeller Swr1. We verified an Swr1-independent role for H2A.Z following genotoxic stress in vivo. Podbat is open source software freely downloadable from www.podbat.org, distributed under the GNU LGPL license. User manuals, test data and instructions are available at the website, as well as a repository for third party–developed plug-in modules. Podbat requires Java version 1.6 or higher.",what deposits ?,Podbat,podbat,True,True
"Active matter systems, and in particular the cell cytoskeleton, exhibit complex mechanochemical dynamics that are still not well understood. While prior computational models of cytoskeletal dynamics have lead to many conceptual insights, an important niche still needs to be filled with a high-resolution structural modeling framework, which includes a minimally-complete set of cytoskeletal chemistries, stochastically treats reaction and diffusion processes in three spatial dimensions, accurately and efficiently describes mechanical deformations of the filamentous network under stresses generated by molecular motors, and deeply couples mechanics and chemistry at high spatial resolution. To address this need, we propose a novel reactive coarse-grained force field, as well as a publicly available software package, named the Mechanochemical Dynamics of Active Networks (MEDYAN), for simulating active network evolution and dynamics (available at www.medyan.org). This model can be used to study the non-linear, far from equilibrium processes in active matter systems, in particular, comprised of interacting semi-flexible polymers embedded in a solution with complex reaction-diffusion processes. In this work, we applied MEDYAN to investigate a contractile actomyosin network consisting of actin filaments, alpha-actinin cross-linking proteins, and non-muscle myosin IIA mini-filaments. We found that these systems undergo a switch-like transition in simulations from a random network to ordered, bundled structures when cross-linker concentration is increased above a threshold value, inducing contraction driven by myosin II mini-filaments. Our simulations also show how myosin II mini-filaments, in tandem with cross-linkers, can produce a range of actin filament polarity distributions and alignment, which is crucially dependent on the rate of actin filament turnover and the actin filament’s resulting super-diffusive behavior in the actomyosin-cross-linker system. We discuss the biological implications of these findings for the arc formation in lamellipodium-to-lamellum architectural remodeling. Lastly, our simulations produce force-dependent accumulation of myosin II, which is thought to be responsible for their mechanosensation ability, also spontaneously generating myosin II concentration gradients in the solution phase of the simulation volume.",what deposits ?,MEDYAN,,False,False
"Accurate mapping of next-generation sequencing (NGS) reads to reference genomes is crucial for almost all NGS applications and downstream analyses. Various repetitive elements in human and other higher eukaryotic genomes contribute in large part to ambiguously (non-uniquely) mapped reads. Most available NGS aligners attempt to address this by either removing all non-uniquely mapping reads, or reporting one random or ""best"" hit based on simple heuristics. Accurate estimation of the mapping quality of NGS reads is therefore critical albeit completely lacking at present. Here we developed a generalized software toolkit ""AlignerBoost"", which utilizes a Bayesian-based framework to accurately estimate mapping quality of ambiguously mapped NGS reads. We tested AlignerBoost with both simulated and real DNA-seq and RNA-seq datasets at various thresholds. In most cases, but especially for reads falling within repetitive regions, AlignerBoost dramatically increases the mapping precision of modern NGS aligners without significantly compromising the sensitivity even without mapping quality filters. When using higher mapping quality cutoffs, AlignerBoost achieves a much lower false mapping rate while exhibiting comparable or higher sensitivity compared to the aligner default modes, therefore significantly boosting the detection power of NGS aligners even using extreme thresholds. AlignerBoost is also SNP-aware, and higher quality alignments can be achieved if provided with known SNPs. AlignerBoost’s algorithm is computationally efficient, and can process one million alignments within 30 seconds on a typical desktop computer. AlignerBoost is implemented as a uniform Java application and is freely available at https://github.com/Grice-Lab/AlignerBoost.",what deposits ?,AlignerBoost,"alignerboost "",",False,True
"I introduce an open-source R package ‘dcGOR’ to provide the bioinformatics community with the ease to analyse ontologies and protein domain annotations, particularly those in the dcGO database. The dcGO is a comprehensive resource for protein domain annotations using a panel of ontologies including Gene Ontology. Although increasing in popularity, this database needs statistical and graphical support to meet its full potential. Moreover, there are no bioinformatics tools specifically designed for domain ontology analysis. As an add-on package built in the R software environment, dcGOR offers a basic infrastructure with great flexibility and functionality. It implements new data structure to represent domains, ontologies, annotations, and all analytical outputs as well. For each ontology, it provides various mining facilities, including: (i) domain-based enrichment analysis and visualisation; (ii) construction of a domain (semantic similarity) network according to ontology annotations; and (iii) significance analysis for estimating a contact (statistical significance) network. To reduce runtime, most analyses support high-performance parallel computing. Taking as inputs a list of protein domains of interest, the package is able to easily carry out in-depth analyses in terms of functional, phenotypic and diseased relevance, and network-level understanding. More importantly, dcGOR is designed to allow users to import and analyse their own ontologies and annotations on domains (taken from SCOP, Pfam and InterPro) and RNAs (from Rfam) as well. The package is freely available at CRAN for easy installation, and also at GitHub for version control. The dedicated website with reproducible demos can be found at http://supfam.org/dcGOR.",what deposits ?,dcGOR,dcgor ’,False,True
"Transmembrane channel proteins play pivotal roles in maintaining the homeostasis and responsiveness of cells and the cross-membrane electrochemical gradient by mediating the transport of ions and molecules through biological membranes. Therefore, computational methods which, given a set of 3D coordinates, can automatically identify and describe channels in transmembrane proteins are key tools to provide insights into how they function. Herein we present PoreWalker, a fully automated method, which detects and fully characterises channels in transmembrane proteins from their 3D structures. A stepwise procedure is followed in which the pore centre and pore axis are first identified and optimised using geometric criteria, and then the biggest and longest cavity through the channel is detected. Finally, pore features, including diameter profiles, pore-lining residues, size, shape and regularity of the pore are calculated, providing a quantitative and visual characterization of the channel. To illustrate the use of this tool, the method was applied to several structures of transmembrane channel proteins and was able to identify shape/size/residue features representative of specific channel families. The software is available as a web-based resource at http://www.ebi.ac.uk/thornton-srv/software/PoreWalker/.",what deposits ?,software,"porewalker,",False,False
"Chemical reaction networks are ubiquitous in biology, and their dynamics is fundamentally stochastic. Here, we present the software library pSSAlib, which provides a complete and concise implementation of the most efficient partial-propensity methods for simulating exact stochastic chemical kinetics. pSSAlib can import models encoded in Systems Biology Markup Language, supports time delays in chemical reactions, and stochastic spatiotemporal reaction-diffusion systems. It also provides tools for statistical analysis of simulation results and supports multiple output formats. It has previously been used for studies of biochemical reaction pathways and to benchmark other stochastic simulation methods. Here, we describe pSSAlib in detail and apply it to a new model of the endocytic pathway in eukaryotic cells, leading to the discovery of a stochastic counterpart of the cut-out switch motif underlying early-to-late endosome conversion. pSSAlib is provided as a stand-alone command-line tool and as a developer API. We also provide a plug-in for the SBMLToolbox. The open-source code and pre-packaged installers are freely available from http://mosaic.mpi-cbg.de.",what deposits ?,pSSAlib,"pssalib,",True,True
"Chaste — Cancer, Heart And Soft Tissue Environment — is an open source C++ library for the computational simulation of mathematical models developed for physiology and biology. Code development has been driven by two initial applications: cardiac electrophysiology and cancer development. A large number of cardiac electrophysiology studies have been enabled and performed, including high-performance computational investigations of defibrillation on realistic human cardiac geometries. New models for the initiation and growth of tumours have been developed. In particular, cell-based simulations have provided novel insight into the role of stem cells in the colorectal crypt. Chaste is constantly evolving and is now being applied to a far wider range of problems. The code provides modules for handling common scientific computing components, such as meshes and solvers for ordinary and partial differential equations (ODEs/PDEs). Re-use of these components avoids the need for researchers to ‘re-invent the wheel’ with each new project, accelerating the rate of progress in new applications. Chaste is developed using industrially-derived techniques, in particular test-driven development, to ensure code quality, re-use and reliability. In this article we provide examples that illustrate the types of problems Chaste can be used to solve, which can be run on a desktop computer. We highlight some scientific studies that have used or are using Chaste, and the insights they have provided. The source code, both for specific releases and the development version, is available to download under an open source Berkeley Software Distribution (BSD) licence at http://www.cs.ox.ac.uk/chaste, together with details of a mailing list and links to documentation and tutorials.",what deposits ?,Chaste,"chaste — cancer, heart and soft tissue environment — is an open source c + + library for the computational simulation of mathematical models developed for physiology and biology. code development has been driven by two initial applications : cardiac electrophysiology and cancer development. a large number of cardiac electrophysiology studies have been enabled and performed, including high - performance computational investigations of defibrillation on realistic human cardiac geometries. new models for the initiation and growth of tumours have been developed. in particular, cell - based simulations have provided novel insight into the role of stem cells in the colorectal crypt. chaste is constantly evolving and is now being applied to a far wider range of problems. the code provides modules for handling common scientific computing components, such as meshes and solvers for ordinary and partial differential equations ( odes / pdes ). re - use of these components avoids the need for researchers to ‘ re - invent the wheel ’ with each new project, accelerating the rate of progress in new applications. chaste is developed using industrially - derived techniques, in particular test - driven development, to ensure code quality, re - use and reliability. in this article we provide examples that illustrate the types of problems chaste can be used to solve, which can be run on a desktop computer. we highlight some scientific studies that have used or are using chaste, and the insights they have provided. the source code, both for specific releases and the development version, is available to download under an open source berkeley software distribution ( bsd ) licence at http : / / www. cs. ox. ac. uk / chaste,",False,True
"Movement is fundamental to human and animal life, emerging through interaction of complex neural, muscular, and skeletal systems. Study of movement draws from and contributes to diverse fields, including biology, neuroscience, mechanics, and robotics. OpenSim unites methods from these fields to create fast and accurate simulations of movement, enabling two fundamental tasks. First, the software can calculate variables that are difficult to measure experimentally, such as the forces generated by muscles and the stretch and recoil of tendons during movement. Second, OpenSim can predict novel movements from models of motor control, such as kinematic adaptations of human gait during loaded or inclined walking. Changes in musculoskeletal dynamics following surgery or due to human–device interaction can also be simulated; these simulations have played a vital role in several applications, including the design of implantable mechanical devices to improve human grasping in individuals with paralysis. OpenSim is an extensible and user-friendly software package built on decades of knowledge about computational modeling and simulation of biomechanical systems. OpenSim’s design enables computational scientists to create new state-of-the-art software tools and empowers others to use these tools in research and clinical applications. OpenSim supports a large and growing community of biomechanics and rehabilitation researchers, facilitating exchange of models and simulations for reproducing and extending discoveries. Examples, tutorials, documentation, and an active user forum support this community. The OpenSim software is covered by the Apache License 2.0, which permits its use for any purpose including both nonprofit and commercial applications. The source code is freely and anonymously accessible on GitHub, where the community is welcomed to make contributions. Platform-specific installers of OpenSim include a GUI and are available on simtk.org.",what deposits ?,OpenSim,opensim,True,True
"PhyloGibbs, our recent Gibbs-sampling motif-finder, takes phylogeny into account in detecting binding sites for transcription factors in DNA and assigns posterior probabilities to its predictions obtained by sampling the entire configuration space. Here, in an extension called PhyloGibbs-MP, we widen the scope of the program, addressing two major problems in computational regulatory genomics. First, PhyloGibbs-MP can localise predictions to small, undetermined regions of a large input sequence, thus effectively predicting cis-regulatory modules (CRMs) ab initio while simultaneously predicting binding sites in those modules—tasks that are usually done by two separate programs. PhyloGibbs-MP's performance at such ab initio CRM prediction is comparable with or superior to dedicated module-prediction software that use prior knowledge of previously characterised transcription factors. Second, PhyloGibbs-MP can predict motifs that differentiate between two (or more) different groups of regulatory regions, that is, motifs that occur preferentially in one group over the others. While other “discriminative motif-finders” have been published in the literature, PhyloGibbs-MP's implementation has some unique features and flexibility. Benchmarks on synthetic and actual genomic data show that this algorithm is successful at enhancing predictions of differentiating sites and suppressing predictions of common sites and compares with or outperforms other discriminative motif-finders on actual genomic data. Additional enhancements include significant performance and speed improvements, the ability to use “informative priors” on known transcription factors, and the ability to output annotations in a format that can be visualised with the Generic Genome Browser. In stand-alone motif-finding, PhyloGibbs-MP remains competitive, outperforming PhyloGibbs-1.0 and other programs on benchmark data.",what deposits ?,PhyloGibbs-MP,"phylogibbs,",False,False
"Abstract Many multicellular systems problems can only be understood by studying how cells move, grow, divide, interact, and die. Tissue-scale dynamics emerge from systems of many interacting cells as they respond to and influence their microenvironment. The ideal “virtual laboratory” for such multicellular systems simulates both the biochemical microenvironment (the “stage”) and many mechanically and biochemically interacting cells (the “players” upon the stage). PhysiCell—physics-based multicellular simulator—is an open source agent-based simulator that provides both the stage and the players for studying many interacting cells in dynamic tissue microenvironments. It builds upon a multi-substrate biotransport solver to link cell phenotype to multiple diffusing substrates and signaling factors. It includes biologically-driven sub-models for cell cycling, apoptosis, necrosis, solid and fluid volume changes, mechanics, and motility “out of the box.” The C++ code has minimal dependencies, making it simple to maintain and deploy across platforms. PhysiCell has been parallelized with OpenMP, and its performance scales linearly with the number of cells. Simulations up to 10 5 -10 6 cells are feasible on quad-core desktop workstations; larger simulations are attainable on single HPC compute nodes. We demonstrate PhysiCell by simulating the impact of necrotic core biomechanics, 3-D geometry, and stochasticity on the dynamics of hanging drop tumor spheroids and ductal carcinoma in situ (DCIS) of the breast. We demonstrate stochastic motility, chemical and contact-based interaction of multiple cell types, and the extensibility of PhysiCell with examples in synthetic multicellular systems (a “cellular cargo delivery” system, with application to anti-cancer treatments), cancer heterogeneity, and cancer immunology. PhysiCell is a powerful multicellular systems simulator that will be continually improved with new capabilities and performance improvements. It also represents a significant independent code base for replicating results from other simulation platforms. The PhysiCell source code, examples, documentation, and support are available under the BSD license at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://PhysiCell.MathCancer.org"">http://PhysiCell.MathCancer.org</jats:ext-link> and <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://PhysiCell.sf.net"">http://PhysiCell.sf.net</jats:ext-link>. Author Summary This paper introduces PhysiCell: an open source, agent-based modeling framework for 3-D multicellular simulations. It includes a standard library of sub-models for cell fluid and solid volume changes, cycle progression, apoptosis, necrosis, mechanics, and motility. PhysiCell is directly coupled to a biotransport solver to simulate many diffusing substrates and cell-secreted signals. Each cell can dynamically update its phenotype based on its microenvironmental conditions. Users can customize or replace the included sub-models. PhysiCell runs on a variety of platforms (Linux, OSX, and Windows) with few software dependencies. Its computational cost scales linearly in the number of cells. It is feasible to simulate 500,000 cells on quad-core desktop workstations, and millions of cells on single HPC compute nodes. We demonstrate PhysiCell by simulating the impact of necrotic core biomechanics, 3-D geometry, and stochasticity on hanging drop tumor spheroids (HDS) and ductal carcinoma in situ (DCIS) of the breast. We demonstrate contact- and chemokine-based interactions among multiple cell types with examples in synthetic multicellular bioengineering, cancer heterogeneity, and cancer immunology. We developed PhysiCell to help the scientific community tackle multicellular systems biology problems involving many interacting cells in multi-substrate microenvironments. PhysiCell is also an independent, cross-platform codebase for replicating results from other simulators.",what deposits ?,PhysiCell,physicell,True,True
"Imaging and analyzing the locomotion behavior of small animals such as Drosophila larvae or C. elegans worms has become an integral subject of biological research. In the past we have introduced FIM, a novel imaging system feasible to extract high contrast images. This system in combination with the associated tracking software FIMTrack is already used by many groups all over the world. However, so far there has not been an in-depth discussion of the technical aspects. Here we elaborate on the implementation details of FIMTrack and give an in-depth explanation of the used algorithms. Among others, the software offers several tracking strategies to cover a wide range of different model organisms, locomotion types, and camera properties. Furthermore, the software facilitates stimuli-based analysis in combination with built-in manual tracking and correction functionalities. All features are integrated in an easy-to-use graphical user interface. To demonstrate the potential of FIMTrack we provide an evaluation of its accuracy using manually labeled data. The source code is available under the GNU GPLv3 at https://github.com/i-git/FIMTrack and pre-compiled binaries for Windows and Mac are available at http://fim.uni-muenster.de.",what deposits ?,FIMTrack,"fim,",False,False
"<jats:title>Abstract</jats:title><jats:p>Somatic copy number variations (CNVs) play a crucial role in development of many human cancers. The broad availability of next-generation sequencing data has enabled the development of algorithms to computationally infer CNV profiles from a variety of data types including exome and targeted sequence data; currently the most prevalent types of cancer genomics data. However, systemic evaluation and comparison of these tools remains challenging due to a lack of ground truth reference sets. To address this need, we have developed Bamgineer, a tool written in Python to introduce user-defined haplotype-phased allele-specific copy number events into an existing Binary Alignment Mapping (BAM) file, with a focus on targeted and exome sequencing experiments. As input, this tool requires a read alignment file (BAM format), lists of non-overlapping genome coordinates for introduction of gains and losses (bed file), and an optional file defining known haplotypes (vcf format). To improve runtime performance, Bamgineer introduces the desired CNVs in parallel using queuing and parallel processing on a local machine or on a high-performance computing cluster. As proof-of-principle, we applied Bamgineer to a single high-coverage (mean: 220X) exome sequence file from a blood sample to simulate copy number profiles of 3 exemplar tumors from each of 10 tumor types at 5 tumor cellularity levels (20-100%, 150 BAM files in total). To demonstrate feasibility beyond exome data, we introduced read alignments to a targeted 5-gene cell-free DNA sequencing library to simulate <jats:italic>EGFR</jats:italic> amplifications at frequencies consistent with circulating tumor DNA (10, 1, 0.1 and 0.01%) while retaining the multimodal insert size distribution of the original data. We expect Bamgineer to be of use for development and systematic benchmarking of CNV calling algorithms by users using locally-generated data for a variety of applications. The source code is freely available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://github.com/pughlab/bamgineer"">http://github.com/pughlab/bamgineer</jats:ext-link>.</jats:p><jats:sec><jats:title>Author summary</jats:title><jats:p>We present Bamgineer, a software program to introduce user-defined, haplotype-specific copy number variants (CNVs) at any frequency into standard Binary Alignment Mapping (BAM) files. Copy number gains are simulated by introducing new DNA sequencing read pairs sampled from existing reads and modified to contain SNPs of the haplotype of interest. This approach retains biases of the original data such as local coverage, strand bias, and insert size. Deletions are simulated by removing reads corresponding to one or both haplotypes. In our proof-of-principle study, we simulated copy number profiles from 10 cancer types at varying cellularity levels typically encountered in clinical samples. We also demonstrated introduction of low frequency CNVs into cell-free DNA sequencing data that retained the bimodal fragment size distribution characteristic of these data. Bamgineer is flexible and enables users to simulate CNVs that reflect characteristics of locally-generated sequence files and can be used for many applications including development and benchmarking of CNV inference tools for a variety of data types.</jats:p></jats:sec>",what uses ?,Bamgineer,"bamgineer,",True,True
"Recent studies of the human genome have indicated that regulatory elements (e.g. promoters and enhancers) at distal genomic locations can interact with each other via chromatin folding and affect gene expression levels. Genomic technologies for mapping interactions between DNA regions, e.g., ChIA-PET and HiC, can generate genome-wide maps of interactions between regulatory elements. These interaction datasets are important resources to infer distal gene targets of non-coding regulatory elements and to facilitate prioritization of critical loci for important cellular functions. With the increasing diversity and complexity of genomic information and public ontologies, making sense of these datasets demands integrative and easy-to-use software tools. Moreover, network representation of chromatin interaction maps enables effective data visualization, integration, and mining. Currently, there is no software that can take full advantage of network theory approaches for the analysis of chromatin interaction datasets. To fill this gap, we developed a web-based application, QuIN, which enables: 1) building and visualizing chromatin interaction networks, 2) annotating networks with user-provided private and publicly available functional genomics and interaction datasets, 3) querying network components based on gene name or chromosome location, and 4) utilizing network based measures to identify and prioritize critical regulatory targets and their direct and indirect interactions. AVAILABILITY: QuIN’s web server is available at http://quin.jax.org QuIN is developed in Java and JavaScript, utilizing an Apache Tomcat web server and MySQL database and the source code is available under the GPLV3 license available on GitHub: https://github.com/UcarLab/QuIN/.",what uses ?,GitHub,"java and javascript,",False,False
"Background While the provision of gender affirming care for transgender people in South Africa is considered legal, ethical, and medically sound, and is—theoretically—available in both the South African private and public health sectors, access remains severely limited and unequal within the country. As there are no national policies or guidelines, little is known about how individual health care professionals providing gender affirming care make clinical decisions about eligibility and treatment options. Method Based on an initial policy review and service mapping, this study employed semi-structured interviews with a snowball sample of twelve health care providers, representing most providers currently providing gender affirming care in South Africa. Data were analysed thematically using NVivo, and are reported following COREQ guidelines. Results Our findings suggest that, whilst a small minority of health care providers offer gender affirming care, this is almost exclusively on their own initiative and is usually unsupported by wider structures and institutions. The ad hoc, discretionary nature of services means that access to care is dependent on whether a transgender person is fortunate enough to access a sympathetic and knowledgeable health care provider. Conclusion Accordingly, national, state-sanctioned guidelines for gender affirming care are necessary to increase access, homogenise quality of care, and contribute to equitable provision of gender affirming care in the public and private health systems.",what uses ?,NVivo,"nvivo,",True,True
"Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.",what uses ?,Pong,deep q - learning,False,False
"The interest in the promotion of entrepreneurship is significantly increasing, particularly in those countries, such as Italy, that suffered during the recent great economic recession and subsequently needed to revitalize their economy. Entrepreneurial intention (EI) is a crucial stage in the entrepreneurial process and represents the basis for consequential entrepreneurial actions. Several research projects have sought to understand the antecedents of EI. This study, using a situational approach, has investigated the personal and contextual determinants of EI, exploring gender differences. In particular, the mediational role of general self-efficacy between internal locus of control (LoC), self-regulation, and support from family and friends, on the one hand, and EI, on the other hand, has been investigated. The study involved a sample of 658 Italian participants, of which 319 were male and 339 were female. Data were collected with a self-report on-line questionnaire and analysed with SPSS 23 and Mplus 7 to test a multi-group structural equation model. The results showed that self-efficacy totally mediated the relationship between internal LoC, self-regulation and EI. Moreover, it partially mediated the relationship between support from family and friends and EI. All the relations were significant for both men and women; however, our findings highlighted a stronger relationship between self-efficacy and EI for men, and between support from family and friends and both self-efficacy and EI for women. Findings highlighted the role of contextual characteristics in addition to personal ones in influencing EI and confirmed the key mediational function of self-efficacy. As for gender, results suggested that differences between men and women in relation to the entrepreneur role still exist. Practical implications for trainers and educators are discussed.",what uses ?,SPSS,spss 23,False,True
"Accurate mapping of next-generation sequencing (NGS) reads to reference genomes is crucial for almost all NGS applications and downstream analyses. Various repetitive elements in human and other higher eukaryotic genomes contribute in large part to ambiguously (non-uniquely) mapped reads. Most available NGS aligners attempt to address this by either removing all non-uniquely mapping reads, or reporting one random or ""best"" hit based on simple heuristics. Accurate estimation of the mapping quality of NGS reads is therefore critical albeit completely lacking at present. Here we developed a generalized software toolkit ""AlignerBoost"", which utilizes a Bayesian-based framework to accurately estimate mapping quality of ambiguously mapped NGS reads. We tested AlignerBoost with both simulated and real DNA-seq and RNA-seq datasets at various thresholds. In most cases, but especially for reads falling within repetitive regions, AlignerBoost dramatically increases the mapping precision of modern NGS aligners without significantly compromising the sensitivity even without mapping quality filters. When using higher mapping quality cutoffs, AlignerBoost achieves a much lower false mapping rate while exhibiting comparable or higher sensitivity compared to the aligner default modes, therefore significantly boosting the detection power of NGS aligners even using extreme thresholds. AlignerBoost is also SNP-aware, and higher quality alignments can be achieved if provided with known SNPs. AlignerBoost’s algorithm is computationally efficient, and can process one million alignments within 30 seconds on a typical desktop computer. AlignerBoost is implemented as a uniform Java application and is freely available at https://github.com/Grice-Lab/AlignerBoost.",what uses ?,Java,bayesian -,False,False
"Detecting similarities between ligand binding sites in the absence of global homology between target proteins has been recognized as one of the critical components of modern drug discovery. Local binding site alignments can be constructed using sequence order-independent techniques, however, to achieve a high accuracy, many current algorithms for binding site comparison require high-quality experimental protein structures, preferably in the bound conformational state. This, in turn, complicates proteome scale applications, where only various quality structure models are available for the majority of gene products. To improve the state-of-the-art, we developed eMatchSite, a new method for constructing sequence order-independent alignments of ligand binding sites in protein models. Large-scale benchmarking calculations using adenine-binding pockets in crystal structures demonstrate that eMatchSite generates accurate alignments for almost three times more protein pairs than SOIPPA. More importantly, eMatchSite offers a high tolerance to structural distortions in ligand binding regions in protein models. For example, the percentage of correctly aligned pairs of adenine-binding sites in weakly homologous protein models is only 4–9% lower than those aligned using crystal structures. This represents a significant improvement over other algorithms, e.g. the performance of eMatchSite in recognizing similar binding sites is 6% and 13% higher than that of SiteEngine using high- and moderate-quality protein models, respectively. Constructing biologically correct alignments using predicted ligand binding sites in protein models opens up the possibility to investigate drug-protein interaction networks for complete proteomes with prospective systems-level applications in polypharmacology and rational drug repositioning. eMatchSite is freely available to the academic community as a web-server and a stand-alone software distribution at http://www.brylinski.org/ematchsite.",what uses ?,eMatchSite,"ematchsite,",True,True
"Nonribosomally and ribosomally synthesized bioactive peptides constitute a source of molecules of great biomedical importance, including antibiotics such as penicillin, immunosuppressants such as cyclosporine, and cytostatics such as bleomycin. Recently, an innovative mass-spectrometry-based strategy, peptidogenomics, has been pioneered to effectively mine microbial strains for novel peptidic metabolites. Even though mass-spectrometric peptide detection can be performed quite fast, true high-throughput natural product discovery approaches have still been limited by the inability to rapidly match the identified tandem mass spectra to the gene clusters responsible for the biosynthesis of the corresponding compounds. With Pep2Path, we introduce a software package to fully automate the peptidogenomics approach through the rapid Bayesian probabilistic matching of mass spectra to their corresponding biosynthetic gene clusters. Detailed benchmarking of the method shows that the approach is powerful enough to correctly identify gene clusters even in data sets that consist of hundreds of genomes, which also makes it possible to match compounds from unsequenced organisms to closely related biosynthetic gene clusters in other genomes. Applying Pep2Path to a data set of compounds without known biosynthesis routes, we were able to identify candidate gene clusters for the biosynthesis of five important compounds. Notably, one of these clusters was detected in a genome from a different subphylum of Proteobacteria than that in which the molecule had first been identified. All in all, our approach paves the way towards high-throughput discovery of novel peptidic natural products. Pep2Path is freely available from http://pep2path.sourceforge.net/, implemented in Python, licensed under the GNU General Public License v3 and supported on MS Windows, Linux and Mac OS X.",what uses ?,Windows,"pep2path,",False,False
"The quantification of cell shape, cell migration, and cell rearrangements is important for addressing classical questions in developmental biology such as patterning and tissue morphogenesis. Time-lapse microscopic imaging of transgenic embryos expressing fluorescent reporters is the method of choice for tracking morphogenetic changes and establishing cell lineages and fate maps in vivo. However, the manual steps involved in curating thousands of putative cell segmentations have been a major bottleneck in the application of these technologies especially for cell membranes. Segmentation of cell membranes while more difficult than nuclear segmentation is necessary for quantifying the relations between changes in cell morphology and morphogenesis. We present a novel and fully automated method to first reconstruct membrane signals and then segment out cells from 3D membrane images even in dense tissues. The approach has three stages: 1) detection of local membrane planes, 2) voting to fill structural gaps, and 3) region segmentation. We demonstrate the superior performance of the algorithms quantitatively on time-lapse confocal and two-photon images of zebrafish neuroectoderm and paraxial mesoderm by comparing its results with those derived from human inspection. We also compared with synthetic microscopic images generated by simulating the process of imaging with fluorescent reporters under varying conditions of noise. Both the over-segmentation and under-segmentation percentages of our method are around 5%. The volume overlap of individual cells, compared to expert manual segmentation, is consistently over 84%. By using our software (ACME) to study somite formation, we were able to segment touching cells with high accuracy and reliably quantify changes in morphogenetic parameters such as cell shape and size, and the arrangement of epithelial and mesenchymal cells. Our software has been developed and tested on Windows, Mac, and Linux platforms and is available publicly under an open source BSD license (https://github.com/krm15/ACME).",what uses ?,Windows,linux,False,False
"Surveys of 16S rDNA sequences from the honey bee, Apis mellifera, have revealed the presence of eight distinctive bacterial phylotypes in intestinal tracts of adult worker bees. Because previous studies have been limited to relatively few sequences from samples pooled from multiple hosts, the extent of variation in this microbiota among individuals within and between colonies and locations has been unclear. We surveyed the gut microbiota of 40 individual workers from two sites, Arizona and Maryland USA, sampling four colonies per site. Universal primers were used to amplify regions of 16S ribosomal RNA genes, and amplicons were sequenced using 454 pyrotag methods, enabling analysis of about 330,000 bacterial reads. Over 99% of these sequences belonged to clusters for which the first blastn hits in GenBank were members of the known bee phylotypes. Four phylotypes, one within Gammaproteobacteria (corresponding to “Candidatus Gilliamella apicola”) one within Betaproteobacteria (“Candidatus Snodgrassella alvi”), and two within Lactobacillus, were present in every bee, though their frequencies varied. The same typical bacterial phylotypes were present in all colonies and at both sites. Community profiles differed significantly among colonies and between sites, mostly due to the presence in some Arizona colonies of two species of Enterobacteriaceae not retrieved previously from bees. Analysis of Sanger sequences of rRNA of the Snodgrassella and Gilliamella phylotypes revealed that single bees contain numerous distinct strains of each phylotype. Strains showed some differentiation between localities, especially for the Snodgrassella phylotype.",what uses ?,Blastn,454 pyrotag,False,False
"Live-cell imaging by light microscopy has demonstrated that all cells are spatially and temporally organized. Quantitative, computational image analysis is an important part of cellular imaging, providing both enriched information about individual cell properties and the ability to analyze large datasets. However, such studies are often limited by the small size and variable shape of objects of interest. Here, we address two outstanding problems in bacterial cell division by developing a generally applicable, standardized, and modular software suite termed Projected System of Internal Coordinates from Interpolated Contours (PSICIC) that solves common problems in image quantitation. PSICIC implements interpolated-contour analysis for accurate and precise determination of cell borders and automatically generates internal coordinate systems that are superimposable regardless of cell geometry. We have used PSICIC to establish that the cell-fate determinant, SpoIIE, is asymmetrically localized during Bacillus subtilis sporulation, thereby demonstrating the ability of PSICIC to discern protein localization features at sub-pixel scales. We also used PSICIC to examine the accuracy of cell division in Esherichia coli and found a new role for the Min system in regulating division-site placement throughout the cell length, but only prior to the initiation of cell constriction. These results extend our understanding of the regulation of both asymmetry and accuracy in bacterial division while demonstrating the general applicability of PSICIC as a computational approach for quantitative, high-throughput analysis of cellular images.",what uses ?,PSICIC,psicic ),True,True
"A calibrated computational model reflects behaviours that are expected or observed in a complex system, providing a baseline upon which sensitivity analysis techniques can be used to analyse pathways that may impact model responses. However, calibration of a model where a behaviour depends on an intervention introduced after a defined time point is difficult, as model responses may be dependent on the conditions at the time the intervention is applied. We present ASPASIA (Automated Simulation Parameter Alteration and SensItivity Analysis), a cross-platform, open-source Java toolkit that addresses a key deficiency in software tools for understanding the impact an intervention has on system behaviour for models specified in Systems Biology Markup Language (SBML). ASPASIA can generate and modify models using SBML solver output as an initial parameter set, allowing interventions to be applied once a steady state has been reached. Additionally, multiple SBML models can be generated where a subset of parameter values are perturbed using local and global sensitivity analysis techniques, revealing the model’s sensitivity to the intervention. To illustrate the capabilities of ASPASIA, we demonstrate how this tool has generated novel hypotheses regarding the mechanisms by which Th17-cell plasticity may be controlled in vivo. By using ASPASIA in conjunction with an SBML model of Th17-cell polarisation, we predict that promotion of the Th1-associated transcription factor T-bet, rather than inhibition of the Th17-associated transcription factor RORγt, is sufficient to drive switching of Th17 cells towards an IFN-γ-producing phenotype. Our approach can be applied to all SBML-encoded models to predict the effect that intervention strategies have on system behaviour. ASPASIA, released under the Artistic License (2.0), can be downloaded from http://www.york.ac.uk/ycil/software.",what uses ?,Java,aspasia,False,False
"Modern DNA sequencing technologies enable geneticists to rapidly identify genetic variation among many human genomes. However, isolating the minority of variants underlying disease remains an important, yet formidable challenge for medical genetics. We have developed GEMINI (GEnome MINIng), a flexible software package for exploring all forms of human genetic variation. Unlike existing tools, GEMINI integrates genetic variation with a diverse and adaptable set of genome annotations (e.g., dbSNP, ENCODE, UCSC, ClinVar, KEGG) into a unified database to facilitate interpretation and data exploration. Whereas other methods provide an inflexible set of variant filters or prioritization methods, GEMINI allows researchers to compose complex queries based on sample genotypes, inheritance patterns, and both pre-installed and custom genome annotations. GEMINI also provides methods for ad hoc queries and data exploration, a simple programming interface for custom analyses that leverage the underlying database, and both command line and graphical tools for common analyses. We demonstrate GEMINI's utility for exploring variation in personal genomes and family based genetic studies, and illustrate its ability to scale to studies involving thousands of human samples. GEMINI is designed for reproducibility and flexibility and our goal is to provide researchers with a standard framework for medical genomics.",what uses ?,GEMINI,gemini ( genome mining ),False,True
"Objective Culture plays a significant role in determining family responsibilities and possibly influences the caregiver burden associated with providing care for a relative with dementia. This study was carried out to determine the elements of caregiver burden in Trinidadians regarding which interventions will provide the most benefit. Methods Seventy-five caregivers of patients diagnosed with dementia participated in this investigation. Demographic data were recorded for each caregiver and patient. Caregiver burden was assessed using the Zarit Burden Interview (ZBI), and the General Health Questionnaire (GHQ) was used as a measure of psychiatric morbidity. Statistical analyses were performed using Stata and SPSS software. Associations between individual ZBI items and GHQ-28 scores in caregivers were analyzed in logistic regression models; the above-median GHQ-28 scores were used a binary dependent variable, and individual ZBI item scores were entered as 5-point ordinal independent variables. Results The caregiver sample was composed of 61 females and 14 males. Caregiver burden was significantly associated with the participant being male; there was heterogeneity by ethnic group, and a higher burden on female caregivers was detected at borderline levels of significance. Upon examining the associations between different ZBI items and the above-median GHQ-28 scores in caregivers, the strongest associations were found with domains reflecting the caregiver’s health having suffered, the caregiver not having sufficient time for him/herself, the caregiver’s social life suffering, and the caregiver admitting to feeling stressed due to caregiving and meeting other responsibilities. Conclusions In this sample, with a majority of female caregivers, the factors of the person with dementia being male and belonging to a minority ethnic group were associated with a greater degree of caregiver burden. The information obtained through the association of individual ZBI items and above-median GHQ-28 scores is a helpful guide for profiling Trinidadian caregiver burden.",what uses ?,SPSS,stata and spss,False,True
"Background Most of child mortality and under nutrition in developing world were attributed to suboptimal childcare and feeding, which needs detailed investigation beyond the proximal factors. This study was conducted with the aim of assessing associations of women’s autonomy and men’s involvement with child anthropometric indices in cash crop livelihood areas of South West Ethiopia. Methods Multi-stage stratified sampling was used to select 749 farming households living in three coffee producing sub-districts of Jimma zone, Ethiopia. Domains of women’s Autonomy were measured by a tool adapted from demographic health survey. A model for determination of paternal involvement in childcare was employed. Caring practices were assessed through the WHO Infant and young child feeding practice core indicators. Length and weight measurements were taken in duplicate using standard techniques. Data were analyzed using SPSS for windows version 21. A multivariable linear regression was used to predict weight for height Z-scores and length for age Z-scores after adjusting for various factors. Results The mean (sd) scores of weight for age (WAZ), height for age (HAZ), weight for height (WHZ) and BMI for age (BAZ) was -0.52(1.26), -0.73(1.43), -0.13(1.34) and -0.1(1.39) respectively. The results of multi variable linear regression analyses showed that WHZ scores of children of mothers who had autonomy of conducting big purchase were higher by 0.42 compared to children's whose mothers had not. In addition, a child whose father was involved in childcare and feeding had higher HAZ score by 0.1. Regarding age, as for every month increase in age of child, a 0.04 point decrease in HAZ score and a 0.01 point decrease in WHZ were noted. Similarly, a child living in food insecure households had lower HAZ score by 0.29 compared to child of food secured households. As family size increased by a person a WHZ score of a child is decreased by 0.08. WHZ and HAZ scores of male child was found lower by 0.25 and 0.38 respectively compared to a female child of same age. Conclusion Women’s autonomy and men’s involvement appeared in tandem with better child anthropometric outcomes. Nutrition interventions in such setting should integrate enhancing women’s autonomy over resource and men’s involvement in childcare and feeding, in addition to food security measures.",what uses ?,SPSS,spss for windows version 21.,False,True
"Background Heart Healthy Lenoir is a transdisciplinary project aimed at creating long-term, sustainable approaches to reduce cardiovascular disease risk disparities in Lenoir County, North Carolina using a design spanning genomic analysis and clinical intervention. We hypothesized that residents of Lenoir County would be unfamiliar and mistrustful of genomic research, and therefore reluctant to participate; additionally, these feelings would be higher in African-Americans. Methodology To test our hypothesis, we conducted qualitative research using community-based participatory research principles to ensure our genomic research strategies addressed the needs, priorities, and concerns of the community. African-American (n = 19) and White (n = 16) adults in Lenoir County participated in four focus groups exploring perceptions about genomics and cardiovascular disease. Demographic surveys were administered and a semi-structured interview guide was used to facilitate discussions. The discussions were digitally recorded, transcribed verbatim, and analyzed in ATLAS.ti. Results and Significance From our analysis, key themes emerged: transparent communication, privacy, participation incentives and barriers, knowledge, and the impact of knowing. African-Americans were more concerned about privacy and community impact compared to Whites, however, African-Americans were still eager to participate in our genomic research project. The results from our formative study were used to improve the informed consent and recruitment processes by: 1) reducing misconceptions of genomic studies; and 2) helping to foster participant understanding and trust with the researchers. Our study demonstrates how community-based participatory research principles can be used to gain deeper insight into the community and increase participation in genomic research studies. Due in part to these efforts 80.3% of eligible African-American participants and 86.9% of eligible White participants enrolled in the Heart Healthy Lenoir Genomics study making our overall enrollment 57.8% African-American. Future research will investigate return of genomic results in the Lenoir community.",what uses ?,ATLAS.ti,,False,False
"Introduction In Ethiopia, the burden of malaria during pregnancy remains a public health problem. Having a good malaria knowledge leads to practicing the prevention of malaria and seeking a health care. Researches regarding pregnant women’s knowledge on malaria in Ethiopia is limited. So the aim of this study was to assess malaria knowledge and its associated factors among pregnant woman, 2018. Methods An institutional-basedcross-sectional study was conducted in Adis Zemen Hospital. Data were collected using pre-tested, an interviewer-administered structured questionnaire among 236 mothers. Women’s knowledge on malaria was measured using six malaria-related questions (cause of malaria, mode of transmission, signs and symptoms, complication and prevention of malaria). The collected data were entered using Epidata version 3.1 and exported to SPSS version 20 for analysis. Bivariate and multivariate logistic regressions were computed to identify predictor variables at 95% confidence interval. Variables having P value of <0.05 were considered as predictor variables of malaria knowledge. Result A total of 235 pregnant women participated which makes the response rate 99.6%. One hundred seventy two pregnant women (73.2%) of mothers had good knowledge on malaria.Women who were from urban (AOR; 2.4: CI; 1.8, 5.7), had better family monthly income (AOR; 3.4: CI; 2.7, 3.8), attended education (AOR; 1.8: CI; 1.4, 3.5) were more knowledgeable. Conclusion and recommendation Majority of participants had good knowledge on malaria. Educational status, household monthly income and residence werepredictors of malaria knowledge. Increasing women’s knowledge especially for those who are from rural, have no education, and have low monthly income is still needed.",what uses ?,SPSS,epidata,False,False
"Recent studies of the human genome have indicated that regulatory elements (e.g. promoters and enhancers) at distal genomic locations can interact with each other via chromatin folding and affect gene expression levels. Genomic technologies for mapping interactions between DNA regions, e.g., ChIA-PET and HiC, can generate genome-wide maps of interactions between regulatory elements. These interaction datasets are important resources to infer distal gene targets of non-coding regulatory elements and to facilitate prioritization of critical loci for important cellular functions. With the increasing diversity and complexity of genomic information and public ontologies, making sense of these datasets demands integrative and easy-to-use software tools. Moreover, network representation of chromatin interaction maps enables effective data visualization, integration, and mining. Currently, there is no software that can take full advantage of network theory approaches for the analysis of chromatin interaction datasets. To fill this gap, we developed a web-based application, QuIN, which enables: 1) building and visualizing chromatin interaction networks, 2) annotating networks with user-provided private and publicly available functional genomics and interaction datasets, 3) querying network components based on gene name or chromosome location, and 4) utilizing network based measures to identify and prioritize critical regulatory targets and their direct and indirect interactions. AVAILABILITY: QuIN’s web server is available at http://quin.jax.org QuIN is developed in Java and JavaScript, utilizing an Apache Tomcat web server and MySQL database and the source code is available under the GPLV3 license available on GitHub: https://github.com/UcarLab/QuIN/.",what uses ?,MySQL,"java and javascript,",False,False
"I introduce an open-source R package ‘dcGOR’ to provide the bioinformatics community with the ease to analyse ontologies and protein domain annotations, particularly those in the dcGO database. The dcGO is a comprehensive resource for protein domain annotations using a panel of ontologies including Gene Ontology. Although increasing in popularity, this database needs statistical and graphical support to meet its full potential. Moreover, there are no bioinformatics tools specifically designed for domain ontology analysis. As an add-on package built in the R software environment, dcGOR offers a basic infrastructure with great flexibility and functionality. It implements new data structure to represent domains, ontologies, annotations, and all analytical outputs as well. For each ontology, it provides various mining facilities, including: (i) domain-based enrichment analysis and visualisation; (ii) construction of a domain (semantic similarity) network according to ontology annotations; and (iii) significance analysis for estimating a contact (statistical significance) network. To reduce runtime, most analyses support high-performance parallel computing. Taking as inputs a list of protein domains of interest, the package is able to easily carry out in-depth analyses in terms of functional, phenotypic and diseased relevance, and network-level understanding. More importantly, dcGOR is designed to allow users to import and analyse their own ontologies and annotations on domains (taken from SCOP, Pfam and InterPro) and RNAs (from Rfam) as well. The package is freely available at CRAN for easy installation, and also at GitHub for version control. The dedicated website with reproducible demos can be found at http://supfam.org/dcGOR.",what uses ?,CRAN,github,False,False
"This study considered all articles published in six Public Library of Science (PLOS) journals in 2012 and Web of Science citations for these articles as of May 2015. A total of 2,406 articles were analyzed to examine the relationships between Altmetric Attention Scores (AAS) and Web of Science citations. The AAS for an article, provided by Altmetric aggregates activities surrounding research outputs in social media (news outlet mentions, tweets, blogs, Wikipedia, etc.). Spearman correlation testing was done on all articles and articles with AAS. Further analysis compared the stratified datasets based on percentile ranks of AAS: top 50%, top 25%, top 10%, and top 1%. Comparisons across the six journals provided additional insights. The results show significant positive correlations between AAS and citations with varied strength for all articles and articles with AAS (or social media mentions), as well as for normalized AAS in the top 50%, top 25%, top 10%, and top 1% datasets. Four of the six PLOS journals, Genetics, Pathogens, Computational Biology, and Neglected Tropical Diseases, show significant positive correlations across all datasets. However, for the two journals with high impact factors, PLOS Biology and Medicine, the results are unexpected: the Medicine articles showed no significant correlations but the Biology articles tested positive for correlations with the whole dataset and the set with AAS. Both journals published substantially fewer articles than the other four journals. Further research to validate the AAS algorithm, adjust the weighting scheme, and include appropriate social media sources is needed to understand the potential uses and meaning of AAS in different contexts and its relationship to other metrics.",what uses ?,Altmetric,social media,False,False
"Transforming natural language questions into formal queries is an integral task in Question Answering (QA) systems. QA systems built on knowledge graphs like DBpedia, require a step after natural language processing for linking words, specifically including named entities and relations, to their corresponding entities in a knowledge graph. To achieve this task, several approaches rely on background knowledge bases containing semantically-typed relations, e.g., PATTY, for an extra disambiguation step. Two major factors may affect the performance of relation linking approaches whenever background knowledge bases are accessed: a) limited availability of such semantic knowledge sources, and b) lack of a systematic approach on how to maximize the benefits of the collected knowledge. We tackle this problem and devise SIBKB, a semantic-based index able to capture knowledge encoded on background knowledge bases like PATTY. SIBKB represents a background knowledge base as a bi-partite and a dynamic index over the relation patterns included in the knowledge base. Moreover, we develop a relation linking component able to exploit SIBKB features. The benefits of SIBKB are empirically studied on existing QA benchmarks and observed results suggest that SIBKB is able to enhance the accuracy of relation linking by up to three times.",what uses ?,PATTY,"dbpedia,",False,False
"Nonribosomally and ribosomally synthesized bioactive peptides constitute a source of molecules of great biomedical importance, including antibiotics such as penicillin, immunosuppressants such as cyclosporine, and cytostatics such as bleomycin. Recently, an innovative mass-spectrometry-based strategy, peptidogenomics, has been pioneered to effectively mine microbial strains for novel peptidic metabolites. Even though mass-spectrometric peptide detection can be performed quite fast, true high-throughput natural product discovery approaches have still been limited by the inability to rapidly match the identified tandem mass spectra to the gene clusters responsible for the biosynthesis of the corresponding compounds. With Pep2Path, we introduce a software package to fully automate the peptidogenomics approach through the rapid Bayesian probabilistic matching of mass spectra to their corresponding biosynthetic gene clusters. Detailed benchmarking of the method shows that the approach is powerful enough to correctly identify gene clusters even in data sets that consist of hundreds of genomes, which also makes it possible to match compounds from unsequenced organisms to closely related biosynthetic gene clusters in other genomes. Applying Pep2Path to a data set of compounds without known biosynthesis routes, we were able to identify candidate gene clusters for the biosynthesis of five important compounds. Notably, one of these clusters was detected in a genome from a different subphylum of Proteobacteria than that in which the molecule had first been identified. All in all, our approach paves the way towards high-throughput discovery of novel peptidic natural products. Pep2Path is freely available from http://pep2path.sourceforge.net/, implemented in Python, licensed under the GNU General Public License v3 and supported on MS Windows, Linux and Mac OS X.",what uses ?,Linux,"pep2path,",False,False
"Objective Culture plays a significant role in determining family responsibilities and possibly influences the caregiver burden associated with providing care for a relative with dementia. This study was carried out to determine the elements of caregiver burden in Trinidadians regarding which interventions will provide the most benefit. Methods Seventy-five caregivers of patients diagnosed with dementia participated in this investigation. Demographic data were recorded for each caregiver and patient. Caregiver burden was assessed using the Zarit Burden Interview (ZBI), and the General Health Questionnaire (GHQ) was used as a measure of psychiatric morbidity. Statistical analyses were performed using Stata and SPSS software. Associations between individual ZBI items and GHQ-28 scores in caregivers were analyzed in logistic regression models; the above-median GHQ-28 scores were used a binary dependent variable, and individual ZBI item scores were entered as 5-point ordinal independent variables. Results The caregiver sample was composed of 61 females and 14 males. Caregiver burden was significantly associated with the participant being male; there was heterogeneity by ethnic group, and a higher burden on female caregivers was detected at borderline levels of significance. Upon examining the associations between different ZBI items and the above-median GHQ-28 scores in caregivers, the strongest associations were found with domains reflecting the caregiver’s health having suffered, the caregiver not having sufficient time for him/herself, the caregiver’s social life suffering, and the caregiver admitting to feeling stressed due to caregiving and meeting other responsibilities. Conclusions In this sample, with a majority of female caregivers, the factors of the person with dementia being male and belonging to a minority ethnic group were associated with a greater degree of caregiver burden. The information obtained through the association of individual ZBI items and above-median GHQ-28 scores is a helpful guide for profiling Trinidadian caregiver burden.",what uses ?,Stata,stata and spss,False,True
"Recent studies of the human genome have indicated that regulatory elements (e.g. promoters and enhancers) at distal genomic locations can interact with each other via chromatin folding and affect gene expression levels. Genomic technologies for mapping interactions between DNA regions, e.g., ChIA-PET and HiC, can generate genome-wide maps of interactions between regulatory elements. These interaction datasets are important resources to infer distal gene targets of non-coding regulatory elements and to facilitate prioritization of critical loci for important cellular functions. With the increasing diversity and complexity of genomic information and public ontologies, making sense of these datasets demands integrative and easy-to-use software tools. Moreover, network representation of chromatin interaction maps enables effective data visualization, integration, and mining. Currently, there is no software that can take full advantage of network theory approaches for the analysis of chromatin interaction datasets. To fill this gap, we developed a web-based application, QuIN, which enables: 1) building and visualizing chromatin interaction networks, 2) annotating networks with user-provided private and publicly available functional genomics and interaction datasets, 3) querying network components based on gene name or chromosome location, and 4) utilizing network based measures to identify and prioritize critical regulatory targets and their direct and indirect interactions. AVAILABILITY: QuIN’s web server is available at http://quin.jax.org QuIN is developed in Java and JavaScript, utilizing an Apache Tomcat web server and MySQL database and the source code is available under the GPLV3 license available on GitHub: https://github.com/UcarLab/QuIN/.",what uses ?,Tomcat,"java and javascript,",False,False
"<jats:title>Abstract</jats:title><jats:p>Somatic copy number variations (CNVs) play a crucial role in development of many human cancers. The broad availability of next-generation sequencing data has enabled the development of algorithms to computationally infer CNV profiles from a variety of data types including exome and targeted sequence data; currently the most prevalent types of cancer genomics data. However, systemic evaluation and comparison of these tools remains challenging due to a lack of ground truth reference sets. To address this need, we have developed Bamgineer, a tool written in Python to introduce user-defined haplotype-phased allele-specific copy number events into an existing Binary Alignment Mapping (BAM) file, with a focus on targeted and exome sequencing experiments. As input, this tool requires a read alignment file (BAM format), lists of non-overlapping genome coordinates for introduction of gains and losses (bed file), and an optional file defining known haplotypes (vcf format). To improve runtime performance, Bamgineer introduces the desired CNVs in parallel using queuing and parallel processing on a local machine or on a high-performance computing cluster. As proof-of-principle, we applied Bamgineer to a single high-coverage (mean: 220X) exome sequence file from a blood sample to simulate copy number profiles of 3 exemplar tumors from each of 10 tumor types at 5 tumor cellularity levels (20-100%, 150 BAM files in total). To demonstrate feasibility beyond exome data, we introduced read alignments to a targeted 5-gene cell-free DNA sequencing library to simulate <jats:italic>EGFR</jats:italic> amplifications at frequencies consistent with circulating tumor DNA (10, 1, 0.1 and 0.01%) while retaining the multimodal insert size distribution of the original data. We expect Bamgineer to be of use for development and systematic benchmarking of CNV calling algorithms by users using locally-generated data for a variety of applications. The source code is freely available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://github.com/pughlab/bamgineer"">http://github.com/pughlab/bamgineer</jats:ext-link>.</jats:p><jats:sec><jats:title>Author summary</jats:title><jats:p>We present Bamgineer, a software program to introduce user-defined, haplotype-specific copy number variants (CNVs) at any frequency into standard Binary Alignment Mapping (BAM) files. Copy number gains are simulated by introducing new DNA sequencing read pairs sampled from existing reads and modified to contain SNPs of the haplotype of interest. This approach retains biases of the original data such as local coverage, strand bias, and insert size. Deletions are simulated by removing reads corresponding to one or both haplotypes. In our proof-of-principle study, we simulated copy number profiles from 10 cancer types at varying cellularity levels typically encountered in clinical samples. We also demonstrated introduction of low frequency CNVs into cell-free DNA sequencing data that retained the bimodal fragment size distribution characteristic of these data. Bamgineer is flexible and enables users to simulate CNVs that reflect characteristics of locally-generated sequence files and can be used for many applications including development and benchmarking of CNV inference tools for a variety of data types.</jats:p></jats:sec>",what uses ?,Python,"bamgineer,",False,False
"Structural and functional brain connectivity are increasingly used to identify and analyze group differences in studies of brain disease. This study presents methods to analyze uni- and bi-modal brain connectivity and evaluate their ability to identify differences. Novel visualizations of significantly different connections comparing multiple metrics are presented. On the global level, “bi-modal comparison plots” show the distribution of uni- and bi-modal group differences and the relationship between structure and function. Differences between brain lobes are visualized using “worm plots”. Group differences in connections are examined with an existing visualization, the “connectogram”. These visualizations were evaluated in two proof-of-concept studies: (1) middle-aged versus elderly subjects; and (2) patients with schizophrenia versus controls. Each included two measures derived from diffusion weighted images and two from functional magnetic resonance images. The structural measures were minimum cost path between two anatomical regions according to the “Statistical Analysis of Minimum cost path based Structural Connectivity” method and the average fractional anisotropy along the fiber. The functional measures were Pearson’s correlation and partial correlation of mean regional time series. The relationship between structure and function was similar in both studies. Uni-modal group differences varied greatly between connectivity types. Group differences were identified in both studies globally, within brain lobes and between regions. In the aging study, minimum cost path was highly effective in identifying group differences on all levels; fractional anisotropy and mean correlation showed smaller differences on the brain lobe and regional levels. In the schizophrenia study, minimum cost path and fractional anisotropy showed differences on the global level and within brain lobes; mean correlation showed small differences on the lobe level. Only fractional anisotropy and mean correlation showed regional differences. The presented visualizations were helpful in comparing and evaluating connectivity measures on multiple levels in both studies.",what uses ?,Statistical Analysis of Minimum cost path based Structural Connectivity,"worm plots ”. group differences in connections are examined with an existing visualization, the “ connectogram ”. these visualizations were evaluated in two proof - of - concept studies : ( 1 ) middle - aged versus elderly subjects ; and ( 2 ) patients with schizophrenia versus controls. each included two measures derived from diffusion weighted images and two from functional magnetic resonance",False,False
"Although automated Acute Lymphoblastic Leukemia (ALL) detection is essential, it is challenging due to the morphological correlation between malignant and normal cells. The traditional ALL classification strategy is arduous, time-consuming, often suffers inter-observer variations, and necessitates experienced pathologists. This article has automated the ALL detection task, employing deep Convolutional Neural Networks (CNNs). We explore the weighted ensemble of deep CNNs to recommend a better ALL cell classifier. The weights are estimated from ensemble candidates' corresponding metrics, such as accuracy, F1-score, AUC, and kappa values. Various data augmentations and pre-processing are incorporated for achieving a better generalization of the network. We train and evaluate the proposed model utilizing the publicly available C-NMC-2019 ALL dataset. Our proposed weighted ensemble model has outputted a weighted F1-score of 88.6%, a balanced accuracy of 86.2%, and an AUC of 0.941 in the preliminary test set. The qualitative results displaying the gradient class activation maps confirm that the introduced model has a concentrated learned region. In contrast, the ensemble candidate models, such as Xception, VGG-16, DenseNet-121, MobileNet, and InceptionResNet-V2, separately produce coarse and scatter learned areas for most example cases. Since the proposed ensemble yields a better result for the aimed task, it can experiment in other domains of medical diagnostic applications.",what Used models ?,MobileNet,"vgg - 16, densenet - 121, mobilenet, and inceptionresnet - v2,",False,True
"Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain‐based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a “black box” that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain‐based disorders which aim to overcome these limitations. We used an artificial neural network known as “deep autoencoder” to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n = 263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p < .005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations.",what Used models ?,Autoencoder,deep autoencoder ”,False,True
"Leukemia is a fatal cancer and has two main types: Acute and chronic. Each type has two more subtypes: Lymphoid and myeloid. Hence, in total, there are four subtypes of leukemia. This study proposes a new approach for diagnosis of all subtypes of leukemia from microscopic blood cell images using convolutional neural networks (CNN), which requires a large training data set. Therefore, we also investigated the effects of data augmentation for an increasing number of training samples synthetically. We used two publicly available leukemia data sources: ALL-IDB and ASH Image Bank. Next, we applied seven different image transformation techniques as data augmentation. We designed a CNN architecture capable of recognizing all subtypes of leukemia. Besides, we also explored other well-known machine learning algorithms such as naive Bayes, support vector machine, k-nearest neighbor, and decision tree. To evaluate our approach, we set up a set of experiments and used 5-fold cross-validation. The results we obtained from experiments showed that our CNN model performance has 88.25% and 81.74% accuracy, in leukemia versus healthy and multi-class classification of all subtypes, respectively. Finally, we also showed that the CNN model has a better performance than other well-known machine learning algorithms.",what Used models ?,naive Bayes,"support vector machine, k - nearest neighbor,",False,False
"Although automated Acute Lymphoblastic Leukemia (ALL) detection is essential, it is challenging due to the morphological correlation between malignant and normal cells. The traditional ALL classification strategy is arduous, time-consuming, often suffers inter-observer variations, and necessitates experienced pathologists. This article has automated the ALL detection task, employing deep Convolutional Neural Networks (CNNs). We explore the weighted ensemble of deep CNNs to recommend a better ALL cell classifier. The weights are estimated from ensemble candidates' corresponding metrics, such as accuracy, F1-score, AUC, and kappa values. Various data augmentations and pre-processing are incorporated for achieving a better generalization of the network. We train and evaluate the proposed model utilizing the publicly available C-NMC-2019 ALL dataset. Our proposed weighted ensemble model has outputted a weighted F1-score of 88.6%, a balanced accuracy of 86.2%, and an AUC of 0.941 in the preliminary test set. The qualitative results displaying the gradient class activation maps confirm that the introduced model has a concentrated learned region. In contrast, the ensemble candidate models, such as Xception, VGG-16, DenseNet-121, MobileNet, and InceptionResNet-V2, separately produce coarse and scatter learned areas for most example cases. Since the proposed ensemble yields a better result for the aimed task, it can experiment in other domains of medical diagnostic applications.",what Used models ?,DenseNet-121,"vgg - 16, densenet - 121, mobilenet, and inceptionresnet - v2,",False,False
"An increasing number of people suffering from mental health conditions resort to online resources (specialized websites, social media, etc.) to share their feelings. Early depression detection using social media data through deep learning models can help to change life trajectories and save lives. But the accuracy of these models was not satisfying due to the real-world imbalanced data distributions. To tackle this problem, we propose a deep learning model (X-A-BiLSTM) for depression detection in imbalanced social media data. The X-A-BiLSTM model consists of two essential components: the first one is XGBoost, which is used to reduce data imbalance; and the second one is an Attention-BiLSTM neural network, which enhances classification capacity. The Reddit Self-reported Depression Diagnosis (RSDD) dataset was chosen, which included approximately 9,000 users who claimed to have been diagnosed with depression (”diagnosed users and approximately 107,000 matched control users. Results demonstrate that our approach significantly outperforms the previous state-of-the-art models on the RSDD dataset.",what Used models ?,LSTM,"xgboost,",False,False
"In the diagnosis of mental health disorder, a large portion of the Bipolar Disorder (BD) patients is likely to be misdiagnosed as Unipolar Depression (UD) on initial presentation. As speech is the most natural way to express emotion, this work focuses on tracking emotion profile of elicited speech for short-term mood disorder identification. In this work, the Deep Scattering Spectrum (DSS) and Low Level Descriptors (LLDs) of the elicited speech signals are extracted as the speech features. The hierarchical spectral clustering (HSC) algorithm is employed to adapt the emotion database to the mood disorder database to alleviate the data bias problem. The denoising autoencoder is then used to extract the bottleneck features of DSS and LLDs for better representation. Based on the bottleneck features, a long short term memory (LSTM) is applied to generate the time-varying emotion profile sequence. Finally, given the emotion profile sequence, the HMM-based identification and verification model is used to determine mood disorder. This work collected the elicited emotional speech data from 15 BDs, 15 UDs and 15 healthy controls for system training and evaluation. Five-fold cross validation was employed for evaluation. Experimental results show that the system using the bottleneck feature achieved an identification accuracy of 73.33%, improving by 8.89%, compared to that without bottleneck features. Furthermore, the system with verification mechanism, improving by 4.44%, outperformed that without verification.",what Used models ?,Autoencoder,autoencoder,True,True
"<sec> <title>BACKGROUND</title> <p>Statistical predictions are useful to predict events based on statistical models. The data is useful to determine outcomes based on inputs and calculations. The Crow-AMSAA method will be explored to predict new cases of Coronavirus 19 (COVID19). This method is currently used within engineering reliability design to predict failures and evaluate the reliability growth. The author intents to use this model to predict the COVID19 cases by using daily reported data from Michigan, New York City, U.S.A and other countries. The piece wise Crow-AMSAA (CA) model fits the data very well for the infected cases and deaths at different phases during the start of the COVID19 outbreak. The slope β of the Crow-AMSAA line indicates the speed of the transmission or death rate. The traditional epidemiological model is based on the exponential distribution, but the Crow-AMSAA is the Non Homogeneous Poisson Process (NHPP) which can be used to modeling the complex problem like COVID19, especially when the various mitigation strategies such as social distance, isolation and locking down were implemented by the government at different places.</p> </sec> <sec> <title>OBJECTIVE</title> <p>This paper is to use piece wise Crow-AMSAA method to fit the COVID19 confirmed cases in Michigan, New York City, U.S.A and other countries.</p> </sec> <sec> <title>METHODS</title> <p>piece wise Crow-AMSAA method to fit the COVID19 confirmed cases</p> </sec> <sec> <title>RESULTS</title> <p>From the Crow-AMSAA analysis above, at the beginning of the COVID 19, the infectious cases did not follow the Crow-AMSAA prediction line, but during the outbreak start, the confirmed cases does follow the CA line, the slope β value indicates the pace of the transmission rate or death rate in each case. The piece wise Crow-AMSAA describes the different phases of spreading. This indicates the speed of the transmission rate could change according to the government interference, social distance order or other factors. Comparing the piece wise CA β slopes (β: 1.683-- 0.834--0.092) in China and in U.S.A (β:5.138--10.48--5.259), the speed of infectious rate in U.S.A is much higher than the infectious rate in China. From the piece wise CA plots and summary table 1 of the CA slope βs, the COVID19 spreading has the different behavior at different places and countries where the government implemented the different policy to slow down the spreading.</p> </sec> <sec> <title>CONCLUSIONS</title> <p>From the analysis of data and conclusions from confirmed cases and deaths of COVID 19 in Michigan, New York city, U.S.A, China and other countries, the piece wise Crow-AMSAA method can be used to modeling the spreading of COVID19.</p> </sec>",what Used models ?,Crow-AMSAA ,crow - amsaa,False,False
"Although automated Acute Lymphoblastic Leukemia (ALL) detection is essential, it is challenging due to the morphological correlation between malignant and normal cells. The traditional ALL classification strategy is arduous, time-consuming, often suffers inter-observer variations, and necessitates experienced pathologists. This article has automated the ALL detection task, employing deep Convolutional Neural Networks (CNNs). We explore the weighted ensemble of deep CNNs to recommend a better ALL cell classifier. The weights are estimated from ensemble candidates' corresponding metrics, such as accuracy, F1-score, AUC, and kappa values. Various data augmentations and pre-processing are incorporated for achieving a better generalization of the network. We train and evaluate the proposed model utilizing the publicly available C-NMC-2019 ALL dataset. Our proposed weighted ensemble model has outputted a weighted F1-score of 88.6%, a balanced accuracy of 86.2%, and an AUC of 0.941 in the preliminary test set. The qualitative results displaying the gradient class activation maps confirm that the introduced model has a concentrated learned region. In contrast, the ensemble candidate models, such as Xception, VGG-16, DenseNet-121, MobileNet, and InceptionResNet-V2, separately produce coarse and scatter learned areas for most example cases. Since the proposed ensemble yields a better result for the aimed task, it can experiment in other domains of medical diagnostic applications.",what Used models ?,Xception,"vgg - 16, densenet - 121, mobilenet, and inceptionresnet - v2,",False,False
"In the diagnosis of mental health disorder, a large portion of the Bipolar Disorder (BD) patients is likely to be misdiagnosed as Unipolar Depression (UD) on initial presentation. As speech is the most natural way to express emotion, this work focuses on tracking emotion profile of elicited speech for short-term mood disorder identification. In this work, the Deep Scattering Spectrum (DSS) and Low Level Descriptors (LLDs) of the elicited speech signals are extracted as the speech features. The hierarchical spectral clustering (HSC) algorithm is employed to adapt the emotion database to the mood disorder database to alleviate the data bias problem. The denoising autoencoder is then used to extract the bottleneck features of DSS and LLDs for better representation. Based on the bottleneck features, a long short term memory (LSTM) is applied to generate the time-varying emotion profile sequence. Finally, given the emotion profile sequence, the HMM-based identification and verification model is used to determine mood disorder. This work collected the elicited emotional speech data from 15 BDs, 15 UDs and 15 healthy controls for system training and evaluation. Five-fold cross validation was employed for evaluation. Experimental results show that the system using the bottleneck feature achieved an identification accuracy of 73.33%, improving by 8.89%, compared to that without bottleneck features. Furthermore, the system with verification mechanism, improving by 4.44%, outperformed that without verification.",what Used models ?,LSTM,autoencoder,False,False
"State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\% accuracy for POS tagging and 91.21\% F1 for NER.",what model ?,CRF,neutral network,False,False
"This paper presents work on a method to detect names of proteins in running text. Our system - Yapex - uses a combination of lexical and syntactic knowledge, heuristic filters and a local dynamic dictionary. The syntactic information given by a general-purpose off-the-shelf parser supports the correct identification of the boundaries of protein names, and the local dynamic dictionary finds protein names in positions incompletely analysed by the parser. We present the different steps involved in our approach to protein tagging, and show how combinations of them influence recall and precision. We evaluate the system on a corpus of MEDLINE abstracts and compare it with the KeX system (Fukuda et al., 1998) along four different notions of correctness.",what model ?,Yapex,yapex -,True,True
"We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",what model ?,SciIE,"scierc,",False,False
"We report on two large corpora of semantically annotated full-text biomedical research papers created in order to devel op information extraction ( IE) tools for the TXM project. Both corpora have been annotated with a range of entities (CellLine, Complex, DevelopmentalStage, Disease, DrugCompound, ExperimentalMethod, Fragment, Fusion, GOMOP, Gene, Modification, mRNAcDNA, Mutant, Protein, Tissue), normalisations of selected entities to the NCBI Taxonomy, RefSeq, EntrezGene, ChEBI and MeSH and enriched relations (protein-protein interactions, tissue expressions and fr agment- or mutant-protein relations). While one corpus targets protein-protein interactions ( PPIs), the focus of other is on tissue expressions ( TEs). This paper describes the selected markables and the annotation process of the ITI TXM corpora, and provides a detailed breakdown of the inter-annotator agreement (IAA).",what Other resources ?,NCBI Taxonomy,,False,False
"The mentions of human health perturbations such as the diseases and adverse effects denote a special entity class in the biomedical literature. They help in understanding the underlying risk factors and develop a preventive rationale. The recognition of these named entities in texts through dictionary-based approaches relies on the availability of appropriate terminological resources. Although few resources are publicly available, not all are suitable for the text mining needs. Therefore, this work provides an overview of the well known resources with respect to human diseases and adverse effects such as the MeSH, MedDRA, ICD-10, SNOMED CT, and UMLS. Individual dictionaries are generated from these resources and their performance in recognizing the named entities is evaluated over a manually annotated corpus. In addition, the steps for curating the dictionaries, rule-based acronym disambiguation and their impact on the dictionary performance is discussed. The results show that the MedDRA and UMLS achieve the best recall. Besides this, MedDRA provides an additional benefit of achieving a higher precision. The combination of search results of all the dictionaries achieve a considerably high recall. The corpus is available on http://www.scai.fraunhofer.de/disease-ae-corpus.html",what Other resources ?,MeSH,"meddra, icd - 10, snomed ct,",False,False
"Automatic extraction of biological network information is one of the most desired and most complex tasks in biological and medical text mining. Track 4 at BioCreative V attempts to approach this complexity using fragments of large-scale manually curated biological networks, represented in Biological Expression Language (BEL), as training and test data. BEL is an advanced knowledge representation format which has been designed to be both human readable and machine processable. The specific goal of track 4 was to evaluate text mining systems capable of automatically constructing BEL statements from given evidence text, and of retrieving evidence text for given BEL statements. Given the complexity of the task, we designed an evaluation methodology which gives credit to partially correct statements. We identified various levels of information expressed by BEL statements, such as entities, functions, relations, and introduced an evaluation framework which rewards systems capable of delivering useful BEL fragments at each of these levels. The aim of this evaluation method is to help identify the characteristics of the systems which, if combined, would be most useful for achieving the overall goal of automatically constructing causal biological networks from text.",what Other resources ?,Biological Expression Language (BEL),"large - scale manually curated biological networks,",False,False
"We present a database of annotated biomedical text corpora merged into a portable data structure with uniform conventions. MedTag combines three corpora, MedPost, ABGene and GENETAG, within a common relational database data model. The GENETAG corpus has been modified to reflect new definitions of genes and proteins. The MedPost corpus has been updated to include 1,000 additional sentences from the clinical medicine domain. All data have been updated with original MEDLINE text excerpts, PubMed identifiers, and tokenization independence to facilitate data accuracy, consistency and usability. The data are available in flat files along with software to facilitate loading the data into a relational SQL database from ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith/MedTag/medtag.tar.gz.",what Other resources ?,GENETAG,abgene,False,False
"We report on two large corpora of semantically annotated full-text biomedical research papers created in order to devel op information extraction ( IE) tools for the TXM project. Both corpora have been annotated with a range of entities (CellLine, Complex, DevelopmentalStage, Disease, DrugCompound, ExperimentalMethod, Fragment, Fusion, GOMOP, Gene, Modification, mRNAcDNA, Mutant, Protein, Tissue), normalisations of selected entities to the NCBI Taxonomy, RefSeq, EntrezGene, ChEBI and MeSH and enriched relations (protein-protein interactions, tissue expressions and fr agment- or mutant-protein relations). While one corpus targets protein-protein interactions ( PPIs), the focus of other is on tissue expressions ( TEs). This paper describes the selected markables and the annotation process of the ITI TXM corpora, and provides a detailed breakdown of the inter-annotator agreement (IAA).",what Other resources ?,MeSH,,False,False
"A large community of research has been developed in recent years to analyze social media and social networks, with the aim of understanding, discovering insights, and exploiting the available information. The focus has shifted from conventional polarity classification to contemporary application-oriented fine-grained aspects such as, emotions, sarcasm, stance, rumor, and hate speech detection in the user-generated content. Detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. The majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. In this paper, we propose a deep learning model called sAtt-BLSTM convNet that is based on the hybrid of soft attention-based bidirectional long short-term memory (sAtt-BLSTM) and convolution neural network (convNet) applying global vectors for word representation (GLoVe) for building semantic word embeddings. In addition to the feature maps generated by the sAtt-BLSTM, punctuation-based auxiliary features are also merged into the convNet. The robustness of the proposed model is investigated using balanced (tweets from benchmark SemEval 2015 Task 11) and unbalanced (approximately 40000 random tweets using the Sarcasm Detector tool with 15000 sarcastic and 25000 non-sarcastic messages) datasets. An experimental study using the training- and test-set accuracy metrics is performed to compare the proposed deep neural model with convNet, LSTM, and bidirectional LSTM with/without attention and it is observed that the novel sAtt-BLSTM convNet model outperforms others with a superior sarcasm-classification accuracy of 97.87% for the Twitter dataset and 93.71% for the random-tweet dataset.",what Data ?,"lexical, syntactic, or pragmatic features","hate speech detection in the user - generated content. detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. the majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. in this paper, we propose a deep learning model called satt - blstm convnet that is based on the hybrid of soft attention - based bidirectional long short - term memory ( satt - blstm ) and convolution neural network ( convnet ) applying global vectors for word representation ( glove ) for building semantic word embeddings. in addition to the feature maps generated by the satt - blstm, punctuation - based auxiliary features are also merged into the convnet. the robustness of the proposed model is investigated using balanced ( tweets from benchmark semeval 2015 task 11 ) and unbalanced ( approximately 40000 random tweets using the sarcasm detector tool with 15000 sarcastic and 25000 non - sarcastic messages ) datasets. an experimental study using the training - and test - set accuracy",False,True
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Data ?,4.42±0.30mg/L,multiple comparisons,False,False
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Data ?,mean±SD of 2.93±0.33mg/L,multiple comparisons,False,False
"The Logical Observation Identifiers, Names and Codes (LOINC) is a common terminology used for standardizing laboratory terms. Within the consortium of the HiGHmed project, LOINC is one of the central terminologies used for health data sharing across all university sites. Therefore, linking the LOINC codes to the site-specific tests and measures is one crucial step to reach this goal. In this work we report our ongoing efforts in implementing LOINC to our laboratory information system and research infrastructure, as well as our challenges and the lessons learned. 407 local terms could be mapped to 376 LOINC codes of which 209 are already available to routine laboratory data. In our experience, mapping of local terms to LOINC is a widely manual and time consuming process for reasons of language and expert knowledge of local laboratory procedures.",what Data ?,"The Logical Observation Identifiers, Names and Codes (LOINC)",,False,False
"While Wikipedia exists in 287 languages, its content is unevenly distributed among them. In this work, we investigate the generation of open domain Wikipedia summaries in underserved languages using structured data from Wikidata. To this end, we propose a neural network architecture equipped with copy actions that learns to generate single-sentence and comprehensible textual summaries from Wikidata triples. We demonstrate the effectiveness of the proposed approach by evaluating it against a set of baselines on two languages of different natures: Arabic, a morphological rich language with a larger vocabulary than English, and Esperanto, a constructed language known for its easy acquisition.",what Data ?,open domain Wikipedia summaries,single - sentence and comprehensible textual summaries from wikidata triples.,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Data ?,the structural consistency and correctness,"participants, tasks and building data.",False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Data ?,the structural consistency and correctness,"participants, tasks and building data.",False,False
"With the rapid growth of online social media content, and the impact these have made on people’s behavior, many researchers have been interested in studying these media platforms. A major part of their work focused on sentiment analysis and opinion mining. These refer to the automatic identification of opinions of people toward specific topics by analyzing their posts and publications. Multi-class sentiment analysis, in particular, addresses the identification of the exact sentiment conveyed by the user rather than the overall sentiment polarity of his text message or post. That being the case, we introduce a task different from the conventional multi-class classification, which we run on a data set collected from Twitter. We refer to this task as “quantification.” By the term “quantification,” we mean the identification of all the existing sentiments within an online post (i.e., tweet) instead of attributing a single sentiment label to it. For this sake, we propose an approach that automatically attributes different scores to each sentiment in a tweet, and selects the sentiments with the highest scores which we judge as conveyed in the text. To reach this target, we added to our previously introduced tool SENTA the necessary components to run and perform such a task. Throughout this work, we present the added components; we study the feasibility of quantification, and propose an approach to perform it on a data set made of tweets for 11 different sentiment classes. The data set was manually labeled and the results of the automatic analysis were checked against the human annotation. Our experiments show the feasibility of this task and reach an F1 score equal to 45.9%.",what Data ?,highest scores,,False,False
"The Logical Observation Identifiers, Names and Codes (LOINC) is a common terminology used for standardizing laboratory terms. Within the consortium of the HiGHmed project, LOINC is one of the central terminologies used for health data sharing across all university sites. Therefore, linking the LOINC codes to the site-specific tests and measures is one crucial step to reach this goal. In this work we report our ongoing efforts in implementing LOINC to our laboratory information system and research infrastructure, as well as our challenges and the lessons learned. 407 local terms could be mapped to 376 LOINC codes of which 209 are already available to routine laboratory data. In our experience, mapping of local terms to LOINC is a widely manual and time consuming process for reasons of language and expert knowledge of local laboratory procedures.",what Data ?,407 local terms,,False,False
"We show that easily accessible digital records of behavior, Facebook Likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. The analysis presented is based on a dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests. The proposed model uses dimensionality reduction for preprocessing the Likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from Likes. The model correctly discriminates between homosexual and heterosexual men in 88% of cases, African Americans and Caucasian Americans in 95% of cases, and between Democrat and Republican in 85% of cases. For the personality trait “Openness,” prediction accuracy is close to the test–retest accuracy of a standard personality test. We give examples of associations between attributes and Likes and discuss implications for online personalization and privacy.",what Data ?,"Dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests.","likes data,",False,False
"It was recently reported that men self-cite >50% more often than women across a wide variety of disciplines in the bibliographic database JSTOR. Here, we replicate this finding in a sample of 1.6 million papers from Author-ity, a version of PubMed with computationally disambiguated author names. More importantly, we show that the gender effect largely disappears when accounting for prior publication count in a multidimensional statistical model. Gender has the weakest effect on the probability of self-citation among an extensive set of features tested, including byline position, affiliation, ethnicity, collaboration size, time lag, subject-matter novelty, reference/citation counts, publication type, language, and venue. We find that self-citation is the hallmark of productive authors, of any gender, who cite their novel journal publications early and in similar venues, and more often cross citation-barriers such as language and indexing. As a result, papers by authors with short, disrupted, or diverse careers miss out on the initial boost in visibility gained from self-citations. Our data further suggest that this disproportionately affects women because of attrition and not because of disciplinary under-specialization.",what Data ?,prior publication count,"reference / citation counts, publication type, language, and venue.",False,False
"A large community of research has been developed in recent years to analyze social media and social networks, with the aim of understanding, discovering insights, and exploiting the available information. The focus has shifted from conventional polarity classification to contemporary application-oriented fine-grained aspects such as, emotions, sarcasm, stance, rumor, and hate speech detection in the user-generated content. Detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. The majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. In this paper, we propose a deep learning model called sAtt-BLSTM convNet that is based on the hybrid of soft attention-based bidirectional long short-term memory (sAtt-BLSTM) and convolution neural network (convNet) applying global vectors for word representation (GLoVe) for building semantic word embeddings. In addition to the feature maps generated by the sAtt-BLSTM, punctuation-based auxiliary features are also merged into the convNet. The robustness of the proposed model is investigated using balanced (tweets from benchmark SemEval 2015 Task 11) and unbalanced (approximately 40000 random tweets using the Sarcasm Detector tool with 15000 sarcastic and 25000 non-sarcastic messages) datasets. An experimental study using the training- and test-set accuracy metrics is performed to compare the proposed deep neural model with convNet, LSTM, and bidirectional LSTM with/without attention and it is observed that the novel sAtt-BLSTM convNet model outperforms others with a superior sarcasm-classification accuracy of 97.87% for the Twitter dataset and 93.71% for the random-tweet dataset.",what Data ?,15000 sarcastic and 25000 non-sarcastic messages,"hate speech detection in the user - generated content. detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. the majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. in this paper, we propose a deep learning model called satt - blstm convnet that is based on the hybrid of soft attention - based bidirectional long short - term memory ( satt - blstm ) and convolution neural network ( convnet ) applying global vectors for word representation ( glove ) for building semantic word embeddings. in addition to the feature maps generated by the satt - blstm, punctuation - based auxiliary features are also merged into the convnet. the robustness of the proposed model is investigated using balanced ( tweets from benchmark semeval 2015 task 11 ) and unbalanced ( approximately 40000 random tweets using the sarcasm detector tool with 15000 sarcastic and 25000 non - sarcastic messages ) datasets. an experimental study using the training - and test - set accuracy",False,False
"Hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. The need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the Open Biodiversity Knowledge Management System (OBKMS). This paper presents OpenBiodiv: An OBKMS that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. It is presented as a Linked Open Dataset generated from scientific literature. OpenBiodiv encompasses data extracted from more than 5000 scholarly articles published by Pensoft and many more taxonomic treatments extracted by Plazi from journals of other publishers. The data from both sources are converted to Resource Description Framework (RDF) and integrated in a graph database using the OpenBiodiv-O ontology and an RDF version of the Global Biodiversity Information Facility (GBIF) taxonomic backbone. Through the application of semantic technologies, the project showcases the value of open publishing of Findable, Accessible, Interoperable, Reusable (FAIR) data towards the establishment of open science practices in the biodiversity domain.",what Data ?,published articles or monographs,"collective biodiversity knowledge in a community - agreed and interoperable open format has evolved into the concept of the open biodiversity knowledge management system ( obkms ). this paper presents openbiodiv : an obkms that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. it is presented as a linked open dataset generated from scientific literature. openbiodiv encompasses data extracted from more than 5000 scholarly articles published by pensoft and many more taxonomic treatments",False,False
"Depression is a typical mood disorder, which affects people in mental and even physical problems. People who suffer depression always behave abnormal in visual behavior and the voice. In this paper, an audio visual based multimodal depression scale prediction system is proposed. Firstly, features are extracted from video and audio are fused in feature level to represent the audio visual behavior. Secondly, long short memory recurrent neural network (LSTM-RNN) is utilized to encode the dynamic temporal information of the abnormal audio visual behavior. Thirdly, emotion information is utilized by multi-task learning to boost the performance further. The proposed approach is evaluated on the Audio-Visual Emotion Challenge (AVEC2014) dataset. Experiments results show the dimensional emotion recognition helps to depression scale prediction.",what Data ?,Voice,avec2014 ),False,False
"Abstract Presently, analytics degree programs exhibit a growing trend to meet a strong market demand. To explore the skill sets required for analytics positions, the authors examined a sample of online job postings related to professions such as business analyst (BA), business intelligence analyst (BIA), data analyst (DA), and data scientist (DS) using content analysis. They present a ranked list of relevant skills belonging to specific skills categories for the studied positions. Also, they conducted a pairwise comparison between DA and DS as well as BA and BIA. Overall, the authors observed that decision making, organization, communication, and structured data management are key to all job categories. The analysis shows that technical skills like statistics and programming skills are in most demand for DAs. The analysis is useful for creating clear definitions with respect to required skills for job categories in the business and data analytics domain and for designing course curricula for this domain.",what Data ?,sample of online job postings,technical skills like statistics and programming skills,False,False
"A large community of research has been developed in recent years to analyze social media and social networks, with the aim of understanding, discovering insights, and exploiting the available information. The focus has shifted from conventional polarity classification to contemporary application-oriented fine-grained aspects such as, emotions, sarcasm, stance, rumor, and hate speech detection in the user-generated content. Detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. The majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. In this paper, we propose a deep learning model called sAtt-BLSTM convNet that is based on the hybrid of soft attention-based bidirectional long short-term memory (sAtt-BLSTM) and convolution neural network (convNet) applying global vectors for word representation (GLoVe) for building semantic word embeddings. In addition to the feature maps generated by the sAtt-BLSTM, punctuation-based auxiliary features are also merged into the convNet. The robustness of the proposed model is investigated using balanced (tweets from benchmark SemEval 2015 Task 11) and unbalanced (approximately 40000 random tweets using the Sarcasm Detector tool with 15000 sarcastic and 25000 non-sarcastic messages) datasets. An experimental study using the training- and test-set accuracy metrics is performed to compare the proposed deep neural model with convNet, LSTM, and bidirectional LSTM with/without attention and it is observed that the novel sAtt-BLSTM convNet model outperforms others with a superior sarcasm-classification accuracy of 97.87% for the Twitter dataset and 93.71% for the random-tweet dataset.",what Data ?,contemporary application-oriented fine-grained aspects,"hate speech detection in the user - generated content. detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. the majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. in this paper, we propose a deep learning model called satt - blstm convnet that is based on the hybrid of soft attention - based bidirectional long short - term memory ( satt - blstm ) and convolution neural network ( convnet ) applying global vectors for word representation ( glove ) for building semantic word embeddings. in addition to the feature maps generated by the satt - blstm, punctuation - based auxiliary features are also merged into the convnet. the robustness of the proposed model is investigated using balanced ( tweets from benchmark semeval 2015 task 11 ) and unbalanced ( approximately 40000 random tweets using the sarcasm detector tool with 15000 sarcastic and 25000 non - sarcastic messages ) datasets. an experimental study using the training - and test - set accuracy",False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Data ?,the structural consistency and correctness,"participants, tasks and building data.",False,False
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Data ?,10mg/kg body weight,multiple comparisons,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Data ?,"participants, tasks and building data","participants, tasks and building data.",True,True
"It was recently reported that men self-cite >50% more often than women across a wide variety of disciplines in the bibliographic database JSTOR. Here, we replicate this finding in a sample of 1.6 million papers from Author-ity, a version of PubMed with computationally disambiguated author names. More importantly, we show that the gender effect largely disappears when accounting for prior publication count in a multidimensional statistical model. Gender has the weakest effect on the probability of self-citation among an extensive set of features tested, including byline position, affiliation, ethnicity, collaboration size, time lag, subject-matter novelty, reference/citation counts, publication type, language, and venue. We find that self-citation is the hallmark of productive authors, of any gender, who cite their novel journal publications early and in similar venues, and more often cross citation-barriers such as language and indexing. As a result, papers by authors with short, disrupted, or diverse careers miss out on the initial boost in visibility gained from self-citations. Our data further suggest that this disproportionately affects women because of attrition and not because of disciplinary under-specialization.",what Data ?,reference/citation counts,"reference / citation counts, publication type, language, and venue.",False,False
"CDR (Call Detail Record) data are one type of mobile phone data collected by operators each time a user initiates/receives a phone call or sends/receives an sms. CDR data are a rich geo-referenced source of user behaviour information. In this work, we perform an analysis of CDR data for the city of Milan that originate from Telecom Italia Big Data Challenge. A set of graphs is generated from aggregated CDR data, where each node represents a centroid of an RBS (Radio Base Station) polygon, and each edge represents aggregated telecom traffic between two RBSs. To explore the community structure, we apply a modularity-based algorithm. Community structure between days is highly dynamic, with variations in number, size and spatial distribution. One general rule observed is that communities formed over the urban core of the city are small in size and prone to dynamic change in spatial distribution, while communities formed in the suburban areas are larger in size and more consistent with respect to their spatial distribution. To evaluate the dynamics of change in community structure between days, we introduced different graph based and spatial community properties which contain latent footprint of human dynamics. We created land use profiles for each RBS polygon based on the Copernicus Land Monitoring Service Urban Atlas data set to quantify the correlation and predictivennes of human dynamics properties based on land use. The results reveal a strong correlation between some properties and land use which motivated us to further explore this topic. The proposed methodology has been implemented in the programming language Scala inside the Apache Spark engine to support the most computationally intensive tasks and in Python using the rich portfolio of data analytics and machine learning libraries for the less demanding tasks.",what Data ?, Copernicus Land Monitoring Service Urban Atlas,user behaviour information.,False,False
"In this paper, we aim to develop a deep learning based automatic Attention Deficit Hyperactive Disorder (ADHD) diagnosis algorithm using resting state functional magnetic resonance imaging (rs-fMRI) scans. However, relative to millions of parameters in deep neural networks (DNN), the number of fMRI samples is still limited to learn discriminative features from the raw data. In light of this, we first encode our prior knowledge on 3D features voxel-wisely, including Regional Homogeneity (ReHo), fractional Amplitude of Low Frequency Fluctuations (fALFF) and Voxel-Mirrored Homotopic Connectivity (VMHC), and take these 3D images as the input to the DNN. Inspired by the way that radiologists examine brain images, we further investigate a novel 3D convolutional neural network (CNN) architecture to learn 3D local patterns which may boost the diagnosis accuracy. Investigation on the hold-out testing data of the ADHD-200 Global competition demonstrates that the proposed 3D CNN approach yields superior performances when compared to the reported classifiers in the literature, even with less training samples.",what Data ?,fMRI,,False,False
"While Wikipedia exists in 287 languages, its content is unevenly distributed among them. In this work, we investigate the generation of open domain Wikipedia summaries in underserved languages using structured data from Wikidata. To this end, we propose a neural network architecture equipped with copy actions that learns to generate single-sentence and comprehensible textual summaries from Wikidata triples. We demonstrate the effectiveness of the proposed approach by evaluating it against a set of baselines on two languages of different natures: Arabic, a morphological rich language with a larger vocabulary than English, and Esperanto, a constructed language known for its easy acquisition.",what Data ?,single-sentence and comprehensible textual summaries from Wikidata,single - sentence and comprehensible textual summaries from wikidata triples.,False,False
"Hackathons have become an increasingly popular approach for organizations to both test their new products and services as well as to generate new ideas. Most events either focus on attracting external developers or requesting employees of the organization to focus on a specific problem. In this paper we describe extensions to this paradigm that open up the event to internal employees and preserve the open-ended nature of the hackathon itself. In this paper we describe our initial motivation and objectives for conducting an internal hackathon, our experience in pioneering an internal hackathon at AT&T including specific things we did to make the internal hackathon successful. We conclude with the benefits (both expected and unexpected) we achieved from the internal hackathon approach, and recommendations for continuing the use of this valuable tool within AT&T.",what Data ?,benefits (both expected and unexpected),"internal employees and preserve the open - ended nature of the hackathon itself. in this paper we describe our initial motivation and objectives for conducting an internal hackathon,",False,False
"It was recently reported that men self-cite >50% more often than women across a wide variety of disciplines in the bibliographic database JSTOR. Here, we replicate this finding in a sample of 1.6 million papers from Author-ity, a version of PubMed with computationally disambiguated author names. More importantly, we show that the gender effect largely disappears when accounting for prior publication count in a multidimensional statistical model. Gender has the weakest effect on the probability of self-citation among an extensive set of features tested, including byline position, affiliation, ethnicity, collaboration size, time lag, subject-matter novelty, reference/citation counts, publication type, language, and venue. We find that self-citation is the hallmark of productive authors, of any gender, who cite their novel journal publications early and in similar venues, and more often cross citation-barriers such as language and indexing. As a result, papers by authors with short, disrupted, or diverse careers miss out on the initial boost in visibility gained from self-citations. Our data further suggest that this disproportionately affects women because of attrition and not because of disciplinary under-specialization.",what Data ?,1.6 million,"reference / citation counts, publication type, language, and venue.",False,False
"Purpose – The purpose of this paper is to investigate what employers seek when recruiting library and information professionals in the UK and whether professional skills, generic skills or personal qualities are most in demand.Design/methodology/approach – A content analysis of a sample of 180 advertisements requiring a professional library or information qualification from Chartered Institute of Library and Information Professional's Library + Information Gazette over the period May 2006‐2007.Findings – The findings reveal that a multitude of skills and qualities are required in the profession. When the results were compared with Information National Training Organisation and Library and Information Management Employability Skills research, customer service, interpersonal and communication skills, and general computing skills emerged as the requirements most frequently sought by employers. Overall, requirements from the generic skills area were most important to employers, but the research also demonstra...",what Data ?,sample of 180 advertisements,"professional skills, generic skills or personal qualities are most in demand. design / methodology / approach – a content analysis of a sample of 180 advertisements",False,True
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Data ?,pioglitazone 10mg/kg body weight,multiple comparisons,False,False
"It was recently reported that men self-cite >50% more often than women across a wide variety of disciplines in the bibliographic database JSTOR. Here, we replicate this finding in a sample of 1.6 million papers from Author-ity, a version of PubMed with computationally disambiguated author names. More importantly, we show that the gender effect largely disappears when accounting for prior publication count in a multidimensional statistical model. Gender has the weakest effect on the probability of self-citation among an extensive set of features tested, including byline position, affiliation, ethnicity, collaboration size, time lag, subject-matter novelty, reference/citation counts, publication type, language, and venue. We find that self-citation is the hallmark of productive authors, of any gender, who cite their novel journal publications early and in similar venues, and more often cross citation-barriers such as language and indexing. As a result, papers by authors with short, disrupted, or diverse careers miss out on the initial boost in visibility gained from self-citations. Our data further suggest that this disproportionately affects women because of attrition and not because of disciplinary under-specialization.",what Data ?,language,"reference / citation counts, publication type, language, and venue.",False,True
"It has been proven that using structured methods to represent the domain reduces human errors in the process of creating models and also in the process of using them. Using modeling patterns is a proven structural method in this regard. A pattern is a generalizable reusable solution to a design problem. Positive effects of using patterns were demonstrated in several experimental studies and explained using theories. However, detailed knowledge about how properties of patterns lead to increased performance in writing and reading conceptual models is currently lacking. This paper proposes a theoretical framework to characterize the properties of ontology-driven conceptual model patterns. The development of such framework is the first step in investigating the effects of pattern properties and devising rules to compose patterns based on well-understood properties.",what Data ?,pattern properties,,False,False
"A large community of research has been developed in recent years to analyze social media and social networks, with the aim of understanding, discovering insights, and exploiting the available information. The focus has shifted from conventional polarity classification to contemporary application-oriented fine-grained aspects such as, emotions, sarcasm, stance, rumor, and hate speech detection in the user-generated content. Detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. The majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. In this paper, we propose a deep learning model called sAtt-BLSTM convNet that is based on the hybrid of soft attention-based bidirectional long short-term memory (sAtt-BLSTM) and convolution neural network (convNet) applying global vectors for word representation (GLoVe) for building semantic word embeddings. In addition to the feature maps generated by the sAtt-BLSTM, punctuation-based auxiliary features are also merged into the convNet. The robustness of the proposed model is investigated using balanced (tweets from benchmark SemEval 2015 Task 11) and unbalanced (approximately 40000 random tweets using the Sarcasm Detector tool with 15000 sarcastic and 25000 non-sarcastic messages) datasets. An experimental study using the training- and test-set accuracy metrics is performed to compare the proposed deep neural model with convNet, LSTM, and bidirectional LSTM with/without attention and it is observed that the novel sAtt-BLSTM convNet model outperforms others with a superior sarcasm-classification accuracy of 97.87% for the Twitter dataset and 93.71% for the random-tweet dataset.",what Data ?,93.71%,"hate speech detection in the user - generated content. detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. the majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. in this paper, we propose a deep learning model called satt - blstm convnet that is based on the hybrid of soft attention - based bidirectional long short - term memory ( satt - blstm ) and convolution neural network ( convnet ) applying global vectors for word representation ( glove ) for building semantic word embeddings. in addition to the feature maps generated by the satt - blstm, punctuation - based auxiliary features are also merged into the convnet. the robustness of the proposed model is investigated using balanced ( tweets from benchmark semeval 2015 task 11 ) and unbalanced ( approximately 40000 random tweets using the sarcasm detector tool with 15000 sarcastic and 25000 non - sarcastic messages ) datasets. an experimental study using the training - and test - set accuracy",False,False
"It was recently reported that men self-cite >50% more often than women across a wide variety of disciplines in the bibliographic database JSTOR. Here, we replicate this finding in a sample of 1.6 million papers from Author-ity, a version of PubMed with computationally disambiguated author names. More importantly, we show that the gender effect largely disappears when accounting for prior publication count in a multidimensional statistical model. Gender has the weakest effect on the probability of self-citation among an extensive set of features tested, including byline position, affiliation, ethnicity, collaboration size, time lag, subject-matter novelty, reference/citation counts, publication type, language, and venue. We find that self-citation is the hallmark of productive authors, of any gender, who cite their novel journal publications early and in similar venues, and more often cross citation-barriers such as language and indexing. As a result, papers by authors with short, disrupted, or diverse careers miss out on the initial boost in visibility gained from self-citations. Our data further suggest that this disproportionately affects women because of attrition and not because of disciplinary under-specialization.",what Data ?,publication type,"reference / citation counts, publication type, language, and venue.",False,True
"It was recently reported that men self-cite >50% more often than women across a wide variety of disciplines in the bibliographic database JSTOR. Here, we replicate this finding in a sample of 1.6 million papers from Author-ity, a version of PubMed with computationally disambiguated author names. More importantly, we show that the gender effect largely disappears when accounting for prior publication count in a multidimensional statistical model. Gender has the weakest effect on the probability of self-citation among an extensive set of features tested, including byline position, affiliation, ethnicity, collaboration size, time lag, subject-matter novelty, reference/citation counts, publication type, language, and venue. We find that self-citation is the hallmark of productive authors, of any gender, who cite their novel journal publications early and in similar venues, and more often cross citation-barriers such as language and indexing. As a result, papers by authors with short, disrupted, or diverse careers miss out on the initial boost in visibility gained from self-citations. Our data further suggest that this disproportionately affects women because of attrition and not because of disciplinary under-specialization.",what Data ?,"affiliation, ethnicity, collaboration size","reference / citation counts, publication type, language, and venue.",False,False
"Hybrid halide perovskites that are currently intensively studied for photovoltaic applications, also present outstanding properties for light emission. Here, we report on the preparation of bright solid state light emitting diodes (LEDs) based on a solution-processed hybrid lead halide perovskite (Pe). In particular, we have utilized the perovskite generally described with the formula CH3NH3PbI(3-x)Cl(x) and exploited a configuration without electron or hole blocking layer in addition to the injecting layers. Compact TiO2 and Spiro-OMeTAD were used as electron and hole injecting layers, respectively. We have demonstrated a bright combined visible-infrared radiance of 7.1 W·sr(-1)·m(-2) at a current density of 232 mA·cm(-2), and a maximum external quantum efficiency (EQE) of 0.48%. The devices prepared surpass the EQE values achieved in previous reports, considering devices with just an injecting layer without any additional blocking layer. Significantly, the maximum EQE value of our devices is obtained at applied voltages as low as 2 V, with a turn-on voltage as low as the Pe band gap (V(turn-on) = 1.45 ± 0.06 V). This outstanding performance, despite the simplicity of the approach, highlights the enormous potentiality of Pe-LEDs. In addition, we present a stability study of unsealed Pe-LEDs, which demonstrates a dramatic influence of the measurement atmosphere on the performance of the devices. The decrease of the electroluminescence (EL) under continuous operation can be attributed to an increase of the non-radiative recombination pathways, rather than a degradation of the perovskite material itself.",what Data ?,current density of 232 mA·cm(-2),"light emission. here, we report on the preparation of bright solid state light emitting diodes ( leds ) based on a solution - processed hybrid lead halide perovskite ( pe ). in particular, we have utilized the perovskite generally described with the formula ch3nh3pbi ( 3 - x ) cl ( x ) and exploited a configuration without electron or hole blocking layer in addition to the injecting layers. compact tio2 and spiro - ometad were used as electron and hole injecting layers, respectively. we have demonstrated a bright combined visible - infrared radiance of 7. 1 w · sr ( - 1 ) · m ( - 2 ) at a current density of 232 ma · cm ( - 2 ), and a maximum external quantum efficiency",False,False
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Data ?,"zero, 4th and 8th week",multiple comparisons,False,False
"With the rapid growth of online social media content, and the impact these have made on people’s behavior, many researchers have been interested in studying these media platforms. A major part of their work focused on sentiment analysis and opinion mining. These refer to the automatic identification of opinions of people toward specific topics by analyzing their posts and publications. Multi-class sentiment analysis, in particular, addresses the identification of the exact sentiment conveyed by the user rather than the overall sentiment polarity of his text message or post. That being the case, we introduce a task different from the conventional multi-class classification, which we run on a data set collected from Twitter. We refer to this task as “quantification.” By the term “quantification,” we mean the identification of all the existing sentiments within an online post (i.e., tweet) instead of attributing a single sentiment label to it. For this sake, we propose an approach that automatically attributes different scores to each sentiment in a tweet, and selects the sentiments with the highest scores which we judge as conveyed in the text. To reach this target, we added to our previously introduced tool SENTA the necessary components to run and perform such a task. Throughout this work, we present the added components; we study the feasibility of quantification, and propose an approach to perform it on a data set made of tweets for 11 different sentiment classes. The data set was manually labeled and the results of the automatic analysis were checked against the human annotation. Our experiments show the feasibility of this task and reach an F1 score equal to 45.9%.",what Data ?,feasibility,,False,False
"Africa has over 2000 languages. Despite this, African languages account for a small portion of available resources and publications in Natural Language Processing (NLP). This is due to multiple factors, including: a lack of focus from government and funding, discoverability, a lack of community, sheer language complexity, difficulty in reproducing papers and no benchmarks to compare techniques. To begin to address the identified problems, MASAKHANE, an open-source, continent-wide, distributed, online research effort for machine translation for African languages, was founded. In this paper, we discuss our methodology for building the community and spurring research from the African continent, as well as outline the success of the community in terms of addressing the identified problems affecting African NLP.",what Data ?,sheer language complexity,,False,False
"It has been proven that using structured methods to represent the domain reduces human errors in the process of creating models and also in the process of using them. Using modeling patterns is a proven structural method in this regard. A pattern is a generalizable reusable solution to a design problem. Positive effects of using patterns were demonstrated in several experimental studies and explained using theories. However, detailed knowledge about how properties of patterns lead to increased performance in writing and reading conceptual models is currently lacking. This paper proposes a theoretical framework to characterize the properties of ontology-driven conceptual model patterns. The development of such framework is the first step in investigating the effects of pattern properties and devising rules to compose patterns based on well-understood properties.",what Data ?,ontology-driven conceptual model patterns,,False,False
"ABSTRACTOBJECTIVE: To assess and compare anti-inflammatory effect of pioglitazone and gemfibrozil by measuring C-reactive protein (CRP) levels in high fat fed non-diabetic rats.METHODS: A comparative animal study was conducted at the Post Graduate Medical Institute, Lahore, Pakistan in which 27, adult healthy male Sprague Dawley rats were used. The rats were divided into three groups. Hyperlipidemia was induced in all three groups by giving hyperlipidemic diet containing cholesterol 1.5%, coconut oil 8.0% and sodium cholate 1.0%. After four weeks, Group A (control) was given distilled water, Group B was given pioglitazone 10mg/kg body weight and Group C was given gemfibrozil 10mg/kg body weight as single morning dose by oral route for four weeks. CRP was estimated at zero, 4th and 8th week.RESULTS: There was significant increase in the level of CRP after giving high lipid diet from mean±SD of 2.59±0.28mg/L, 2.63±0.32mg/L and 2.67±0.23mg/L at 0 week to 3.55±0.44mg/L, 3.59±0.34mg/L and 3.6±0.32mg/L at 4th week in groups A, B and C respectively.Multiple comparisons by ANOVA revealed significant difference between groups at 8th week only. Post hoc analysis disclosed that CRP level was significantly low in pioglitazone treated group having mean±SD of 2.93±0.33mg/L compared to control group’s 4.42±0.30mg/L and gemfibrozil group’s 4.28±0.39mg/L. The p-value in each case was <0.001, while difference between control and gemfibrozil was not statistically significant.CONCLUSION: Pioglitazone is effective in reducing hyperlipidemia associated inflammation, evidenced by decreased CRP level while gemfibrozil is not effective.KEY WORDS: Pioglitazone (MeSH); Gemfibrozil (MeSH); Hyperlipidemia (MeSH); Anti-inflammatory (MeSH); C-reactive protein (MeSH).",what Data ?,4.28±0.39mg/L.,multiple comparisons,False,False
"We present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. The learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. To map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. This network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. The feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. Once motion is generated it can be edited by performing optimization in the space of the motion manifold. This allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. As a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data.",what Has evaluation ?,Autoencoder,optimization,False,False
"Nowadays, the enormous volume of health and fitness data gathered from IoT wearable devices offers favourable opportunities to the research community. For instance, it can be exploited using sophisticated data analysis techniques, such as automatic reasoning, to find patterns and, extract information and new knowledge in order to enhance decision-making and deliver better healthcare. However, due to the high heterogeneity of data representation formats, the IoT healthcare landscape is characterised by an ubiquitous presence of data silos which prevents users and clinicians from obtaining a consistent representation of the whole knowledge. Semantic web technologies, such as ontologies and inference rules, have been shown as a promising way for the integration and exploitation of data from heterogeneous sources. In this paper, we present a semantic data model useful to: (1) consistently represent health and fitness data from heterogeneous IoT sources; (2) integrate and exchange them; and (3) enable automatic reasoning by inference engines.",what Has evaluation ?,automatic reasoning by inference engines,"automatic reasoning,",False,False
"Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.",what Has evaluation ?,Citation Prediction,"classification and recommendation,",False,False
"Background COVID-19, caused by the novel SARS-CoV-2, is considered the most threatening respiratory infection in the world, with over 40 million people infected and over 0.934 million related deaths reported worldwide. It is speculated that epidemiological and clinical features of COVID-19 may differ across countries or continents. Genomic comparison of 48,635 SARS-CoV-2 genomes has shown that the average number of mutations per sample was 7.23, and most SARS-CoV-2 strains belong to one of 3 clades characterized by geographic and genomic specificity: Europe, Asia, and North America. Objective The aim of this study was to compare the genomes of SARS-CoV-2 strains isolated from Italy, Sweden, and Congo, that is, 3 different countries in the same meridian (longitude) but with different climate conditions, and from Brazil (as an outgroup country), to analyze similarities or differences in patterns of possible evolutionary pressure signatures in their genomes. Methods We obtained data from the Global Initiative on Sharing All Influenza Data repository by sampling all genomes available on that date. Using HyPhy, we achieved the recombination analysis by genetic algorithm recombination detection method, trimming, removal of the stop codons, and phylogenetic tree and mixed effects model of evolution analyses. We also performed secondary structure prediction analysis for both sequences (mutated and wild-type) and “disorder” and “transmembrane” analyses of the protein. We analyzed both protein structures with an ab initio approach to predict their ontologies and 3D structures. Results Evolutionary analysis revealed that codon 9628 is under episodic selective pressure for all SARS-CoV-2 strains isolated from the 4 countries, suggesting it is a key site for virus evolution. Codon 9628 encodes the P0DTD3 (Y14_SARS2) uncharacterized protein 14. Further investigation showed that the codon mutation was responsible for helical modification in the secondary structure. The codon was positioned in the more ordered region of the gene (41-59) and near to the area acting as the transmembrane (54-67), suggesting its involvement in the attachment phase of the virus. The predicted protein structures of both wild-type and mutated P0DTD3 confirmed the importance of the codon to define the protein structure. Moreover, ontological analysis of the protein emphasized that the mutation enhances the binding probability. Conclusions Our results suggest that RNA secondary structure may be affected and, consequently, the protein product changes T (threonine) to G (glycine) in position 50 of the protein. This position is located close to the predicted transmembrane region. Mutation analysis revealed that the change from G (glycine) to D (aspartic acid) may confer a new function to the protein—binding activity, which in turn may be responsible for attaching the virus to human eukaryotic cells. These findings can help design in vitro experiments and possibly facilitate a vaccine design and successful antiviral strategies.",what Has evaluation ?,codon mutation,,False,False
"Petroleum exploration and production in the Nigeria’s Niger Delta region and export of oil and gas resources by the petroleum sector has substantially improved the nation’s economy over the past five decades. However, activities associated with petroleum exploration, development and production operations have local detrimental and significant impacts on the atmosphere, soils and sediments, surface and groundwater, marine environment and terrestrial ecosystems in the Niger Delta. Discharges of petroleum hydrocarbon and petroleum–derived waste streams have caused environmental pollution, adverse human health effects, socio–economic problems and degradation of host communities in the 9 oil–producing states in the Niger Delta region. Many approaches have been developed for the management of environmental impacts of petroleum production–related activities and several environmental laws have been institutionalized to regulate the Nigerian petroleum industry. However, the existing statutory laws and regulations for environmental protection appear to be grossly inadequate and some of the multinational oil companies operating in the Niger Delta region have failed to adopt sustainable practices to prevent environmental pollution. This review examines the implications of multinational oil companies operations and further highlights some of the past and present environmental issues associated with petroleum exploitation and production in the Nigeria’s Niger Delta. Although effective understanding of petroleum production and associated environmental degradation is importance for developing management strategies, there is a need for more multidisciplinary approaches for sustainable risk mitigation and effective environmental protection of the oil–producing host communities in the Niger Delta.",what Has evaluation ?,"Although effective understanding of petroleum production and associated environmental degradation is importance for developing management strategies, there is a need for more multidisciplinary approaches for sustainable risk mitigation and effective environmental protection of the oil–producing host communities in the Niger Delta.",,False,False
"ABSTRACT This study examined the neural basis of processing high- and low-message sensation value (MSV) antidrug public service announcements (PSAs) in high (HSS) and low sensation seekers (LSS) using fMRI. HSS more strongly engaged the salience network when processing PSAs (versus LSS), suggesting that high-MSV PSAs attracted their attention. HSS and LSS participants who engaged higher level cognitive processing regions reported that the PSAs were more convincing and believable and recalled the PSAs better immediately after testing. In contrast, HSS and LSS participants who strongly engaged visual attention regions for viewing PSAs reported lower personal relevance. These findings provide neurobiological evidence that high-MSV content is salient to HSS, a primary target group for antidrug messages, and additional cognitive processing is associated with higher perceived message effectiveness.",what Has method ?,fMRI,abstract,False,False
"Knowledge graph completion is still a challenging solution that uses techniques from distinct areas to solve many different tasks. Most recent works, which are based on embedding models, were conceived to improve an existing knowledge graph using the link prediction task. However, even considering the ability of these solutions to solve other tasks, they did not present results for data linking, for example. Furthermore, most of these works focuses only on structural information, i.e., the relations between entities. In this paper, we present an approach for data linking that enrich entity embeddings in a model with their literal information and that do not rely on external information of these entities. The key aspect of this proposal is that we use a blocking scheme to improve the effectiveness of the solution in relation to the use of literals. Thus, in addition to the literals from object elements in a triple, we use other literals from subjects and predicates. By merging entity embeddings with their literal information it is possible to extend many popular embedding models. Preliminary experiments were performed on real-world datasets and our solution showed competitive results to the performance of the task of data linking.",what Has method ?,Blocking,"link prediction task. however, even considering the ability of these solutions to solve other tasks, they did not present results for data linking, for example. furthermore, most of these works focuses only on structural information, i. e., the relations between entities. in this paper, we present an approach for data linking that enrich entity embeddings in a model with their literal information and that don't rely on external information of these entities. the key aspect of this proposal is that we use a blocking scheme",False,True
"Abstract Background Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection can spread rapidly within skilled nursing facilities. After identification of a case of Covid-19 in a skilled nursing facility, we assessed transmission and evaluated the adequacy of symptom-based screening to identify infections in residents. Methods We conducted two serial point-prevalence surveys, 1 week apart, in which assenting residents of the facility underwent nasopharyngeal and oropharyngeal testing for SARS-CoV-2, including real-time reverse-transcriptase polymerase chain reaction (rRT-PCR), viral culture, and sequencing. Symptoms that had been present during the preceding 14 days were recorded. Asymptomatic residents who tested positive were reassessed 7 days later. Residents with SARS-CoV-2 infection were categorized as symptomatic with typical symptoms (fever, cough, or shortness of breath), symptomatic with only atypical symptoms, presymptomatic, or asymptomatic. Results Twenty-three days after the first positive test result in a resident at this skilled nursing facility, 57 of 89 residents (64%) tested positive for SARS-CoV-2. Among 76 residents who participated in point-prevalence surveys, 48 (63%) tested positive. Of these 48 residents, 27 (56%) were asymptomatic at the time of testing; 24 subsequently developed symptoms (median time to onset, 4 days). Samples from these 24 presymptomatic residents had a median rRT-PCR cycle threshold value of 23.1, and viable virus was recovered from 17 residents. As of April 3, of the 57 residents with SARS-CoV-2 infection, 11 had been hospitalized (3 in the intensive care unit) and 15 had died (mortality, 26%). Of the 34 residents whose specimens were sequenced, 27 (79%) had sequences that fit into two clusters with a difference of one nucleotide. Conclusions Rapid and widespread transmission of SARS-CoV-2 was demonstrated in this skilled nursing facility. More than half of residents with positive test results were asymptomatic at the time of testing and most likely contributed to transmission. Infection-control strategies focused solely on symptomatic residents were not sufficient to prevent transmission after SARS-CoV-2 introduction into this facility.",what Has method ?,sequencing,reverse - transcriptase polymerase chain reaction,False,False
"In order to improve the signal-to-noise ratio of the hyperspectral sensors and exploit the potential of satellite hyperspectral data for predicting soil properties, we took MingShui County as the study area, which the study area is approximately 1481 km2, and we selected Gaofen-5 (GF-5) satellite hyperspectral image of the study area to explore an applicable and accurate denoising method that can effectively improve the prediction accuracy of soil organic matter (SOM) content. First, fractional-order derivative (FOD) processing is performed on the original reflectance (OR) to evaluate the optimal FOD. Second, singular value decomposition (SVD), Fourier transform (FT) and discrete wavelet transform (DWT) are used to denoise the OR and optimal FOD reflectance. Third, the spectral indexes of the reflectance under different denoising methods are extracted by optimal band combination algorithm, and the input variables of different denoising methods are selected by the recursive feature elimination (RFE) algorithm. Finally, the SOM content is predicted by a random forest prediction model. The results reveal that 0.6-order reflectance describes more useful details in satellite hyperspectral data. Five spectral indexes extracted from the reflectance under different denoising methods have a strong correlation with the SOM content, which is helpful for realizing high-accuracy SOM predictions. All three denoising methods can reduce the noise in hyperspectral data, and the accuracies of the different denoising methods are ranked DWT > FT > SVD, where 0.6-order-DWT has the highest accuracy (R2 = 0.84, RMSE = 3.36 g kg−1, and RPIQ = 1.71). This paper is relatively novel, in that GF-5 satellite hyperspectral data based on different denoising methods are used to predict SOM, and the results provide a highly robust and novel method for mapping the spatial distribution of SOM content at the regional scale.",what Has method ?,fractional-order derivative (FOD),"fourier transform ( ft ) and discrete wavelet transform ( dwt ) are used to denoise the or and optimal fod reflectance. third, the spectral indexes of the reflectance under different denoising methods are extracted by optimal band combination algorithm, and the input variables of different denoising methods are selected by the recursive feature elimination ( rfe )",False,False
"Due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving. In the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated. However, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes. To integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities. Based on this survey, a taxonomy has been created which is used to classify the existing research. Furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments.",what Has method ?,Systematic Literature Review,taxonomy,False,False
"Background The COVID-19 outbreak has affected the lives of millions of people by causing a dramatic impact on many health care systems and the global economy. This devastating pandemic has brought together communities across the globe to work on this issue in an unprecedented manner. Objective This case study describes the steps and methods employed in the conduction of a remote online health hackathon centered on challenges posed by the COVID-19 pandemic. It aims to deliver a clear implementation road map for other organizations to follow. Methods This 4-day hackathon was conducted in April 2020, based on six COVID-19–related challenges defined by frontline clinicians and researchers from various disciplines. An online survey was structured to assess: (1) individual experience satisfaction, (2) level of interprofessional skills exchange, (3) maturity of the projects realized, and (4) overall quality of the event. At the end of the event, participants were invited to take part in an online survey with 17 (+5 optional) items, including multiple-choice and open-ended questions that assessed their experience regarding the remote nature of the event and their individual project, interprofessional skills exchange, and their confidence in working on a digital health project before and after the hackathon. Mentors, who guided the participants through the event, also provided feedback to the organizers through an online survey. Results A total of 48 participants and 52 mentors based in 8 different countries participated and developed 14 projects. A total of 75 mentorship video sessions were held. Participants reported increased confidence in starting a digital health venture or a research project after successfully participating in the hackathon, and stated that they were likely to continue working on their projects. Of the participants who provided feedback, 60% (n=18) would not have started their project without this particular hackathon and indicated that the hackathon encouraged and enabled them to progress faster, for example, by building interdisciplinary teams, gaining new insights and feedback provided by their mentors, and creating a functional prototype. Conclusions This study provides insights into how online hackathons can contribute to solving the challenges and effects of a pandemic in several regions of the world. The online format fosters team diversity, increases cross-regional collaboration, and can be executed much faster and at lower costs compared to in-person events. Results on preparation, organization, and evaluation of this online hackathon are useful for other institutions and initiatives that are willing to introduce similar event formats in the fight against COVID-19.",what Has method ?,remote online health hackathon,online survey,False,False
"The activity of a host of antimicrobial peptides has been examined against a range of lipid bilayers mimicking bacterial and eukaryotic membranes. Despite this, the molecular mechanisms and the nature of the physicochemical properties underlying the peptide–lipid interactions that lead to membrane disruption are yet to be fully elucidated. In this study, the interaction of the short antimicrobial peptide aurein 1.2 was examined in the presence of an anionic cardiolipin-containing lipid bilayer using molecular dynamics simulations. Aurein 1.2 is known to interact strongly with anionic lipid membranes. In the simulations, the binding of aurein 1.2 was associated with buckling of the lipid bilayer, the degree of which varied with the peptide concentration. The simulations suggest that the intrinsic properties of cardiolipin, especially the fact that it promotes negative membrane curvature, may help protect membranes against the action of peptides such as aurein 1.2 by counteracting the tendency of the peptide to induce positive curvature in target membranes.",what Has method ?,Molecular Dynamics Simulations,molecular dynamics,False,False
"In order to improve the signal-to-noise ratio of the hyperspectral sensors and exploit the potential of satellite hyperspectral data for predicting soil properties, we took MingShui County as the study area, which the study area is approximately 1481 km2, and we selected Gaofen-5 (GF-5) satellite hyperspectral image of the study area to explore an applicable and accurate denoising method that can effectively improve the prediction accuracy of soil organic matter (SOM) content. First, fractional-order derivative (FOD) processing is performed on the original reflectance (OR) to evaluate the optimal FOD. Second, singular value decomposition (SVD), Fourier transform (FT) and discrete wavelet transform (DWT) are used to denoise the OR and optimal FOD reflectance. Third, the spectral indexes of the reflectance under different denoising methods are extracted by optimal band combination algorithm, and the input variables of different denoising methods are selected by the recursive feature elimination (RFE) algorithm. Finally, the SOM content is predicted by a random forest prediction model. The results reveal that 0.6-order reflectance describes more useful details in satellite hyperspectral data. Five spectral indexes extracted from the reflectance under different denoising methods have a strong correlation with the SOM content, which is helpful for realizing high-accuracy SOM predictions. All three denoising methods can reduce the noise in hyperspectral data, and the accuracies of the different denoising methods are ranked DWT > FT > SVD, where 0.6-order-DWT has the highest accuracy (R2 = 0.84, RMSE = 3.36 g kg−1, and RPIQ = 1.71). This paper is relatively novel, in that GF-5 satellite hyperspectral data based on different denoising methods are used to predict SOM, and the results provide a highly robust and novel method for mapping the spatial distribution of SOM content at the regional scale.",what Has method ?,singular value decomposition (SVD),"fourier transform ( ft ) and discrete wavelet transform ( dwt ) are used to denoise the or and optimal fod reflectance. third, the spectral indexes of the reflectance under different denoising methods are extracted by optimal band combination algorithm, and the input variables of different denoising methods are selected by the recursive feature elimination ( rfe )",False,False
"ABSTRACT Contested heritage has increasingly been studied by scholars over the last two decades in multiple disciplines, however, there is still limited knowledge about what contested heritage is and how it is realized in society. Therefore, the purpose of this paper is to produce a systematic literature review on this topic to provide a holistic understanding of contested heritage, and delineate its current state, trends and gaps. Methodologically, four electronic databases were searched, and 102 journal articles published before 2020 were extracted. A content analysis of each article was then conducted to identify key themes and variables for classification. Findings show that while its research often lacks theoretical underpinnings, contested heritage is marked by its diversity and complexity as it becomes a global issue for both tourism and urbanization. By presenting a holistic understanding of contested heritage, this review offers an extensive investigation of the topic area to help move literature pertaining contested heritage forward.",what Has method ?,systematic literature review,content analysis,False,False
"Abstract Background Data papers have emerged as a powerful instrument for open data publishing, obtaining credit, and establishing priority for datasets generated in scientific experiments. Academic publishing improves data and metadata quality through peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. Objective We aimed to establish a new type of article structure and template for omics studies: the omics data paper. To improve data interoperability and further incentivize researchers to publish well-described datasets, we created a prototype workflow for streamlined import of genomics metadata from the European Nucleotide Archive directly into a data paper manuscript. Methods An omics data paper template was designed by defining key article sections that encourage the description of omics datasets and methodologies. A metadata import workflow, based on REpresentational State Transfer services and Xpath, was prototyped to extract information from the European Nucleotide Archive, ArrayExpress, and BioSamples databases. Findings The template and workflow for automatic import of standard-compliant metadata into an omics data paper manuscript provide a mechanism for enhancing existing metadata through publishing. Conclusion The omics data paper structure and workflow for import of genomics metadata will help to bring genomic and other omics datasets into the spotlight. Promoting enhanced metadata descriptions and enforcing manuscript peer review and data auditing of the underlying datasets brings additional quality to datasets. We hope that streamlined metadata reuse for scholarly publishing encourages authors to create enhanced metadata descriptions in the form of data papers to improve both the quality of their metadata and its findability and accessibility.",what Has method ?,xpath,"omics data paper template was designed by defining key article sections that encourage the description of omics datasets and methodologies. a metadata import workflow,",False,False
"Iodine deficiency disorders (IDD) has been a major global public health problem threatening more than 2 billion people worldwide. Considering various human health implications associated with iodine deficiency, universal salt iodization programme has been recognized as one of the best methods of preventing iodine deficiency disorder and iodizing table salt is currently done in many countries. In this study, comparative assessment of iodine content of commercially available table salt brands in Nigerian market were investigated and iodine content were measured in ten table salt brands samples using iodometric titration. The iodine content ranged from 14.80 mg/kg – 16.90 mg/kg with mean value of 15.90 mg/kg for Sea salt; 24.30 mg/kg – 25.40 mg/kg with mean value of 24.60 mg/kg for Dangote salt (blue sachet); 22.10 mg/kg – 23.10 mg/kg with mean value of 22.40 mg/kg for Dangote salt (red sachet); 23.30 mg/kg – 24.30 mg/kg with mean value of 23.60 mg/kg for Mr Chef salt; 23.30 mg/kg – 24.30 mg/kg with mean value of 23.60 mg/kg for Annapurna; 26.80 mg/kg – 27.50 mg/kg with mean value of 27.20mg/kg for Uncle Palm salt; 23.30 mg/kg – 29.60 mg/kg with mean content of 26.40 mg/kg for Dangote (bag); 25.40 mg/kg – 26.50 mg/kg with mean value of 26.50 mg/kg for Royal salt; 36.80 mg/kg – 37.20 mg/kg with mean iodine content of 37.0 mg/kg for Abakaliki refined salt, and 30.07 mg/kg – 31.20 mg/kg with mean value of 31.00 mg/kg for Ikom refined salt. The mean iodine content measured in the Sea salt brand (15.70 mg/kg) was significantly P < 0.01 lower compared to those measured in other table salt brands. Although the iodine content of Abakaliki and Ikom refined salt exceed the recommended value, it is clear that only Sea salt brand falls below the World Health Organization (WHO) recommended value (20 – 30 mg/kg), while the remaining table salt samples are just within the range. The results obtained have revealed that 70 % of the table salt brands were adequately iodized while 30 % of the table salt brands were not adequately iodized and provided baseline data that can be used for potential identification of human health risks associated with inadequate and/or excess iodine content in table salt brands consumed in households in Nigeria.",what Has method ?,"Considering various human health implications associated with iodine deficiency, universal salt iodization programme has been recognized as one of the best methods of preventing iodine deficiency disorder and iodizing table salt is currently done in many countries. ",universal salt iodization programme,False,False
"<jats:p>This paper discusses the potential of current advancements in Information Communication Technologies (ICT) for cultural heritage preservation, valorization and management within contemporary cities. The paper highlights the potential of virtual environments to assess the impacts of heritage policies on urban development. It does so by discussing the implications of virtual globes and crowdsourcing to support the participatory valuation and management of cultural heritage assets. To this purpose, a review of available valuation techniques is here presented together with a discussion on how these techniques might be coupled with ICT tools to promote inclusive governance. </jats:p>",what Has method ?,review,valuation techniques,False,False
"Abstract Hackathons, time-bounded events where participants write computer code and build apps, have become a popular means of socializing tech students and workers to produce “innovation” despite little promise of material reward. Although they offer participants opportunities for learning new skills and face-to-face networking and set up interaction rituals that create an emotional “high,” potential advantage is even greater for the events’ corporate sponsors, who use them to outsource work, crowdsource innovation, and enhance their reputation. Ethnographic observations and informal interviews at seven hackathons held in New York during the course of a single school year show how the format of the event and sponsors’ discursive tropes, within a dominant cultural frame reflecting the appeal of Silicon Valley, reshape unpaid and precarious work as an extraordinary opportunity, a ritual of ecstatic labor, and a collective imaginary for fictional expectations of innovation that benefits all, a powerful strategy for manufacturing workers’ consent in the “new” economy.",what Has method ?,informal interviews,ethnographic observations,False,False
"Reconstructing the energy landscape of a protein holds the key to characterizing its structural dynamics and function [1]. While the disparate spatio-temporal scales spanned by the slow dynamics challenge reconstruction in wet and dry laboratories, computational efforts have had recent success on proteins where a wealth of experimentally-known structures can be exploited to extract modes of motion. In [2], the authors propose the SoPriM method that extracts principle components (PCs) and utilizes them as variables of the structure space of interest. Stochastic optimization is employed to sample the structure space and its associated energy landscape in the defined varible space. We refer to this algorithm as SoPriM-PCA and compare it here to SoPriM-NMA, which investigates whether the landscape can be reconstructed with knowledge of modes of motion (normal modes) extracted from one single known structure. Some representative results are shown in Figure 1, where structures obtained by SoPriM-PCA and those obtained by SoPriM-NMA for the H-Ras enzyme are compared via color-coded projections onto the top two variables utilized by each algorithm. The results show that precious information can be obtained on the energy landscape even when one structural model is available. The presented work opens up interesting venues of research on structure-based inference of dynamics. Acknowledgment: This work is supported in part by NSF Grant No. 1421001 to AS and NSF Grant No. 1440581 to AS and EP. Computations were run on ARGO, a research computing cluster provided by the Office of Research Computing at George Mason University, VA (URL: http://orc.gmu.edu).",what Has method ?,SoPriM-NMA,soprim method that extracts principle components ( pcs ) and utilizes them as variables of the structure space of interest. stochastic optimization is employed to sample the structure space and its associated energy landscape in the defined varible space. we refer to this algorithm as soprim - pca,False,False
"During global health crises, such as the recent H1N1 pandemic, the mass media provide the public with timely information regarding risk. To obtain new insights into how these messages are received, we measured neural data while participants, who differed in their preexisting H1N1 risk perceptions, viewed a TV report about H1N1. Intersubject correlation (ISC) of neural time courses was used to assess how similarly the brains of viewers responded to the TV report. We found enhanced intersubject correlations among viewers with high-risk perception in the anterior cingulate, a region which classical fMRI studies associated with the appraisal of threatening information. By contrast, neural coupling in sensory-perceptual regions was similar for the high and low H1N1-risk perception groups. These results demonstrate a novel methodology for understanding how real-life health messages are processed in the human brain, with particular emphasis on the role of emotion and differences in risk perceptions.",what Has approach ?,fMRI,"intersubject correlation ( isc ) of neural time courses was used to assess how similarly the brains of viewers responded to the tv report. we found enhanced intersubject correlations among viewers with high - risk perception in the anterior cingulate, a region which classical fmri",False,True
"Interference between pharmacological substances can cause serious medical injuries. Correctly predicting so-called drug-drug interactions (DDI) does not only reduce these cases but can also result in a reduction of drug development cost. Presently, most drug-related knowledge is the result of clinical evaluations and post-marketing surveillance; resulting in a limited amount of information. Existing data-driven prediction approaches for DDIs typically rely on a single source of information, while using information from multiple sources would help improve predictions. Machine learning (ML) techniques are used, but the techniques are often unable to deal with skewness in the data. Hence, we propose a new ML approach for predicting DDIs based on multiple data sources. For this task, we use 12,000 drug features from DrugBank, PharmGKB, and KEGG drugs, which are integrated using Knowledge Graphs (KGs). To train our prediction model, we first embed the nodes in the graph using various embedding approaches. We found that the best performing combination was a ComplEx embedding method creating using PyTorch-BigGraph (PBG) with a Convolutional-LSTM network and classic machine learning-based prediction models. The model averaging ensemble method of three best classifiers yields up to 0.94, 0.92, 0.80 for AUPR, F1 F1-score, and MCC, respectively during 5-fold cross-validation tests.",what Has approach ?,Convolutional-LSTM Network,ml,False,False
"Query optimization in RDF Stores is a challenging problem as SPARQL queries typically contain many more joins than equivalent relational plans, and hence lead to a large join order search space. In such cases, cost-based query optimization often is not possible. One practical reason for this is that statistics typically are missing in web scale setting such as the Linked Open Datasets (LOD). The more profound reason is that due to the absence of schematic structure in RDF, join-hit ratio estimation requires complicated forms of correlated join statistics; and currently there are no methods to identify the relevant correlations beforehand. For this reason, the use of good heuristics is essential in SPARQL query optimization, even in the case that are partially used with cost-based statistics (i.e., hybrid query optimization). In this paper we describe a set of useful heuristics for SPARQL query optimizers. We present these in the context of a new Heuristic SPARQL Planner (HSP) that is capable of exploiting the syntactic and the structural variations of the triple patterns in a SPARQL query in order to choose an execution plan without the need of any cost model. For this, we define the variable graph and we show a reduction of the SPARQL query optimization problem to the maximum weight independent set problem. We implemented our planner on top of the MonetDB open source column-store and evaluated its effectiveness against the state-of-the-art RDF-3X engine as well as comparing the plan quality with a relational (SQL) equivalent of the benchmarks.",what Has approach ?,SPARQL,join - hit ratio,False,False
"With the proliferation of the RDF data format, engines for RDF query processing are faced with very large graphs that contain hundreds of millions of RDF triples. This paper addresses the resulting scalability problems. Recent prior work along these lines has focused on indexing and other physical-design issues. The current paper focuses on join processing, as the fine-grained and schema-relaxed use of RDF often entails star- and chain-shaped join queries with many input streams from index scans. We present two contributions for scalable join processing. First, we develop very light-weight methods for sideways information passing between separate joins at query run-time, to provide highly effective filters on the input streams of joins. Second, we improve previously proposed algorithms for join-order optimization by more accurate selectivity estimations for very large RDF graphs. Experimental studies with several RDF datasets, including the UniProt collection, demonstrate the performance gains of our approach, outperforming the previously fastest systems by more than an order of magnitude.",what Has approach ?,Very large RDF Graphs,join - order,False,False
"We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",what Dataset name ?,SciERC,"scierc,",True,True
"This paper presents the formal release of {\em MedMentions}, a new manually annotated resource for the recognition of biomedical concepts. What distinguishes MedMentions from other annotated biomedical corpora is its size (over 4,000 abstracts and over 350,000 linked mentions), as well as the size of the concept ontology (over 3 million concepts from UMLS 2017) and its broad coverage of biomedical disciplines. In addition to the full corpus, a sub-corpus of MedMentions is also presented, comprising annotations for a subset of UMLS 2017 targeted towards document retrieval. To encourage research in Biomedical Named Entity Recognition and Linking, data splits for training and testing are included in the release, and a baseline model and its metrics for entity linking are also described.",what Dataset name ?,MedMentions,medmentions },False,True
"We present LatinISE, a Latin corpus for the Sketch Engine. LatinISE consists of Latin works comprising a total of 13 million words, covering the time span from the 2 nd century B. C. to the 21 st century A. D. LatinISE is provided with rich metadata mark-up, including author, title, genre, era, date and century, as well as book, section, paragraph and line of verses. We have automatically annotated LatinISE with lemma and part-of-speech information. The annotation enables the users to search the corpus with a number of criteria, ranging from lemma, part-of-speech, context, to subcorpora defined chronologically or by genre. We also illustrate word sketches, one-page summaries of a word’s corpus-based collocational behaviour. Our future plan is to produce word sketches for Latin words by adding richer morphological and syntactic annotation to the corpus.",what Dataset name ?,LatinISE,"latinise,",True,True
"MOTIVATION Natural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack of an extensively annotated corpus of this literature, however, causes a major bottleneck for applying NLP techniques. GENIA corpus is being developed to provide reference materials to let NLP techniques work for bio-textmining. RESULTS GENIA corpus version 3.0 consisting of 2000 MEDLINE abstracts has been released with more than 400,000 words and almost 100,000 annotations for biological terms.",what Dataset name ?,GENIA corpus,genia corpus,True,True
"Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .",what Dataset name ?,SciREX,"scirex,",True,True
"Knowledge about software used in scientific investigations is important for several reasons, for instance, to enable an understanding of provenance and methods involved in data handling. However, software is usually not formally cited, but rather mentioned informally within the scholarly description of the investigation, raising the need for automatic information extraction and disambiguation. Given the lack of reliable ground truth data, we present SoMeSci-Software Mentions in Science-a gold standard knowledge graph of software mentions in scientific articles. It contains high quality annotations (IRR: K=.82) of 3756 software mentions in 1367 PubMed Central articles. Besides the plain mention of the software, we also provide relation labels for additional information, such as the version, the developer, a URL or citations. Moreover, we distinguish between different types, such as application, plugin or programming environment, as well as different types of mentions, such as usage or creation. To the best of our knowledge, SoMeSci is the most comprehensive corpus about software mentions in scientific articles, providing training samples for Named Entity Recognition, Relation Extraction, Entity Disambiguation, and Entity Linking. Finally, we sketch potential use cases and provide baseline results.",what Dataset name ?,SoMeSci,,False,False
"This paper introduces the ACL Reference Dataset for Terminology Extraction and Classification, version 2.0 (ACL RD-TEC 2.0). The ACL RD-TEC 2.0 has been developed with the aim of providing a benchmark for the evaluation of term and entity recognition tasks based on specialised text from the computational linguistics domain. This release of the corpus consists of 300 abstracts from articles in the ACL Anthology Reference Corpus, published between 1978–2006. In these abstracts, terms (i.e., single or multi-word lexical units with a specialised meaning) are manually annotated. In addition to their boundaries in running text, annotated terms are classified into one of the seven categories method, tool, language resource (LR), LR product, model, measures and measurements, and other. To assess the quality of the annotations and to determine the difficulty of this annotation task, more than 171 of the abstracts are annotated twice, independently, by each of the two annotators. In total, 6,818 terms are identified and annotated in more than 1300 sentences, resulting in a specialised vocabulary made of 3,318 lexical forms, mapped to 3,471 concepts. We explain the development of the annotation guidelines and discuss some of the challenges we encountered in this annotation task.",what Dataset name ?,ACL RD-TEC 2.0,"acl reference dataset for terminology extraction and classification, version 2. 0 ( acl rd - tec 2. 0 )",False,False
"We report on two large corpora of semantically annotated full-text biomedical research papers created in order to devel op information extraction ( IE) tools for the TXM project. Both corpora have been annotated with a range of entities (CellLine, Complex, DevelopmentalStage, Disease, DrugCompound, ExperimentalMethod, Fragment, Fusion, GOMOP, Gene, Modification, mRNAcDNA, Mutant, Protein, Tissue), normalisations of selected entities to the NCBI Taxonomy, RefSeq, EntrezGene, ChEBI and MeSH and enriched relations (protein-protein interactions, tissue expressions and fr agment- or mutant-protein relations). While one corpus targets protein-protein interactions ( PPIs), the focus of other is on tissue expressions ( TEs). This paper describes the selected markables and the annotation process of the ITI TXM corpora, and provides a detailed breakdown of the inter-annotator agreement (IAA).",what Concept types ?,Tissue,"cellline, complex, developmentalstage, disease, drugcompound, experimentalmethod,",False,False
"This paper introduces the ACL Reference Dataset for Terminology Extraction and Classification, version 2.0 (ACL RD-TEC 2.0). The ACL RD-TEC 2.0 has been developed with the aim of providing a benchmark for the evaluation of term and entity recognition tasks based on specialised text from the computational linguistics domain. This release of the corpus consists of 300 abstracts from articles in the ACL Anthology Reference Corpus, published between 1978–2006. In these abstracts, terms (i.e., single or multi-word lexical units with a specialised meaning) are manually annotated. In addition to their boundaries in running text, annotated terms are classified into one of the seven categories method, tool, language resource (LR), LR product, model, measures and measurements, and other. To assess the quality of the annotations and to determine the difficulty of this annotation task, more than 171 of the abstracts are annotated twice, independently, by each of the two annotators. In total, 6,818 terms are identified and annotated in more than 1300 sentences, resulting in a specialised vocabulary made of 3,318 lexical forms, mapped to 3,471 concepts. We explain the development of the annotation guidelines and discuss some of the challenges we encountered in this annotation task.",what Concept types ?,Language Resource,"tool, language resource ( lr ), lr product, model, measures and measurements,",False,True
"Abstract The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants’ systems had to identify and rank relevant articles in a collection of 5.2 M MEDLINE citations (task 1) or 530 000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search.",what Concept types ?,human protein kinase,,False,False
"We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.",what Concept types ?,Material,"semeval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials.",False,True
"With the information overload in genome-related field, there is an increasing need for natural language processing technology to extract information from literature and various attempts of information extraction using NLP has been being made. We are developing the necessary resources including domain ontology and annotated corpus from research abstracts in MEDLINE database (GENIA corpus). We are building the ontology and the corpus simultaneously, using each other. In this paper we report on our new corpus, its ontological basis, annotation scheme, and statistics of annotated objects. We also describe the tools used for corpus annotation and management.",what Concept types ?,Other,domain ontology,False,False
"This paper introduces the ACL Reference Dataset for Terminology Extraction and Classification, version 2.0 (ACL RD-TEC 2.0). The ACL RD-TEC 2.0 has been developed with the aim of providing a benchmark for the evaluation of term and entity recognition tasks based on specialised text from the computational linguistics domain. This release of the corpus consists of 300 abstracts from articles in the ACL Anthology Reference Corpus, published between 1978–2006. In these abstracts, terms (i.e., single or multi-word lexical units with a specialised meaning) are manually annotated. In addition to their boundaries in running text, annotated terms are classified into one of the seven categories method, tool, language resource (LR), LR product, model, measures and measurements, and other. To assess the quality of the annotations and to determine the difficulty of this annotation task, more than 171 of the abstracts are annotated twice, independently, by each of the two annotators. In total, 6,818 terms are identified and annotated in more than 1300 sentences, resulting in a specialised vocabulary made of 3,318 lexical forms, mapped to 3,471 concepts. We explain the development of the annotation guidelines and discuss some of the challenges we encountered in this annotation task.",what Concept types ?,Other,"tool, language resource ( lr ), lr product, model, measures and measurements,",False,False
"A crucial step toward the goal of automatic extraction of propositional information from natural language text is the identification of semantic relations between constituents in sentences. We examine the problem of distinguishing among seven relation types that can occur between the entities ""treatment"" and ""disease"" in bioscience text, and the problem of identifying such entities. We compare five generative graphical models and a neural network, using lexical, syntactic, and semantic features, finding that the latter help achieve high classification accuracy.",what Concept types ?,Treatment,"treatment "" and "" disease """,False,True
"We report on two large corpora of semantically annotated full-text biomedical research papers created in order to devel op information extraction ( IE) tools for the TXM project. Both corpora have been annotated with a range of entities (CellLine, Complex, DevelopmentalStage, Disease, DrugCompound, ExperimentalMethod, Fragment, Fusion, GOMOP, Gene, Modification, mRNAcDNA, Mutant, Protein, Tissue), normalisations of selected entities to the NCBI Taxonomy, RefSeq, EntrezGene, ChEBI and MeSH and enriched relations (protein-protein interactions, tissue expressions and fr agment- or mutant-protein relations). While one corpus targets protein-protein interactions ( PPIs), the focus of other is on tissue expressions ( TEs). This paper describes the selected markables and the annotation process of the ITI TXM corpora, and provides a detailed breakdown of the inter-annotator agreement (IAA).",what Concept types ?,Disease,"cellline, complex, developmentalstage, disease, drugcompound, experimentalmethod,",False,True
"We report on two large corpora of semantically annotated full-text biomedical research papers created in order to devel op information extraction ( IE) tools for the TXM project. Both corpora have been annotated with a range of entities (CellLine, Complex, DevelopmentalStage, Disease, DrugCompound, ExperimentalMethod, Fragment, Fusion, GOMOP, Gene, Modification, mRNAcDNA, Mutant, Protein, Tissue), normalisations of selected entities to the NCBI Taxonomy, RefSeq, EntrezGene, ChEBI and MeSH and enriched relations (protein-protein interactions, tissue expressions and fr agment- or mutant-protein relations). While one corpus targets protein-protein interactions ( PPIs), the focus of other is on tissue expressions ( TEs). This paper describes the selected markables and the annotation process of the ITI TXM corpora, and provides a detailed breakdown of the inter-annotator agreement (IAA).",what Concept types ?,mRNAcDNA,"cellline, complex, developmentalstage, disease, drugcompound, experimentalmethod,",False,False
"A considerable effort has been made to extract biological and chemical entities, as well as their relationships, from the scientific literature, either manually through traditional literature curation or by using information extraction and text mining technologies. Medicinal chemistry patents contain a wealth of information, for instance to uncover potential biomarkers that might play a role in cancer treatment and prognosis. However, current biomedical annotation databases do not cover such information, partly due to limitations of publicly available biomedical patent mining software. As part of the BioCreative V CHEMDNER patents track, we present the results of the first named entity recognition (NER) assignment carried out to detect mentions of chemical compounds and genes/proteins in running patent text. More specifically, this task aimed to evaluate the performance of automatic name recognition strategies capable of isolating chemical names and gene and gene product mentions from surrounding text within patent titles and abstracts. A total of 22 unique teams submitted results for at least one of the three CHEMDNER subtasks. The first subtask, called the CEMP (chemical entity mention in patents) task, focused on the detection of chemical named entity mentions in patents, requesting teams to return the start and end indices corresponding to all the chemical entities found in a given record. A total of 21 teams submitted 93 runs, for this subtask. The top performing team reached an f-measure of 0.89 with a precision of 0.87 and a recall of 0.91. The CPD (chemical passage detection) task required the classification of patent titles and abstracts whether they do or do not contain chemical compound mentions. Nine teams returned predictions for this task (40 runs). The top run in terms of Matthew’s correlation coefficient (MCC) had a score of 0.88, the highest sensitivity ? Corresponding author",what Concept types ?,Chemical compounds,gene,False,False
"We present a method for characterizing a research work in terms of its focus, domain of application, and techniques used. We show how tracing these aspects over time provides a novel measure of the influence of research communities on each other. We extract these characteristics by matching semantic extraction patterns, learned using bootstrapping, to the dependency trees of sentences in an article’s",what Concept types ?,Focus,"focus,",True,True
"This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2016, which follows the previous 2013 and 2011 editions. The task focuses on the extraction of the locations (biotopes and geographical places) of bacteria from PubMe abstracts and the characterization of bacteria and their associated habitats with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology). The task is motivated by the importance of the knowledge on bacteria habitats for fundamental research and applications in microbiology. The paper describes the different proposed subtasks, the corpus characteristics, the challenge organization, and the evaluation metrics. We also provide an analysis of the results obtained by participants.",what Concept types ?,Habitat,biotopes and geographical places ),False,False
"We report on two large corpora of semantically annotated full-text biomedical research papers created in order to devel op information extraction ( IE) tools for the TXM project. Both corpora have been annotated with a range of entities (CellLine, Complex, DevelopmentalStage, Disease, DrugCompound, ExperimentalMethod, Fragment, Fusion, GOMOP, Gene, Modification, mRNAcDNA, Mutant, Protein, Tissue), normalisations of selected entities to the NCBI Taxonomy, RefSeq, EntrezGene, ChEBI and MeSH and enriched relations (protein-protein interactions, tissue expressions and fr agment- or mutant-protein relations). While one corpus targets protein-protein interactions ( PPIs), the focus of other is on tissue expressions ( TEs). This paper describes the selected markables and the annotation process of the ITI TXM corpora, and provides a detailed breakdown of the inter-annotator agreement (IAA).",what Concept types ?,Gene,"cellline, complex, developmentalstage, disease, drugcompound, experimentalmethod,",False,False
"While the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions, keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult. The community could greatly benefit from an automatic system able to summarize scientific results, e.g., in the form of a leaderboard. In this paper we build two datasets and develop a framework (TDMS-IE) aimed at automatically extracting task, dataset, metric and score from NLP papers, towards the automatic construction of leaderboards. Experiments show that our model outperforms several baselines by a large margin. Our model is a first step towards automatic leaderboard construction, e.g., in the NLP domain.",what Concept types ?,Metric,"dataset,",False,False
"One of the biomedical entity types of relevance for medicine or biosciences are chemical compounds and drugs. The correct detection these entities is critical for other text mining applications building on them, such as adverse drug-reaction detection, medication-related fake news or drug-target extraction. Although a significant effort was made to detect mentions of drugs/chemicals in English texts, so far only very limited attempts were made to recognize them in medical documents in other languages. Taking into account the growing amount of medical publications and clinical records written in Spanish, we have organized the first shared task on detecting drug and chemical entities in Spanish medical documents. Additionally, we included a clinical concept-indexing sub-track asking teams to return SNOMED-CT identifiers related to drugs/chemicals for a collection of documents. For this task, named PharmaCoNER, we generated annotation guidelines together with a corpus of 1,000 manually annotated clinical case studies. A total of 22 teams participated in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system runs). Top scoring teams used sophisticated deep learning approaches yielding very competitive results with F-measures above 0.91. These results indicate that there is a real interest in promoting biomedical text mining efforts beyond English. We foresee that the PharmaCoNER annotation guidelines, corpus and participant systems will foster the development of new resources for clinical and biomedical text mining systems of Spanish medical data.",what Concept types ?,chemical,chemical compounds,False,True
"This paper presents the fourth edition of the Bacteria Biotope task at BioNLP Open Shared Tasks 2019. The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and full-text excerpts, and the characterization of these entities with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology). The task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology. The paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization. We also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016.",what Concept types ?,Microorganism,microorganisms,False,True
"This paper presents work on a method to detect names of proteins in running text. Our system - Yapex - uses a combination of lexical and syntactic knowledge, heuristic filters and a local dynamic dictionary. The syntactic information given by a general-purpose off-the-shelf parser supports the correct identification of the boundaries of protein names, and the local dynamic dictionary finds protein names in positions incompletely analysed by the parser. We present the different steps involved in our approach to protein tagging, and show how combinations of them influence recall and precision. We evaluate the system on a corpus of MEDLINE abstracts and compare it with the KeX system (Fukuda et al., 1998) along four different notions of correctness.",what Concept types ?,Protein,proteins,False,True
"BioC is a simple XML format for text, annotations and relations, and was developed to achieve interoperability for biomedical text processing. Following the success of BioC in BioCreative IV, the BioCreative V BioC track addressed a collaborative task to build an assistant system for BioGRID curation. In this paper, we describe the framework of the collaborative BioC task and discuss our findings based on the user survey. This track consisted of eight subtasks including gene/protein/organism named entity recognition, protein–protein/genetic interaction passage identification and annotation visualization. Using BioC as their data-sharing and communication medium, nine teams, world-wide, participated and contributed either new methods or improvements of existing tools to address different subtasks of the BioC track. Results from different teams were shared in BioC and made available to other teams as they addressed different subtasks of the track. In the end, all submitted runs were merged using a machine learning classifier to produce an optimized output. The biocurator assistant system was evaluated by four BioGRID curators in terms of practical usability. The curators’ feedback was overall positive and highlighted the user-friendly design and the convenient gene/protein curation tool based on text mining. Database URL: http://www.biocreative.org/tasks/biocreative-v/track-1-bioc/",what Concept types ?,gene,"gene / protein / organism named entity recognition, protein – protein /",False,True
"Abstract The text-mining services for kinome curation track, part of BioCreative VI, proposed a competition to assess the effectiveness of text mining to perform literature triage. The track has exploited an unpublished curated data set from the neXtProt database. This data set contained comprehensive annotations for 300 human protein kinases. For a given protein and a given curation axis [diseases or gene ontology (GO) biological processes], participants’ systems had to identify and rank relevant articles in a collection of 5.2 M MEDLINE citations (task 1) or 530 000 full-text articles (task 2). Explored strategies comprised named-entity recognition and machine-learning frameworks. For that latter approach, participants developed methods to derive a set of negative instances, as the databases typically do not store articles that were judged as irrelevant by curators. The supervised approaches proposed by the participating groups achieved significant improvements compared to the baseline established in a previous study and compared to a basic PubMed search.",what data source ?,MEDLINE,nextprot database.,False,False
"Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for biotextmining. As the focus of information extraction is shifting from ""nominal"" information such as named entity to ""verbal"" information such as function and interaction of substances, application of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sentences is in demand. A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XMLbased format based on Penn Treebank II (PTB) scheme. Inter-annotator agreement test indicated that the writing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of biology with appropriate guidelines regarding to linguistic phenomena particular to scientific texts.",what data source ?,MEDLINE,genia corpus,False,False
"In order to deal with heterogeneous knowledge in the medical field, this paper proposes a method which can learn a heavy-weighted medical ontology based on medical glossaries and Web resources. Firstly, terms and taxonomic relations are extracted based on disease and drug glossaries and a light-weighted ontology is constructed, Secondly, non-taxonomic relations are automatically learned from Web resources with linguistic patterns, and the two ontologies (disease and drug) are expanded from light-weighted level towards heavy-weighted level, At last, the disease ontology and drug ontology are integrated to create a practical medical ontology. Experiment shows that this method can integrate and expand medical terms with taxonomic and different kinds of non-taxonomic relations. Our experiments show that the performance is promising.",what data source ?,Medical glossaries,web resources.,False,False
"A rapidly growing amount of content posted online, such as food recipes, opens doors to new exciting applications at the intersection of vision and language. In this work, we aim to estimate the calorie amount of a meal directly from an image by learning from recipes people have published on the Internet, thus skipping time-consuming manual data annotation. Since there are few large-scale publicly available datasets captured in unconstrained environments, we propose the pic2kcal benchmark comprising 308 000 images from over 70 000 recipes including photographs, ingredients, and instructions. To obtain nutritional information of the ingredients and automatically determine the ground-truth calorie value, we match the items in the recipes with structured information from a food item database. We evaluate various neural networks for regression of the calorie quantity and extend them with the multi-task paradigm. Our learning procedure combines the calorie estimation with prediction of proteins, carbohydrates, and fat amounts as well as a multi-label ingredient classification. Our experiments demonstrate clear benefits of multi-task learning for calorie estimation, surpassing the single-task calorie regression by 9.9%. To encourage further research on this task, we make the code for generating the dataset and the models publicly available.",what data source ?,Food item database,pic2kcal,False,False
"We present the BioCreative VII Task 3 which focuses on drug names extraction from tweets. Recognized to provide unique insights into population health, detecting health related tweets is notoriously challenging for natural language processing tools. Tweets are written about any and all topics, most of them not related to health. Additionally, they are written with little regard for proper grammar, are inherently colloquial, and are almost never proof-read. Given a tweet, task 3 consists of detecting if the tweet has a mention of a drug name and, if so, extracting the span of the drug mention. We made available 182,049 tweets publicly posted by 212 Twitter users with all drugs mentions manually annotated. This corpus exhibits the natural and strongly imbalanced distribution of positive tweets, with only 442 tweets (0.2%) mentioning a drug. This task was an opportunity for participants to evaluate methods robust to classimbalance beyond the simple lexical match. A total of 65 teams registered, and 16 teams submitted a system run. We summarize the corpus and the tools created for the challenge, which is freely available at https://biocreative.bioinformatics.udel.edu/tasks/biocreativevii/track-3/. We analyze the methods and the results of the competing systems with a focus on learning from classimbalanced data. Keywords—social media; pharmacovigilance; named entity recognition; drug name extraction; class-imbalance.",what data source ?,Twitter,https : / / biocreative. bioinformatics.,False,False
"Motivation: Coreference resolution, the process of identifying different mentions of an entity, is a very important component in a text-mining system. Compared with the work in news articles, the existing study of coreference resolution in biomedical texts is quite preliminary by only focusing on specific types of anaphors like pronouns or definite noun phrases, using heuristic methods, and running on small data sets. Therefore, there is a need for an in-depth exploration of this task in the biomedical domain. Results: In this article, we presented a learning-based approach to coreference resolution in the biomedical domain. We made three contributions in our study. Firstly, we annotated a large scale coreference corpus, MedCo, which consists of 1,999 medline abstracts in the GENIA data set. Secondly, we proposed a detailed framework for the coreference resolution task, in which we augmented the traditional learning model by incorporating non-anaphors into training. Lastly, we explored various sources of knowledge for coreference resolution, particularly, those that can deal with the complexity of biomedical texts. The evaluation on the MedCo corpus showed promising results. Our coreference resolution system achieved a high precision of 85.2% with a reasonable recall of 65.3%, obtaining an F-measure of 73.9%. The results also suggested that our augmented learning model significantly boosted precision (up to 24.0%) without much loss in recall (less than 5%), and brought a gain of over 8% in F-measure.",what data source ?,MEDLINE,"medco,",False,False
"The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall comprehension of documents by both users and machines. In this paper we introduce DoCO, the Document Components Ontology, an OWL 2 DL ontology that provides a general-purpose structured vocabulary of document elements to describe both structural and rhetorical document components in RDF. In addition to describing the formal description of the ontology, this paper showcases its utility in practice in a variety of our own applications and other activities of the Semantic Publishing community that rely on DoCO to annotate and retrieve document components of scholarly articles.",what Ontology ?,DoCO,"doco,",True,True
"The SPAR Ontology Network is a suite of complementary ontology modules to describe the scholarly publishing domain. BiDO Standard Bibliometric Measures is part of its set of ontologies. It allows describing of numerical and categorical bibliometric data such as h-index, author citation count, journal impact factor. These measures may be used to evaluate scientific production of researchers. However, they are not enough. In a previous study, we determined the lack of some terms to provide a more complete representation of scientific production. Hence, we have built an extension using the NeOn Methodology to restructure the BiDO ontology. With this extension, it is possible to represent and measure the number of documents from research, the number of citations from a paper and the number of publications in high impact journals according to its area and discipline.",what Ontology ?,BIDO,spar ontology network,False,False
"The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall comprehension of documents by both users and machines. In this paper we introduce DoCO, the Document Components Ontology, an OWL 2 DL ontology that provides a general-purpose structured vocabulary of document elements to describe both structural and rhetorical document components in RDF. In addition to describing the formal description of the ontology, this paper showcases its utility in practice in a variety of our own applications and other activities of the Semantic Publishing community that rely on DoCO to annotate and retrieve document components of scholarly articles.",what Ontology ?,DoCO,"doco,",True,True
"While the fast-paced inception of novel tasks and new datasets helps foster active research in a community towards interesting directions, keeping track of the abundance of research activity in different areas on different datasets is likely to become increasingly difficult. The community could greatly benefit from an automatic system able to summarize scientific results, e.g., in the form of a leaderboard. In this paper we build two datasets and develop a framework (TDMS-IE) aimed at automatically extracting task, dataset, metric and score from NLP papers, towards the automatic construction of leaderboards. Experiments show that our model outperforms several baselines by a large margin. Our model is a first step towards automatic leaderboard construction, e.g., in the NLP domain.",what has model ?,TDMS-IE,tdms - ie ),False,False
"Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT’14 English-French and WMT’16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.",what has model ?,PBSMT,neural and a phrase - based model.,False,False
"We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",what has model ?,DyGIE++,dygie + + ),False,False
"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",what has model ?,XLNet,"xlnet,",True,True
"This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.",what has model ?,VPN,vpn ),True,True
"We introduce SpERT, an attention model for span-based joint entity and relation extraction. Our key contribution is a light-weight reasoning on BERT embeddings, which features entity recognition and filtering, as well as relation classification with a localized, marker-free context representation. The model is trained using strong within-sentence negative samples, which are efficiently extracted in a single BERT pass. These aspects facilitate a search over all spans in the sentence. In ablation studies, we demonstrate the benefits of pre-training, strong negative sampling and localized context. Our model outperforms prior work by up to 2.6% F1 score on several datasets for joint entity and relation extraction.",what has model ?,SpERT,"spert,",True,True
"Although simple individually, artificial neurons provide state-of-the-art performance when interconnected in deep networks. Arguably, the Tsetlin Automaton is an even simpler and more versatile learning mechanism, capable of solving the multi-armed bandit problem. Merely by means of a single integer as memory, it learns the optimal action in stochastic environments through increment and decrement operations. In this paper, we introduce the Tsetlin Machine, which solves complex pattern recognition problems with propositional formulas, composed by a collective of Tsetlin Automata. To eliminate the longstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine orchestrates the automata using a novel game. Further, both inputs, patterns, and outputs are expressed as bits, while recognition and learning rely on bit manipulation, simplifying computation. Our theoretical analysis establishes that the Nash equilibria of the game align with the propositional formulas that provide optimal pattern recognition accuracy. This translates to learning without local optima, only global ones. In five benchmarks, the Tsetlin Machine provides competitive accuracy compared with SVMs, Decision Trees, Random Forests, Naive Bayes Classifier, Logistic Regression, and Neural Networks. We further demonstrate how the propositional formulas facilitate interpretation. We believe the combination of high accuracy, interpretability, and computational simplicity makes the Tsetlin Machine a promising tool for a wide range of domains.",what has model ?,Tsetlin Machine,"tsetlin machine,",True,True
"Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.",what has model ?,DCN,dynamic coattention network ( dcn ),False,True
"Increasingly large document collections require improved information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of traditional supervised classifiers has degraded as the number of documents has increased. This is because along with growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.",what has model ?,HDLTex,hdltex ),True,True
"Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.",what has model ?,SRU++,"sru + +,",False,False
"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel {\em table-sequence encoders} where two different encoders -- a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having {\em two} encoders over {\em one} encoder. On several standard datasets, our model shows significant improvements over existing approaches.",what has model ?,Table-Sequence,{ \ em table - sequence encoders },False,False
"Machine comprehension(MC) style question answering is a representative problem in natural language processing. Previous methods rarely spend time on the improvement of encoding layer, especially the embedding of syntactic information and name entity of the words, which are very crucial to the quality of encoding. Moreover, existing attention methods represent each query word as a vector or use a single vector to represent the whole query sentence, neither of them can handle the proper weight of the key words in query sentence. In this paper, we introduce a novel neural network architecture called Multi-layer Embedding with Memory Network(MEMEN) for machine reading task. In the encoding layer, we employ classic skip-gram model to the syntactic and semantic information of the words to train a new kind of embedding layer. We also propose a memory network of full-orientation matching of the query and passage to catch more pivotal information. Experiments show that our model has competitive results both from the perspectives of precision and efficiency in Stanford Question Answering Dataset(SQuAD) among all published results and achieves the state-of-the-art results on TriviaQA dataset.",what has model ?,MEMEN,skip - gram model to the syntactic and semantic information of the words to train a new kind of embedding layer. we also propose a memory network of full - orientation matching of the query and passage to catch more pivotal information. experiments show that our model has competitive results both from the perspectives of precision and efficiency in stanford question answering dataset ( squad ),False,False
"While normalizing flows have led to significant advances in modeling high-dimensional continuous distributions, their applicability to discrete distributions remains unknown. In this paper, we show that flows can in fact be extended to discrete events---and under a simple change-of-variables formula not requiring log-determinant-Jacobian computations. Discrete flows have numerous applications. We consider two flow architectures: discrete autoregressive flows that enable bidirectionality, allowing, for example, tokens in text to depend on both left-to-right and right-to-left contexts in an exact language model; and discrete bipartite flows that enable efficient non-autoregressive generation as in RealNVP. Empirically, we find that discrete autoregressive flows outperform autoregressive baselines on synthetic discrete distributions, an addition task, and Potts models; and bipartite flows can obtain competitive performance with autoregressive baselines on character-level language modeling for Penn Tree Bank and text8.",what has model ?,Bipartite Flow,realnvp.,False,False
"Modern machine learning algorithms crucially rely on several design decisions to achieve strong performance, making the problem of Hyperparameter Optimization (HPO) more important than ever. Here, we combine the advantages of the popular bandit-based HPO method Hyperband (HB) and the evolutionary search approach of Differential Evolution (DE) to yield a new HPO method which we call DEHB. Comprehensive results on a very broad range of HPO problems, as well as a wide range of tabular benchmarks from neural architecture search, demonstrate that DEHB achieves strong performance far more robustly than all previous HPO methods we are aware of, especially for high-dimensional problems with discrete input dimensions. For example, DEHB is up to 1000x faster than random search. It is also efficient in computational time, conceptually simple and easy to implement, positioning it well to become a new default HPO method.",what keywords ?,Differential Evolution,"machine learning algorithms crucially rely on several design decisions to achieve strong performance, making the problem of hyperparameter optimization",False,False
"A novel and highly sensitive nonenzymatic glucose biosensor was developed by nucleating colloidal silver nanoparticles (AgNPs) on MoS2. The facile fabrication method, high reproducibility (97.5%) and stability indicates a promising capability for large-scale manufacturing. Additionally, the excellent sensitivity (9044.6 μA·mM−1·cm−2), low detection limit (0.03 μM), appropriate linear range of 0.1–1000 μM, and high selectivity suggests that this biosensor has a great potential to be applied for noninvasive glucose detection in human body fluids, such as sweat and saliva.",what keywords ?,glucose biosensor,glucose biosensor was developed by nucleating colloidal silver nanoparticles,False,True
"Herein, a novel electrochemical glucose biosensor based on glucose oxidase (GOx) immobilized on a surface containing platinum nanoparticles (PtNPs) electrodeposited on poly(Azure A) (PAA) previously electropolymerized on activated screen-printed carbon electrodes (GOx-PtNPs-PAA-aSPCEs) is reported. The resulting electrochemical biosensor was validated towards glucose oxidation in real samples and further electrochemical measurement associated with the generated H2O2. The electrochemical biosensor showed an excellent sensitivity (42.7 μA mM−1 cm−2), limit of detection (7.6 μM), linear range (20 μM–2.3 mM), and good selectivity towards glucose determination. Furthermore, and most importantly, the detection of glucose was performed at a low potential (0.2 V vs. Ag). The high performance of the electrochemical biosensor was explained through surface exploration using field emission SEM, XPS, and impedance measurements. The electrochemical biosensor was successfully applied to glucose quantification in several real samples (commercial juices and a plant cell culture medium), exhibiting a high accuracy when compared with a classical spectrophotometric method. This electrochemical biosensor can be easily prepared and opens up a good alternative in the development of new sensitive glucose sensors.",what keywords ?, activated screen-printed carbon electrodes,glucose oxidase,False,False
"Abstract With the increased dependence on online learning platforms and educational resource repositories, a unified representation of digital learning resources becomes essential to support a dynamic and multi-source learning experience. We introduce the EduCOR ontology, an educational, career-oriented ontology that provides a foundation for representing online learning resources for personalised learning systems. The ontology is designed to enable learning material repositories to offer learning path recommendations, which correspond to the user’s learning goals and preferences, academic and psychological parameters, and labour-market skills. We present the multiple patterns that compose the EduCOR ontology, highlighting its cross-domain applicability and integrability with other ontologies. A demonstration of the proposed ontology on the real-life learning platform eDoer is discussed as a use case. We evaluate the EduCOR ontology using both gold standard and task-based approaches. The comparison of EduCOR to three gold schemata, and its application in two use-cases, shows its coverage and adaptability to multiple OER repositories, which allows generating user-centric and labour-market oriented recommendations. Resource : <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""https://tibonto.github.io/educor/"">https://tibonto.github.io/educor/</jats:ext-link>.",what keywords ?,Skill,labour - market skills.,False,True
"A new variant of the classic pulsed laser deposition (PLD) process is introduced as a room-temperature dry process for the growth and stoichiometry control of hybrid perovskite films through the use of nonstoichiometric single target ablation and off-axis growth. Mixed halide hybrid perovskite films nominally represented by CH3NH3PbI3–xAx (A = Cl or F) are also grown and are shown to reveal interesting trends in the optical properties and photoresponse. Growth of good quality lead-free CH3NH3SnI3 films is also demonstrated, and the corresponding optical properties are presented. Finally, perovskite solar cells fabricated at room temperature (which makes the process adaptable to flexible substrates) are shown to yield a conversion efficiency of about 7.7%.",what keywords ?,Solar Cells,laser deposition,False,False
"This paper presents a new RF MEMS tunable capacitor based on the zipper principle and with interdigitated RF and actuation electrodes. The electrode configuration prevents dielectric charging under high actuation voltages. It also increases the capacitance ratio and the tunable analog range. The effect of the residual stress on the capacitance tunability is also investigated. Two devices with different interdigital RF and actuation electrodes are fabricated on an alumina substrate and result in a capacitance ratio around 3.0 (Cmin = 70?90 fF, Cmax = 240?270 fF) and with a Q > 100 at 3 GHz. This design can be used in wideband tunable filters and matching networks.",what keywords ?,Tunable capacitor,capacitor based on the zipper principle and with interdigitated rf and actuation electrodes.,False,False
"Radioresistant hypoxic cells may contribute to the failure of radiation therapy in controlling certain tumors. Some studies have suggested the radiosensitizing effect of paclitaxel. The poly(D,L-lactide-co-glycolide)(PLGA) nanoparticles containing paclitaxel were prepared by o/w emulsification-solvent evaporation method. The physicochemical characteristics of the nanoparticles (i.e. encapsulation efficiency, particle size distribution, morphology, in vitro release) were studied. The morphology of the two human tumor cell lines: a carcinoma cervicis (HeLa) and a hepatoma (HepG2), treated with paclitaxel-loaded nanoparticles was photomicrographed. Flow cytometry was used to quantify the number of the tumor cells held in the G2/M phase of the cell cycle. The cellular uptake of nanoparticles was evaluated by transmission electronic microscopy. Cell viability was determined by the ability of single cell to form colonies in vitro. The prepared nanoparticles were spherical in shape with size between 200nm and 800nm. The encapsulation efficiency was 85.5％. The release behaviour of paclitaxel from the nanoparticles exhibited a biphasic pattern characterised by a fast initial release during the first 24 h, followed by a slower and continuous release. Co-culture of the two tumor cell lines with paclitaxel-loaded nanoparticles demonstrated that the cell morphology was changed and the released paclitaxel retained its bioactivity to block cells in the G2/M phase. The cellular uptake of nanoparticles was observed. The free paclitaxel and paclitaxel-loaded nanoparticles effectively sensitized hypoxic HeLa and HepG2 cells to radiation. Under this experimental condition, the radiosensitization of paclitaxel-loaded nanoparticles was more significant than that of free paclitaxel.Keywords: Paclitaxel；Drug delivery；Nanoparticle；Radiotherapy；Hypoxia；Human tumor cells；cellular uptake",what keywords ?,Hypoxia,paclitaxel.,False,False
"In this report, we demonstrate high spectral responsivity (SR) solar blind deep ultraviolet (UV) β-Ga2O3 metal-semiconductor-metal (MSM) photodetectors grown by the mist chemical-vapor deposition (Mist-CVD) method. The β-Ga2O3 thin film was grown on c-plane sapphire substrates, and the fabricated MSM PDs with Al contacts in an interdigitated geometry were found to exhibit peak SR>150A/W for the incident light wavelength of 254 nm at a bias of 20 V. The devices exhibited very low dark current, about 14 pA at 20 V, and showed sharp transients with a photo-to-dark current ratio>105. The corresponding external quantum efficiency is over 7 × 104%. The excellent deep UV β-Ga2O3 photodetectors will enable significant advancements for the next-generation photodetection applications.",what keywords ?,Photodetectors,,False,False
"PurposeTo develop a novel nanoparticle drug delivery system consisting of chitosan and glyceryl monooleate (GMO) for the delivery of a wide variety of therapeutics including paclitaxel.MethodsChitosan/GMO nanoparticles were prepared by multiple emulsion (o/w/o) solvent evaporation methods. Particle size and surface charge were determined. The morphological characteristics and cellular adhesion were evaluated with surface or transmission electron microscopy methods. The drug loading, encapsulation efficiency, in vitro release and cellular uptake were determined using HPLC methods. The safety and efficacy were evaluated by MTT cytotoxicity assay in human breast cancer cells (MDA-MB-231).ResultsThese studies provide conceptual proof that chitosan/GMO can form polycationic nano-sized particles (400 to 700 nm). The formulation demonstrates high yields (98 to 100%) and similar entrapment efficiencies. The lyophilized powder can be stored and easily be resuspended in an aqueous matrix. The nanoparticles have a hydrophobic inner-core with a hydrophilic coating that exhibits a significant positive charge and sustained release characteristics. This novel nanoparticle formulation shows evidence of mucoadhesive properties; a fourfold increased cellular uptake and a 1000-fold reduction in the IC50 of PTX.ConclusionThese advantages allow lower doses of PTX to achieve a therapeutic effect, thus presumably minimizing the adverse side effects.",what keywords ?,Nanoparticles,paclitaxel.,False,False
"Herein, we incorporated dual biotemplates, i.e., cellulose nanocrystals (CNC) and apoferritin, into electrospinning solution to achieve three distinct benefits, i.e., (i) facile synthesis of a WO3 nanotube by utilizing the self-agglomerating nature of CNC in the core of as-spun nanofibers, (ii) effective sensitization by partial phase transition from WO3 to Na2W4O13 induced by interaction between sodium-doped CNC and WO3 during calcination, and (iii) uniform functionalization with monodispersive apoferritin-derived Pt catalytic nanoparticles (2.22 ± 0.42 nm). Interestingly, the sensitization effect of Na2W4O13 on WO3 resulted in highly selective H2S sensing characteristics against seven different interfering molecules. Furthermore, synergistic effects with a bioinspired Pt catalyst induced a remarkably enhanced H2S response ( Rair/ Rgas = 203.5), unparalleled selectivity ( Rair/ Rgas < 1.3 for the interfering molecules), and rapid response (<10 s)/recovery (<30 s) time at 1 ppm of H2S under 95% relative humidity level. This work paves the way for a new class of cosensitization routes to overcome critical shortcomings of SMO-based chemical sensors, thus providing a potential platform for diagnosis of halitosis.",what keywords ?, apoferritin,"biotemplates,",False,False
"This paper studies the effect of surface roughness on up-state and down-state capacitances of microelectromechanical systems (MEMS) capacitive switches. When the root-mean-square (RMS) roughness is 10 nm, the up-state capacitance is approximately 9% higher than the theoretical value. When the metal bridge is driven down, the normalized contact area between the metal bridge and the surface of the dielectric layer is less than 1% if the RMS roughness is larger than 2 nm. Therefore, the down-state capacitance is actually determined by the non-contact part of the metal bridge. The normalized isolation is only 62% for RMS roughness of 10 nm when the hold-down voltage is 30 V. The analysis also shows that the down-state capacitance and the isolation increase with the hold-down voltage. The normalized isolation increases from 58% to 65% when the hold-down voltage increases from 10 V to 60 V for RMS roughness of 10 nm.",what keywords ?,MEMS,surface roughness,False,False
"A hybrid cascaded configuration consisting of a fiber Sagnac interferometer (FSI) and a Fabry-Perot interferometer (FPI) was proposed and experimentally demonstrated to enhance the temperature intensity by the Vernier-effect. The FSI, which consists of a certain length of Panda fiber, is for temperature sensing, while the FPI acts as a filter due to its temperature insensitivity. The two interferometers have almost the same free spectral range, with the spectral envelope of the cascaded sensor shifting much more than the single FSI. Experimental results show that the temperature sensitivity is enhanced from −1.4 nm/°C (single FSI) to −29.0 (cascaded configuration). The enhancement factor is 20.7, which is basically consistent with theoretical analysis (19.9).",what keywords ?,cascaded configuration,,False,False
"E-learning recommender systems are gaining significance nowadays due to its ability to enhance the learning experience by providing tailor-made services based on learner preferences. A Personalized Learning Environment (PLE) that automatically adapts to learner characteristics such as learning styles and knowledge level can recommend appropriate learning resources that would favor the learning process and improve learning outcomes. The pure cold-start problem is a relevant issue in PLEs, which arises due to the lack of prior information about the new learner in the PLE to create appropriate recommendations. This article introduces a semantic framework based on ontology to address the pure cold-start problem in content recommenders. The ontology encapsulates the domain knowledge about the learners as well as Learning Objects (LOs). The semantic model that we built has been experimented with different combinations of the key learner parameters such as learning style, knowledge level, and background knowledge. The proposed framework utilizes these parameters to build natural learner groups from the learner ontology using SPARQL queries. The ontology holds 480 learners’ data, 468 annotated learning objects with 5,600 learner ratings. A multivariate k-means clustering algorithm, an unsupervised machine learning technique for grouping similar data, is used to evaluate the learner similarity computation accuracy. The learner satisfaction achieved with the proposed model is measured based on the ratings given by the 40 participants of the experiments. From the evaluation perspective, it is evident that 79% of the learners are satisfied with the recommendations generated by the proposed model in pure cold-start condition.",what keywords ?,personalized learning environment,ontology,False,False
"An electrically conductive ultralow percolation threshold of 0.1 wt% graphene was observed in the thermoplastic polyurethane (TPU) nanocomposites. The homogeneously dispersed graphene effectively enhanced the mechanical properties of TPU significantly at a low graphene loading of 0.2 wt%. These nanocomposites were subjected to cyclic loading to investigate the influences of graphene loading, strain amplitude and strain rate on the strain sensing performances. The two dimensional graphene and the flexible TPU matrix were found to endow these nanocomposites with a wide range of strain sensitivity (gauge factor ranging from 0.78 for TPU with 0.6 wt% graphene at the strain rate of 0.1 min−1 to 17.7 for TPU with 0.2 wt% graphene at the strain rate of 0.3 min−1) and good sensing stability for different strain patterns. In addition, these nanocomposites demonstrated good recoverability and reproducibility after stabilization by cyclic loading. An analytical model based on tunneling theory was used to simulate the resistance response to strain under different strain rates. The change in the number of conductive pathways and tunneling distance under strain was responsible for the observed resistance-strain behaviors. This study provides guidelines for the fabrication of graphene based polymer strain sensors.",what keywords ?,Graphene,,False,False
"Treatment of breast cancer underwent extensive progress in recent years with molecularly targeted therapies. However, non-specific pharmaceutical approaches (chemotherapy) persist, inducing severe side-effects. Phytochemicals provide a promising alternative for breast cancer prevention and treatment. Specifically, resveratrol (res) is a plant-derived polyphenolic phytoalexin with potent biological activity but displays poor water solubility, limiting its clinical use. Here we have developed a strategy for delivering res using a newly synthesized nano-carrier with the potential for both diagnosis and treatment. Methods: Res-loaded nanoparticles were synthesized by the emulsion method using Pluronic F127 block copolymer and Vitamin E-TPGS. Nanoparticle characterization was performed by SEM and tunable resistive pulse sensing. Encapsulation Efficiency (EE%) and Drug Loading (DL%) content were determined by analysis of the supernatant during synthesis. Nanoparticle uptake kinetics in breast cancer cell lines MCF-7 and MDA-MB-231 as well as in MCF-10A breast epithelial cells were evaluated by flow cytometry and the effects of res on cell viability via MTT assay. Results: Res-loaded nanoparticles with spherical shape and a dominant size of 179±22 nm were produced. Res was loaded with high EE of 73±0.9% and DL content of 6.2±0.1%. Flow cytometry revealed higher uptake efficiency in breast cancer cells compared to the control. An MTT assay showed that res-loaded nanoparticles reduced the viability of breast cancer cells with no effect on the control cells. Conclusions: These results demonstrate that the newly synthesized nanoparticle is a good model for the encapsulation of hydrophobic drugs. Additionally, the nanoparticle delivers a natural compound and is highly effective and selective against breast cancer cells rendering this type of nanoparticle an excellent candidate for diagnosis and therapy of difficult to treat mammary malignancies.",what keywords ?,Resveratrol,breast cancer,False,False
"Modern machine learning algorithms crucially rely on several design decisions to achieve strong performance, making the problem of Hyperparameter Optimization (HPO) more important than ever. Here, we combine the advantages of the popular bandit-based HPO method Hyperband (HB) and the evolutionary search approach of Differential Evolution (DE) to yield a new HPO method which we call DEHB. Comprehensive results on a very broad range of HPO problems, as well as a wide range of tabular benchmarks from neural architecture search, demonstrate that DEHB achieves strong performance far more robustly than all previous HPO methods we are aware of, especially for high-dimensional problems with discrete input dimensions. For example, DEHB is up to 1000x faster than random search. It is also efficient in computational time, conceptually simple and easy to implement, positioning it well to become a new default HPO method.",what keywords ?,HPO,"machine learning algorithms crucially rely on several design decisions to achieve strong performance, making the problem of hyperparameter optimization",False,False
"Recently, pure transformer-based models have shown great potentials for vision tasks such as image classification and detection. However, the design of transformer networks is challenging. It has been observed that the depth, embedding dimension, and number of heads can largely affect the performance of vision transformers. Previous models configure these dimensions based upon manual crafting. In this work, we propose a new one-shot architecture search framework, namely AutoFormer, dedicated to vision transformer search. AutoFormer entangles the weights of different blocks in the same layers during supernet training. Benefiting from the strategy, the trained supernet allows thousands of subnets to be very well-trained. Specifically, the performance of these subnets with weights inherited from the supernet is comparable to those retrained from scratch. Besides, the searched models, which we refer to AutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In particular, AutoFormer-tiny/small/base achieve 74.7%/81.7%/82.4% top-1 accuracy on ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify the transferability of AutoFormer by providing the performance on downstream benchmarks and distillation experiments. Code and models are available at https://github.com/microsoft/Cream.",what keywords ?,Image Classification,image classification and detection.,False,True
"Abstract Despite rapid progress, most of the educational technologies today lack a strong instructional design knowledge basis leading to questionable quality of instruction. In addition, a major challenge is to customize these educational technologies for a wide range of customizable instructional designs. Ontologies are one of the pertinent mechanisms to represent instructional design in the literature. However, existing approaches do not support modeling of flexible instructional designs. To address this problem, in this paper, we propose an ontology based framework for systematic modeling of different aspects of instructional design knowledge based on domain patterns. As part of the framework, we present ontologies for modeling goals , instructional processes and instructional material . We demonstrate the ontology framework by presenting instances of the ontology for the large scale case study of adult literacy in India (287 million learners spread across 22 Indian Languages), which requires creation of hundreds of similar but varied e Learning Systems based on flexible instructional designs. The implemented framework is available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://rice.iiit.ac.in"">http://rice.iiit.ac.in</jats:ext-link> and is transferred to National Literacy Mission Authority of Government of India . The proposed framework could be potentially used for modeling instructional design knowledge for school education, vocational skills and beyond.",what keywords ?,instructional process,,False,False
"PurposeTo develop a novel nanoparticle drug delivery system consisting of chitosan and glyceryl monooleate (GMO) for the delivery of a wide variety of therapeutics including paclitaxel.MethodsChitosan/GMO nanoparticles were prepared by multiple emulsion (o/w/o) solvent evaporation methods. Particle size and surface charge were determined. The morphological characteristics and cellular adhesion were evaluated with surface or transmission electron microscopy methods. The drug loading, encapsulation efficiency, in vitro release and cellular uptake were determined using HPLC methods. The safety and efficacy were evaluated by MTT cytotoxicity assay in human breast cancer cells (MDA-MB-231).ResultsThese studies provide conceptual proof that chitosan/GMO can form polycationic nano-sized particles (400 to 700 nm). The formulation demonstrates high yields (98 to 100%) and similar entrapment efficiencies. The lyophilized powder can be stored and easily be resuspended in an aqueous matrix. The nanoparticles have a hydrophobic inner-core with a hydrophilic coating that exhibits a significant positive charge and sustained release characteristics. This novel nanoparticle formulation shows evidence of mucoadhesive properties; a fourfold increased cellular uptake and a 1000-fold reduction in the IC50 of PTX.ConclusionThese advantages allow lower doses of PTX to achieve a therapeutic effect, thus presumably minimizing the adverse side effects.",what keywords ?,mucoadhesive,paclitaxel.,False,False
"A simple and inexpensive method for growing Ga<sub>2</sub>O<sub>3</sub> using GaAs wafers is demonstrated. Si-doped GaAs wafers are heated to 1050 °C in a horizontal tube furnace in both argon and air ambients in order to convert their surfaces to β-Ga<sub>2</sub>O<sub>3</sub>. The β-Ga<sub>2</sub>O<sub>3</sub> films are characterized using scanning electron micrograph, energy-dispersive X-ray spectroscopy, and X-ray diffraction. They are also used to fabricate solar blind photodetectors. The devices, which had nanotextured surfaces, exhibited a high sensitivity to ultraviolet (UV) illumination due in part to large surface areas. Furthermore, the films have coherent interfaces with the substrate, which leads to a robust device with high resistance to thermo-mechanical stress. The photoconductance of the β-Ga<sub>2</sub>O<sub>3</sub> films is found to increase by more than three orders of magnitude under 270 nm ultraviolet illumination with respect to the dark current. The fabricated device shows a responsivity of ∼292 mA/W at this wavelength.",what keywords ?,photodetector,,False,False
"Programs offered by academic institutions in higher education need to meet specific standards that are established by the appropriate accreditation bodies. Curriculum mapping is an important part of the curriculum management process that is used to document the expected learning outcomes, ensure quality, and align programs and courses with industry standards. Semantic web languages can be used to express and share common agreement about the vocabularies used in the domain under study. In this paper, we present an approach based on ontology for curriculum mapping in higher education. Our proposed approach is focused on the creation of a core curriculum ontology that can support effective knowledge representation and knowledge discovery. The research work presents the case of ontology reuse through the extension of the curriculum ontology to support the creation of micro-credentials. We also present a conceptual framework for knowledge discovery to support various business use case scenarios based on ontology inferencing and querying operations.",what keywords ?,curriculum ontology,curriculum mapping,False,False
"Poor delivery of insoluble anticancer drugs has so far precluded their clinical application. In this study, we developed a tumor-targeting delivery system for insoluble drug (paclitaxel, PTX) by PEGylated O-carboxymethyl-chitosan (CMC) nanoparticles grafted with cyclic Arg-Gly-Asp (RGD) peptide. To improve the loading efficiency (LE), we combined O/W/O double emulsion method with temperature-programmed solidification technique and controlled PTX within the matrix network as in situ nanocrystallite form. Furthermore, these CMC nanoparticles were PEGylated, which could reduce recognition by the reticuloendothelial system (RES) and prolong the circulation time in blood. In addition, further graft of cyclic RGD peptide at the terminal of PEG chain endowed these nanoparticles with higher affinity to in vitro Lewis lung carcinoma (LLC) cells and in vivo tumor tissue. These outstanding properties enabled as-designed nanodevice to exhibit a greater tumor growth inhibition effect and much lower side effects over the commercial formulation Taxol.",what keywords ?,Nanoparticles,tumor -,False,False
"Ultralight graphene-based cellular elastomers are found to exhibit nearly frequency-independent piezoresistive behaviors. Surpassing the mechanoreceptors in the human skin, these graphene elastomers can provide an instantaneous and high-fidelity electrical response to dynamic pressures ranging from quasi-static up to 2000 Hz, and are capable of detecting ultralow pressures as small as 0.082 Pa.",what keywords ?,graphene elastomers ,graphene - based cellular elastomers,False,False
CdTe-based solar cells exhibiting 19% power conversion efficiency were produced using widely available thermal evaporation deposition of the absorber layers on SnO2-coated glass with or without a t...,what keywords ?,Solar cells,solar cells,True,True
"Radioresistant hypoxic cells may contribute to the failure of radiation therapy in controlling certain tumors. Some studies have suggested the radiosensitizing effect of paclitaxel. The poly(D,L-lactide-co-glycolide)(PLGA) nanoparticles containing paclitaxel were prepared by o/w emulsification-solvent evaporation method. The physicochemical characteristics of the nanoparticles (i.e. encapsulation efficiency, particle size distribution, morphology, in vitro release) were studied. The morphology of the two human tumor cell lines: a carcinoma cervicis (HeLa) and a hepatoma (HepG2), treated with paclitaxel-loaded nanoparticles was photomicrographed. Flow cytometry was used to quantify the number of the tumor cells held in the G2/M phase of the cell cycle. The cellular uptake of nanoparticles was evaluated by transmission electronic microscopy. Cell viability was determined by the ability of single cell to form colonies in vitro. The prepared nanoparticles were spherical in shape with size between 200nm and 800nm. The encapsulation efficiency was 85.5％. The release behaviour of paclitaxel from the nanoparticles exhibited a biphasic pattern characterised by a fast initial release during the first 24 h, followed by a slower and continuous release. Co-culture of the two tumor cell lines with paclitaxel-loaded nanoparticles demonstrated that the cell morphology was changed and the released paclitaxel retained its bioactivity to block cells in the G2/M phase. The cellular uptake of nanoparticles was observed. The free paclitaxel and paclitaxel-loaded nanoparticles effectively sensitized hypoxic HeLa and HepG2 cells to radiation. Under this experimental condition, the radiosensitization of paclitaxel-loaded nanoparticles was more significant than that of free paclitaxel.Keywords: Paclitaxel；Drug delivery；Nanoparticle；Radiotherapy；Hypoxia；Human tumor cells；cellular uptake",what keywords ?,Nanoparticles,paclitaxel.,False,False
"The development of p-type metal-oxide semiconductors (MOSs) is of increasing interest for applications in next-generation optoelectronic devices, display backplane, and low-power-consumption complementary MOS circuits. Here, we report the high performance of solution-processed, p-channel copper-tin-sulfide-gallium oxide (CTSGO) thin-film transistors (TFTs) using UV/O3 exposure. Hall effect measurement confirmed the p-type conduction of CTSGO with Hall mobility of 6.02 ± 0.50 cm2 V-1 s-1. The p-channel CTSGO TFT using UV/O3 treatment exhibited the field-effect mobility (μFE) of 1.75 ± 0.15 cm2 V-1 s-1 and an on/off current ratio (ION/IOFF) of ∼104 at a low operating voltage of -5 V. The significant enhancement in the device performance is due to the good p-type CTSGO material, smooth surface morphology, and fewer interfacial traps between the semiconductor and the Al2O3 gate insulator. Therefore, the p-channel CTSGO TFT can be applied for CMOS MOS TFT circuits for next-generation display.",what keywords ?,Transistors,semiconductors,False,False
"This paper studies the effect of surface roughness on up-state and down-state capacitances of microelectromechanical systems (MEMS) capacitive switches. When the root-mean-square (RMS) roughness is 10 nm, the up-state capacitance is approximately 9% higher than the theoretical value. When the metal bridge is driven down, the normalized contact area between the metal bridge and the surface of the dielectric layer is less than 1% if the RMS roughness is larger than 2 nm. Therefore, the down-state capacitance is actually determined by the non-contact part of the metal bridge. The normalized isolation is only 62% for RMS roughness of 10 nm when the hold-down voltage is 30 V. The analysis also shows that the down-state capacitance and the isolation increase with the hold-down voltage. The normalized isolation increases from 58% to 65% when the hold-down voltage increases from 10 V to 60 V for RMS roughness of 10 nm.",what keywords ?,Capacitive switches,surface roughness,False,False
"PurposeTo develop a novel nanoparticle drug delivery system consisting of chitosan and glyceryl monooleate (GMO) for the delivery of a wide variety of therapeutics including paclitaxel.MethodsChitosan/GMO nanoparticles were prepared by multiple emulsion (o/w/o) solvent evaporation methods. Particle size and surface charge were determined. The morphological characteristics and cellular adhesion were evaluated with surface or transmission electron microscopy methods. The drug loading, encapsulation efficiency, in vitro release and cellular uptake were determined using HPLC methods. The safety and efficacy were evaluated by MTT cytotoxicity assay in human breast cancer cells (MDA-MB-231).ResultsThese studies provide conceptual proof that chitosan/GMO can form polycationic nano-sized particles (400 to 700 nm). The formulation demonstrates high yields (98 to 100%) and similar entrapment efficiencies. The lyophilized powder can be stored and easily be resuspended in an aqueous matrix. The nanoparticles have a hydrophobic inner-core with a hydrophilic coating that exhibits a significant positive charge and sustained release characteristics. This novel nanoparticle formulation shows evidence of mucoadhesive properties; a fourfold increased cellular uptake and a 1000-fold reduction in the IC50 of PTX.ConclusionThese advantages allow lower doses of PTX to achieve a therapeutic effect, thus presumably minimizing the adverse side effects.",what keywords ?,Chitosan,paclitaxel.,False,False
"Mixed tin (Sn)-lead (Pb) perovskites with high Sn content exhibit low bandgaps suitable for fabricating the bottom cell of perovskite-based tandem solar cells. In this work, we report on the fabrication of efficient mixed Sn-Pb perovskite solar cells using precursors combining formamidinium tin iodide (FASnI3) and methylammonium lead iodide (MAPbI3). The best-performing cell fabricated using a (FASnI3)0.6(MAPbI3)0.4 absorber with an absorption edge of ∼1.2 eV achieved a power conversion efficiency (PCE) of 15.08 (15.00)% with an open-circuit voltage of 0.795 (0.799) V, a short-circuit current density of 26.86(26.82) mA/cm(2), and a fill factor of 70.6(70.0)% when measured under forward (reverse) voltage scan. The average PCE of 50 cells we have fabricated is 14.39 ± 0.33%, indicating good reproducibility.",what keywords ?,Perovskites,solar cells.,False,False
"The Vernier effect of two cascaded in-fiber Mach-Zehnder interferometers (MZIs) based on a spherical-shaped structure has been investigated. The envelope based on the Vernier effect is actually formed by a frequency component of the superimposed spectrum, and the frequency value is determined by the subtraction between the optical path differences of two cascaded MZIs. A method based on band-pass filtering is put forward to extract the envelope efficiently; strain and curvature measurements are carried out to verify the validity of the method. The results show that the strain and curvature sensitivities are enhanced to -8.47 pm/με and -33.70 nm/m-1 with magnification factors of 5.4 and -5.4, respectively. The detection limit of the sensors with the Vernier effect is also discussed.",what keywords ?,Vernier effect,,False,False
"This paper reports on the experimental and theoretical characterization of RF microelectromechanical systems (MEMS) switches for high-power applications. First, we investigate the problem of self-actuation due to high RF power and we demonstrate switches that do not self-actuate or catastrophically fail with a measured RF power of up to 5.5 W. Second, the problem of switch stiction to the down state as a function of the applied RF power is also theoretically and experimentally studied. Finally, a novel switch design with a top electrode is introduced and its advantages related to RF power-handling capabilities are presented. By applying this technology, we demonstrate hot-switching measurements with a maximum power of 0.8 W. Our results, backed by theory and measurements, illustrate that careful design can significantly improve the power-handling capabilities of RF MEMS switches.",what keywords ?,Switch stiction,,False,False
"In this paper, a liquid-based micro thermal convective accelerometer (MTCA) is optimized by the Rayleigh number (<italic>Ra</italic>) based compact model and fabricated using the <inline-formula> <tex-math notation=""LaTeX"">$0.35\mu $ </tex-math></inline-formula> m CMOS MEMS technology. To achieve water-proof performance, the conformal Parylene C coating was adopted as the isolation layer with the accelerated life-testing results of a 9-year-lifetime for liquid-based MTCA. Then, the device performance was characterized considering sensitivity, response time, and noise. Both the theoretical and experimental results demonstrated that fluid with a larger <italic>Ra</italic> number can provide better performance for the MTCA. More significantly, <italic>Ra</italic> based model showed its advantage to make a more accurate prediction than the simple linear model to select suitable fluid to enhance the sensitivity and balance the linear range of the device. Accordingly, an alcohol-based MTCA was achieved with a two-order-of magnitude increase in sensitivity (43.8 mV/g) and one-order-of-magnitude decrease in the limit of detection (LOD) (<inline-formula> <tex-math notation=""LaTeX"">$61.9~\mu \text{g}$ </tex-math></inline-formula>) compared with the air-based MTCA. [2021-0092]",what keywords ?,CMOS,liquid - based micro thermal convective accelerometer,False,False
"In this work, pure and IIIA element doped ZnO thin films were grown on p type silicon (Si) with (100) orientated surface by sol-gel method, and were characterized for comparing their electrical characteristics. The heterojunction parameters were obtained from the current-voltage (I-V) and capacitance-voltage (C-V) characteristics at room temperature. The ideality factor (n), saturation current (Io) and junction resistance of ZnO/p-Si heterojunction for both pure and doped (with Al or In) cases were determined by using different methods at room ambient. Other electrical parameters such as Fermi energy level (EF), barrier height (ΦB), acceptor concentration (Na), built-in potential (Φi) and voltage dependence of surface states (Nss) profile were obtained from the C-V measurements. The results reveal that doping ZnO with IIIA (Al or In) elements to fabricate n-ZnO/p-Si heterojunction can result in high performance diode characteristics.",what keywords ?,ZnO Thin film,silicon,False,False
"Current approaches to RDF graph indexing suffer from weak data locality, i.e., information regarding a piece of data appears in multiple locations, spanning multiple data structures. Weak data locality negatively impacts storage and query processing costs. Towards stronger data locality, we propose a Three-way Triple Tree (TripleT) secondary memory indexing technique to facilitate flexible and efficient join evaluation on RDF data. The novelty of TripleT is that the index is built over the atoms occurring in the data set, rather than at a coarser granularity, such as whole triples occurring in the data set; and, the atoms are indexed regardless of the roles (i.e., subjects, predicates, or objects) they play in the triples of the data set. We show through extensive empirical evaluation that TripleT exhibits multiple orders of magnitude improvement over the state-of-the-art, in terms of both storage and query processing costs.",what keywords ?,Query processing costs,"data locality,",False,False
"This paper presents a new RF MEMS tunable capacitor based on the zipper principle and with interdigitated RF and actuation electrodes. The electrode configuration prevents dielectric charging under high actuation voltages. It also increases the capacitance ratio and the tunable analog range. The effect of the residual stress on the capacitance tunability is also investigated. Two devices with different interdigital RF and actuation electrodes are fabricated on an alumina substrate and result in a capacitance ratio around 3.0 (Cmin = 70?90 fF, Cmax = 240?270 fF) and with a Q > 100 at 3 GHz. This design can be used in wideband tunable filters and matching networks.",what keywords ?,Interdigitated RF,capacitor based on the zipper principle and with interdigitated rf and actuation electrodes.,False,True
"Abstract Objective: Paclitaxel (PTX)-loaded polymer (Poly(lactic-co-glycolic acid), PLGA)-based nanoformulation was developed with the objective of formulating cremophor EL-free nanoformulation intended for intravenous use. Significance: The polymeric PTX nanoparticles free from the cremophor EL will help in eliminating the shortcomings of the existing delivery system as cremophor EL causes serious allergic reactions to the subjects after intravenous use. Methods and results: Paclitaxel-loaded nanoparticles were formulated by nanoprecipitation method. The diminutive nanoparticles (143.2 nm) with uniform size throughout (polydispersity index, 0.115) and high entrapment efficiency (95.34%) were obtained by employing the Box–Behnken design for the optimization of the formulation with the aid of desirability approach-based numerical optimization technique. Optimized levels for each factor viz. polymer concentration (X1), amount of organic solvent (X2), and surfactant concentration (X3) were 0.23%, 5 ml %, and 1.13%, respectively. The results of the hemocompatibility studies confirmed the safety of PLGA-based nanoparticles for intravenous administration. Pharmacokinetic evaluations confirmed the longer retention of PTX in systemic circulation. Conclusion: In a nutshell, the developed polymeric nanoparticle formulation of PTX precludes the inadequacy of existing PTX formulation and can be considered as superior alternative carrier system of the same.",what keywords ?,Paclitaxel,,False,False
"In recent years, the development of recommender systems has attracted increased interest in several domains, especially in e-learning. Massive Open Online Courses have brought a revolution. However, deficiency in support and personalization in this context drive learners to lose their motivation and leave the learning process. To overcome this problem we focus on adapting learning activities to learners' needs using a recommender system.This paper attempts to provide an introduction to different recommender systems for e-learning settings, as well as to present our proposed recommender system for massive learning activities in order to provide learners with the suitable learning activities to follow the learning process and maintain their motivation. We propose a hybrid knowledge-based recommender system based on ontology for recommendation of e-learning activities to learners in the context of MOOCs. In the proposed recommendation approach, ontology is used to model and represent the knowledge about the domain model, learners and learning activities.",what keywords ?,recommender system,e - learning.,False,False
"Abstract Objective: Paclitaxel (PTX)-loaded polymer (Poly(lactic-co-glycolic acid), PLGA)-based nanoformulation was developed with the objective of formulating cremophor EL-free nanoformulation intended for intravenous use. Significance: The polymeric PTX nanoparticles free from the cremophor EL will help in eliminating the shortcomings of the existing delivery system as cremophor EL causes serious allergic reactions to the subjects after intravenous use. Methods and results: Paclitaxel-loaded nanoparticles were formulated by nanoprecipitation method. The diminutive nanoparticles (143.2 nm) with uniform size throughout (polydispersity index, 0.115) and high entrapment efficiency (95.34%) were obtained by employing the Box–Behnken design for the optimization of the formulation with the aid of desirability approach-based numerical optimization technique. Optimized levels for each factor viz. polymer concentration (X1), amount of organic solvent (X2), and surfactant concentration (X3) were 0.23%, 5 ml %, and 1.13%, respectively. The results of the hemocompatibility studies confirmed the safety of PLGA-based nanoparticles for intravenous administration. Pharmacokinetic evaluations confirmed the longer retention of PTX in systemic circulation. Conclusion: In a nutshell, the developed polymeric nanoparticle formulation of PTX precludes the inadequacy of existing PTX formulation and can be considered as superior alternative carrier system of the same.",what keywords ?,Nanoparticles,,False,False
"Bladder cancer (BC) is a very common cancer. Nonmuscle-invasive bladder cancer (NMIBC) is the most common type of bladder cancer. After postoperative tumor resection, chemotherapy intravesical instillation is recommended as a standard treatment to significantly reduce recurrences. Nanomedicine-mediated delivery of a chemotherapeutic agent targeting cancer could provide a solution to obtain longer residence time and high bioavailability of an anticancer drug. The approach described here provides a nanomedicine with sustained and prolonged delivery of paclitaxel and enhanced therapy of intravesical bladder cancer, which is paclitaxel/chitosan (PTX/CS) nanosupensions (NSs). The positively charged PTX/CS NSs exhibited a rod-shaped morphology with a mean diameter about 200 nm. They have good dispersivity in water without any protective agents, and the positively charged properties make them easy to be adsorbed on the inner mucosa of the bladder through electrostatic adsorption. PTX/CS NSs also had a high drug loading capacity and can maintain sustained release of paclitaxel which could be prolonged over 10 days. Cell experiments in vitro demonstrated that PTX/CS NSs had good biocompatibility and effective bladder cancer cell proliferation inhibition. The significant anticancer efficacy against intravesical bladder cancer was verified by an in situ bladder cancer model. The paclitaxel/chitosan nanosupensions could provide sustained delivery of chemotherapeutic agents with significant anticancer efficacy against intravesical bladder cancer.",what keywords ?,Chitosan,bladder cancer,False,False
"In this study, we synthesized hierarchical CuO nanoleaves in large-quantity via the hydrothermal method. We employed different techniques to characterize the morphological, structural, optical properties of the as-prepared hierarchical CuO nanoleaves sample. An electrochemical based nonenzymatic glucose biosensor was fabricated using engineered hierarchical CuO nanoleaves. The electrochemical behavior of fabricated biosensor towards glucose was analyzed with cyclic voltammetry (CV) and amperometry (i–t) techniques. Owing to the high electroactive surface area, hierarchical CuO nanoleaves based nonenzymatic biosensor electrode shows enhanced electrochemical catalytic behavior for glucose electro-oxidation in 100 mM sodium hydroxide (NaOH) electrolyte. The nonenzymatic biosensor displays a high sensitivity (1467.32 μ A/(mM cm 2 )), linear range (0.005–5.89 mM), and detection limit of 12 nM (S/N = 3). Moreover, biosensor displayed good selectivity, reproducibility, repeatability, and stability at room temperature over three-week storage period. Further, as-fabricated nonenzymatic glucose biosensors were employed for practical applications in human serum sample measurements. The obtained data were compared to the commercial biosensor, which demonstrates the practical usability of nonenzymatic glucose biosensors in real sample analysis.",what keywords ?,CuO nanoleaves,,False,False
"Background: Paclitaxel (PTX) is one of the most important and effective anticancer drugs for the treatment of human cancer. However, its low solubility and severe adverse effects limited clinical use. To overcome this limitation, nanotechnology has been used to overcome tumors due to its excellent antimicrobial activity. Objective: This study was to demonstrate the anticancer properties of functionalization silver nanoparticles loaded with paclitaxel (Ag@PTX) induced A549 cells apoptosis through ROS-mediated signaling pathways. Methods: The Ag@PTX nanoparticles were charged with a zeta potential of about -17 mv and characterized around 2 nm with a narrow size distribution. Results: Ag@PTX significantly decreased the viability of A549 cells and possessed selectivity between cancer and normal cells. Ag@PTX induced A549 cells apoptosis was confirmed by nuclear condensation, DNA fragmentation, and activation of caspase-3. Furthermore, Ag@PTX enhanced the anti-cancer activity of A549 cells through ROS-mediated p53 and AKT signalling pathways. Finally, in a xenograft nude mice model, Ag@PTX suppressed the growth of tumors. Conclusion: Our findings suggest that Ag@PTX may be a candidate as a chemopreventive agent and could be a highly efficient way to achieve anticancer synergism for human cancers.",what keywords ?,Apoptosis,,False,False
"Choosing a higher education course at university is not an easy task for students. A wide range of courses are offered by the individual universities whose delivery mode and entry requirements differ. A personalized recommendation system can be an effective way of suggesting the relevant courses to the prospective students. This paper introduces a novel approach that personalizes course recommendations that will match the individual needs of users. The proposed approach developed a framework of an ontology-based hybrid-filtering system called the ontology-based personalized course recommendation (OPCR). This approach aims to integrate the information from multiple sources based on the hierarchical ontology similarity with a view to enhancing the efficiency and the user satisfaction and to provide students with appropriate recommendations. The OPCR combines collaborative-based filtering with content-based filtering. It also considers familiar related concepts that are evident in the profiles of both the student and the course, determining the similarity between them. Furthermore, OPCR uses an ontology mapping technique, recommending jobs that will be available following the completion of each course. This method can enable students to gain a comprehensive knowledge of courses based on their relevance, using dynamic ontology mapping to link the course profiles and student profiles with job profiles. Results show that a filtering algorithm that uses hierarchically related concepts produces better outcomes compared to a filtering method that considers only keyword similarity. In addition, the quality of the recommendations is improved when the ontology similarity between the items’ and the users’ profiles were utilized. This approach, using a dynamic ontology mapping, is flexible and can be adapted to different domains. The proposed framework can be used to filter the items for both postgraduate courses and items from other domains.",what keywords ?,ontology,user satisfaction,False,False
"Abstract Despite rapid progress, most of the educational technologies today lack a strong instructional design knowledge basis leading to questionable quality of instruction. In addition, a major challenge is to customize these educational technologies for a wide range of customizable instructional designs. Ontologies are one of the pertinent mechanisms to represent instructional design in the literature. However, existing approaches do not support modeling of flexible instructional designs. To address this problem, in this paper, we propose an ontology based framework for systematic modeling of different aspects of instructional design knowledge based on domain patterns. As part of the framework, we present ontologies for modeling goals , instructional processes and instructional material . We demonstrate the ontology framework by presenting instances of the ontology for the large scale case study of adult literacy in India (287 million learners spread across 22 Indian Languages), which requires creation of hundreds of similar but varied e Learning Systems based on flexible instructional designs. The implemented framework is available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" ext-link-type=""uri"" xlink:href=""http://rice.iiit.ac.in"">http://rice.iiit.ac.in</jats:ext-link> and is transferred to National Literacy Mission Authority of Government of India . The proposed framework could be potentially used for modeling instructional design knowledge for school education, vocational skills and beyond.",what keywords ?,goals,,False,False
"Herein, a novel electrochemical glucose biosensor based on glucose oxidase (GOx) immobilized on a surface containing platinum nanoparticles (PtNPs) electrodeposited on poly(Azure A) (PAA) previously electropolymerized on activated screen-printed carbon electrodes (GOx-PtNPs-PAA-aSPCEs) is reported. The resulting electrochemical biosensor was validated towards glucose oxidation in real samples and further electrochemical measurement associated with the generated H2O2. The electrochemical biosensor showed an excellent sensitivity (42.7 μA mM−1 cm−2), limit of detection (7.6 μM), linear range (20 μM–2.3 mM), and good selectivity towards glucose determination. Furthermore, and most importantly, the detection of glucose was performed at a low potential (0.2 V vs. Ag). The high performance of the electrochemical biosensor was explained through surface exploration using field emission SEM, XPS, and impedance measurements. The electrochemical biosensor was successfully applied to glucose quantification in several real samples (commercial juices and a plant cell culture medium), exhibiting a high accuracy when compared with a classical spectrophotometric method. This electrochemical biosensor can be easily prepared and opens up a good alternative in the development of new sensitive glucose sensors.",what keywords ?,poly(Azure A),glucose oxidase,False,False
"High-performance p-type oxide thin film transistors (TFTs) have great potential for many semiconductor applications. However, these devices typically suffer from low hole mobility and high off-state currents. We fabricated p-type TFTs with a phase-pure polycrystalline Cu2O semiconductor channel grown by atomic layer deposition (ALD). The TFT switching characteristics were improved by applying a thin ALD Al2O3 passivation layer on the Cu2O channel, followed by vacuum annealing at 300 °C. Detailed characterization by transmission electron microscopy-energy dispersive X-ray analysis and X-ray photoelectron spectroscopy shows that the surface of Cu2O is reduced following Al2O3 deposition and indicates the formation of a 1-2 nm thick CuAlO2 interfacial layer. This, together with field-effect passivation caused by the high negative fixed charge of the ALD Al2O3, leads to an improvement in the TFT performance by reducing the density of deep trap states as well as by reducing the accumulation of electrons in the semiconducting layer in the device off-state.",what keywords ?,Passivation,transistors,False,False
"<jats:p>The application of thinner cadmium sulfide (CdS) window layer is a feasible approach to improve the performance of cadmium telluride (CdTe) thin film solar cells. However, the reduction of compactness and continuity of thinner CdS always deteriorates the device performance. In this work, transparent Al2O3 films with different thicknesses, deposited by using atomic layer deposition (ALD), were utilized as buffer layers between the front electrode transparent conductive oxide (TCO) and CdS layers to solve this problem, and then, thin-film solar cells with a structure of TCO/Al2O3/CdS/CdTe/BC/Ni were fabricated. The characteristics of the ALD-Al2O3 films were studied by UV–visible transmittance spectrum, Raman spectroscopy, and atomic force microscopy (AFM). The light and dark J–V performances of solar cells were also measured by specific instrumentations. The transmittance measurement conducted on the TCO/Al2O3 films verified that the transmittance of TCO/Al2O3 were comparable to that of single TCO layer, meaning that no extra absorption loss occurred when Al2O3 buffer layers were introduced into cells. Furthermore, due to the advantages of the ALD method, the ALD-Al2O3 buffer layers formed an extremely continuous and uniform coverage on the substrates to effectively fill and block the tiny leakage channels in CdS/CdTe polycrystalline films and improve the characteristics of the interface between TCO and CdS. However, as the thickness of alumina increased, the negative effects of cells were gradually exposed, especially the increase of the series resistance (Rs) and the more serious “roll-over” phenomenon. Finally, the cell conversion efficiency (η) of more than 13.0% accompanied by optimized uniformity performances was successfully achieved corresponding to the 10 nm thick ALD-Al2O3 thin film.</jats:p>",what keywords ?,Al2O3,atomic layer deposition,False,False
"A novel and highly sensitive nonenzymatic glucose biosensor was developed by nucleating colloidal silver nanoparticles (AgNPs) on MoS2. The facile fabrication method, high reproducibility (97.5%) and stability indicates a promising capability for large-scale manufacturing. Additionally, the excellent sensitivity (9044.6 μA·mM−1·cm−2), low detection limit (0.03 μM), appropriate linear range of 0.1–1000 μM, and high selectivity suggests that this biosensor has a great potential to be applied for noninvasive glucose detection in human body fluids, such as sweat and saliva.",what keywords ?,nonenzymatic,glucose biosensor was developed by nucleating colloidal silver nanoparticles,False,False
SnO2 nanowire gas sensors have been fabricated on Cd−Au comb-shaped interdigitating electrodes using thermal evaporation of the mixed powders of SnO2 and active carbon. The self-assembly grown sensors have excellent performance in sensor response to hydrogen concentration in the range of 10 to 1000 ppm. This high response is attributed to the large portion of undercoordinated atoms on the surface of the SnO2 nanowires. The influence of the Debye length of the nanowires and the gap between electrodes in the gas sensor response is examined and discussed.,what keywords ?,Nanowires,,False,False
"A highly sensitive strain sensor consisting of two cascaded fiber ring resonators based on the Vernier effect is proposed. Each fiber ring resonator, composed of an input optical coupler, an output optical coupler, and a polarization controller, has a comb-like transmission spectrum with peaks at its resonance wavelengths. As a result, the Vernier effect will be generated, due to the displacement of the two transmission spectra. Using this technique, strain measurements can be achieved by measuring the free spectral range of the cascaded fiber ring resonators. The experimental results show that the sensing setup can operate in large strain range with a sensitivity of 0.0129 nm-1/με. The new generation of Vernier strain sensor can also be useful for micro-displacement measurement.",what keywords ?,Strain,strain sensor consisting of two cascaded fiber ring resonators based on the vernier effect,False,True
"A new variant of the classic pulsed laser deposition (PLD) process is introduced as a room-temperature dry process for the growth and stoichiometry control of hybrid perovskite films through the use of nonstoichiometric single target ablation and off-axis growth. Mixed halide hybrid perovskite films nominally represented by CH3NH3PbI3–xAx (A = Cl or F) are also grown and are shown to reveal interesting trends in the optical properties and photoresponse. Growth of good quality lead-free CH3NH3SnI3 films is also demonstrated, and the corresponding optical properties are presented. Finally, perovskite solar cells fabricated at room temperature (which makes the process adaptable to flexible substrates) are shown to yield a conversion efficiency of about 7.7%.",what keywords ?,Pulsed Laser Deposition,laser deposition,False,False
"Compositional engineering of recently arising methylammonium (MA) lead (Pb) halide based perovskites is an essential approach for finding better perovskite compositions to resolve still remaining issues of toxic Pb, long-term instability, etc. In this work, we carried out crystallographic, morphological, optical, and photovoltaic characterization of compositional MASn0.6Pb0.4I3-xBrx by gradually introducing bromine (Br) into parental Pb-Sn binary perovskite (MASn0.6Pb0.4I3) to elucidate its function in Sn-rich (Sn:Pb = 6:4) perovskites. We found significant advances in crystallinity and dense coverage of the perovskite films by inserting the Br into Sn-rich perovskite lattice. Furthermore, light-intensity-dependent open circuit voltage (Voc) measurement revealed much suppressed trap-assisted recombination for a proper Br-added (x = 0.4) device. These contributed to attaining the unprecedented power conversion efficiency of 12.1% and Voc of 0.78 V, which are, to the best of our knowledge, the highest performance in the Sn-rich (≥60%) perovskite solar cells reported so far. In addition, impressive enhancement of photocurrent-output stability and little hysteresis were found, which paves the way for the development of environmentally benign (Pb reduction), stable monolithic tandem cells using the developed low band gap (1.24-1.26 eV) MASn0.6Pb0.4I3-xBrx with suggested composition (x = 0.2-0.4).",what keywords ?,Perovskites,,False,False
"Graphene (Gr) has been widely used as a transparent electrode material for photodetectors because of its high conductivity and high transmittance in recent years. However, the current low-efficiency manipulation of Gr has hindered the arraying and practical use of such detectors. We invented a multistep method of accurately tailoring graphene into interdigital electrodes for fabricating a sensitive, stable deep-ultraviolet photodetector based on Zn-doped Ga2O3 films. The fabricated photodetector exhibits a series of excellent performance, including extremely low dark current (∼10-11 A), an ultrahigh photo-to-dark ratio (>105), satisfactory responsivity (1.05 A/W), and excellent selectivity for the deep-ultraviolet band, compared to those with ordinary metal electrodes. The raise of photocurrent and responsivity is attributed to the increase of incident photons through Gr and separated carriers caused by the built-in electric field formed at the interface of Gr and Ga2O3:Zn films. The proposed ideas and methods of tailoring Gr can not only improve the performance of devices but more importantly contribute to the practical development of graphene.",what keywords ?,Electrodes,graphene,False,False
"In recent years, the development of electronic skin and smart wearable body sensors has put forward high requirements for flexible pressure sensors with high sensitivity and large linear measuring range. However it turns out to be difficult to increase both of them simultaneously. In this paper, a flexible capacitive pressure sensor based on porous carbon conductive paste-PDMS composite is reported, the sensitivity and the linear measuring range of which were developed using multiple methods including adjusting the stiffness of the dielectric layer material, fabricating micro-structure and increasing dielectric permittivity of dielectric layer. The capacitive pressure sensor reported here has a relatively high sensitivity of 1.1 kPa-1 and a large linear measuring range of 10 kPa, making the product of the sensitivity and linear measuring range is 11, which is higher than that of the most reported capacitive pressure sensor to our best knowledge. The sensor has a detection of limit of 4 Pa, response time of 60 ms and great stability. Some potential applications of the sensor were demonstrated such as arterial pulse wave measuring and breathe measuring, which shows a promising candidate for wearable biomedical devices. In addition, a pressure sensor array based on the material was also fabricated and it could identify objects in the shape of different letters clearly, which shows a promising application in the future electronic skins.",what keywords ?,Flexible,pressure sensors,False,False
"Bladder cancer (BC) is a very common cancer. Nonmuscle-invasive bladder cancer (NMIBC) is the most common type of bladder cancer. After postoperative tumor resection, chemotherapy intravesical instillation is recommended as a standard treatment to significantly reduce recurrences. Nanomedicine-mediated delivery of a chemotherapeutic agent targeting cancer could provide a solution to obtain longer residence time and high bioavailability of an anticancer drug. The approach described here provides a nanomedicine with sustained and prolonged delivery of paclitaxel and enhanced therapy of intravesical bladder cancer, which is paclitaxel/chitosan (PTX/CS) nanosupensions (NSs). The positively charged PTX/CS NSs exhibited a rod-shaped morphology with a mean diameter about 200 nm. They have good dispersivity in water without any protective agents, and the positively charged properties make them easy to be adsorbed on the inner mucosa of the bladder through electrostatic adsorption. PTX/CS NSs also had a high drug loading capacity and can maintain sustained release of paclitaxel which could be prolonged over 10 days. Cell experiments in vitro demonstrated that PTX/CS NSs had good biocompatibility and effective bladder cancer cell proliferation inhibition. The significant anticancer efficacy against intravesical bladder cancer was verified by an in situ bladder cancer model. The paclitaxel/chitosan nanosupensions could provide sustained delivery of chemotherapeutic agents with significant anticancer efficacy against intravesical bladder cancer.",what keywords ?,nanosupension,bladder cancer,False,False
"High-performance p-type oxide thin film transistors (TFTs) have great potential for many semiconductor applications. However, these devices typically suffer from low hole mobility and high off-state currents. We fabricated p-type TFTs with a phase-pure polycrystalline Cu2O semiconductor channel grown by atomic layer deposition (ALD). The TFT switching characteristics were improved by applying a thin ALD Al2O3 passivation layer on the Cu2O channel, followed by vacuum annealing at 300 °C. Detailed characterization by transmission electron microscopy-energy dispersive X-ray analysis and X-ray photoelectron spectroscopy shows that the surface of Cu2O is reduced following Al2O3 deposition and indicates the formation of a 1-2 nm thick CuAlO2 interfacial layer. This, together with field-effect passivation caused by the high negative fixed charge of the ALD Al2O3, leads to an improvement in the TFT performance by reducing the density of deep trap states as well as by reducing the accumulation of electrons in the semiconducting layer in the device off-state.",what keywords ?,Atomic layer deposition,transistors,False,False
"Ni-MOF (metal-organic framework)/Ni/NiO/carbon frame nanocomposite was formed by combing Ni and NiO nanoparticles and a C frame with Ni-MOF using an efficient one-step calcination method. The morphology and structure of Ni-MOF/Ni/NiO/C nanocomposite were characterized by transmission electron microscopy (TEM), X-ray photoelectron spectroscopy (XPS), X-ray diffraction (XRD), and energy disperse spectroscopy (EDS) mapping. Ni-MOF/Ni/NiO/C nanocomposites were immobilized onto glassy carbon electrodes (GCEs) with Nafion film to construct high-performance nonenzymatic glucose and H2O2 electrochemical sensors. Cyclic voltammetric (CV) study showed Ni-MOF/Ni/NiO/C nanocomposite displayed better electrocatalytic activity toward glucose oxidation as compared to Ni-MOF. Amperometric study indicated the glucose sensor displayed high performance, offering a low detection limit (0.8 μM), a high sensitivity of 367.45 mA M-1 cm-2, and a wide linear range (from 4 to 5664 μM). Importantly, good reproducibility, long-time stability, and excellent selectivity were obtained within the as-fabricated glucose sensor. Furthermore, the constructed high-performance sensor was utilized to monitor the glucose levels in human serum, and satisfactory results were obtained. It demonstrated the Ni-MOF/Ni/NiO/C nanocomposite can be used as a good electrochemical sensing material in practical biological applications.",what keywords ?,Human serum,,False,False
"Most Semantic Web applications rely on querying graphs, typically by using SPARQL with a triple store. Increasingly, applications also analyze properties of the graph structure to compute statistical inferences. The current Semantic Web infrastructure, however, does not efficiently support such operations. This forces developers to extract the relevant data for external statistical post-processing. In this paper we propose to rethink query execution in a triple store as a highly parallelized asynchronous graph exploration on an active index data structure. This approach also allows to integrate SPARQL-querying with the sampling of graph properties. To evaluate this architecture we implemented Random Walk TripleRush, which is built on a distributed graph processing system. Our evaluations show that this architecture enables both competitive graph querying, as well as the ability to execute various types of random walks with restarts that sample interesting graph properties. Thanks to the asynchronous architecture, first results are sometimes returned in a fraction of the full execution time. We also evaluate the scalability and show that the architecture supports fast query-times on a dataset with more than a billion triples.",what Algorithm ?,Random walk,triple store.,False,False
"Adaptive support weight (ASW) methods represent the state of the art in local stereo matching, while the bilateral filter-based ASW method achieves outstanding performance. However, this method fails to resolve the ambiguity induced by nearby pixels at different disparities but with similar colors. In this paper, we introduce a novel trilateral filter (TF)-based ASW method that remedies such ambiguities by considering the possible disparity discontinuities through color discontinuity boundaries, i.e., the boundary strength between two pixels, which is measured by a local energy model. We also present a recursive TF-based ASW method whose computational complexity is O(N) for the cost aggregation step, and O(NLog2(N)) for boundary detection, where N denotes the input image size. This complexity is thus independent of the support window size. The recursive TF-based method is a nonlocal cost aggregation strategy. The experimental evaluation on the Middlebury benchmark shows that the proposed method, whose average error rate is 4.95%, outperforms other local methods in terms of accuracy. Equally, the average runtime of the proposed TF-based cost aggregation is roughly 260 ms on a 3.4-GHz Inter Core i7 CPU, which is comparable with state-of-the-art efficiency.",what Algorithm ?,Trilateral,3. 4 - ghz,False,False
"Binocular stereo matching is one of the most important algorithms in the field of computer vision. Adaptive support-weight approaches, the current state-of-the-art local methods, produce results comparable to those generated by global methods. However, excessive time consumption is the main problem of these algorithms since the computational complexity is proportionally related to the support window size. In this paper, we present a novel cost aggregation method inspired by domain transformation, a recently proposed dimensionality reduction technique. This transformation enables the aggregation of 2-D cost data to be performed using a sequence of 1-D filters, which lowers computation and memory costs compared to conventional 2-D filters. Experiments show that the proposed method outperforms the state-of-the-art local methods in terms of computational performance, since its computational complexity is independent of the input parameters. Furthermore, according to the experimental results with the Middlebury dataset and real-world images, our algorithm is currently one of the most accurate and efficient local algorithms.",what Algorithm ?,Domain transformation,local,False,False
"Several computer programs are available for detecting copy number variants (CNVs) using genome-wide SNP arrays. We evaluated the performance of four CNV detection software suites—Birdsuite, Partek, HelixTree, and PennCNV-Affy—in the identification of both rare and common CNVs. Each program's performance was assessed in two ways. The first was its recovery rate, i.e., its ability to call 893 CNVs previously identified in eight HapMap samples by paired-end sequencing of whole-genome fosmid clones, and 51,440 CNVs identified by array Comparative Genome Hybridization (aCGH) followed by validation procedures, in 90 HapMap CEU samples. The second evaluation was program performance calling rare and common CNVs in the Bipolar Genome Study (BiGS) data set (1001 bipolar cases and 1033 controls, all of European ancestry) as measured by the Affymetrix SNP 6.0 array. Accuracy in calling rare CNVs was assessed by positive predictive value, based on the proportion of rare CNVs validated by quantitative real-time PCR (qPCR), while accuracy in calling common CNVs was assessed by false positive/false negative rates based on qPCR validation results from a subset of common CNVs. Birdsuite recovered the highest percentages of known HapMap CNVs containing >20 markers in two reference CNV datasets. The recovery rate increased with decreased CNV frequency. In the tested rare CNV data, Birdsuite and Partek had higher positive predictive values than the other software suites. In a test of three common CNVs in the BiGS dataset, Birdsuite's call was 98.8% consistent with qPCR quantification in one CNV region, but the other two regions showed an unacceptable degree of accuracy. We found relatively poor consistency between the two “gold standards,” the sequence data of Kidd et al., and aCGH data of Conrad et al. Algorithms for calling CNVs especially common ones need substantial improvement, and a “gold standard” for detection of CNVs remains to be established.",what Algorithm ?,HelixTree,,False,False
"The disparity estimation problem is commonly solved using graph cut (GC) methods, in which the disparity assignment problem is transformed to one of minimizing global energy function. Although such an approach yields an accurate disparity map, the computational cost is relatively high. Accordingly, this paper proposes a hierarchical bilateral disparity structure (HBDS) algorithm in which the efficiency of the GC method is improved without any loss in the disparity estimation performance by dividing all the disparity levels within the stereo image hierarchically into a series of bilateral disparity structures of increasing fineness. To address the well-known foreground fattening effect, a disparity refinement process is proposed comprising a fattening foreground region detection procedure followed by a disparity recovery process. The efficiency and accuracy of the HBDS-based GC algorithm are compared with those of the conventional GC method using benchmark stereo images selected from the Middlebury dataset. In addition, the general applicability of the proposed approach is demonstrated using several real-world stereo images.",what Algorithm ?,Hierarchical bilateral disparity structure (HBDS),global,False,False
"With the proliferation of the RDF data format, engines for RDF query processing are faced with very large graphs that contain hundreds of millions of RDF triples. This paper addresses the resulting scalability problems. Recent prior work along these lines has focused on indexing and other physical-design issues. The current paper focuses on join processing, as the fine-grained and schema-relaxed use of RDF often entails star- and chain-shaped join queries with many input streams from index scans. We present two contributions for scalable join processing. First, we develop very light-weight methods for sideways information passing between separate joins at query run-time, to provide highly effective filters on the input streams of joins. Second, we improve previously proposed algorithms for join-order optimization by more accurate selectivity estimations for very large RDF graphs. Experimental studies with several RDF datasets, including the UniProt collection, demonstrate the performance gains of our approach, outperforming the previously fastest systems by more than an order of magnitude.",what Has implementation ?,Join-order optimization,uniprot,False,False
"Query optimization in RDF Stores is a challenging problem as SPARQL queries typically contain many more joins than equivalent relational plans, and hence lead to a large join order search space. In such cases, cost-based query optimization often is not possible. One practical reason for this is that statistics typically are missing in web scale setting such as the Linked Open Datasets (LOD). The more profound reason is that due to the absence of schematic structure in RDF, join-hit ratio estimation requires complicated forms of correlated join statistics; and currently there are no methods to identify the relevant correlations beforehand. For this reason, the use of good heuristics is essential in SPARQL query optimization, even in the case that are partially used with cost-based statistics (i.e., hybrid query optimization). In this paper we describe a set of useful heuristics for SPARQL query optimizers. We present these in the context of a new Heuristic SPARQL Planner (HSP) that is capable of exploiting the syntactic and the structural variations of the triple patterns in a SPARQL query in order to choose an execution plan without the need of any cost model. For this, we define the variable graph and we show a reduction of the SPARQL query optimization problem to the maximum weight independent set problem. We implemented our planner on top of the MonetDB open source column-store and evaluated its effectiveness against the state-of-the-art RDF-3X engine as well as comparing the plan quality with a relational (SQL) equivalent of the benchmarks.",what Has implementation ?,RDF-3X,monetdb,False,False
"This paper presents the MPI parallelization of a new algorithm—DPD-B thermostat—for molecular dynamics simulations. The presented results are using Martini Coarse Grained Water System. It should be taken into account that molecular dynamics simulations are time consuming. In some cases the running time varies from days to weeks and even months. Therefore, parallelization is one solution for reducing the execution time. The paper describes the new algorithm, the main characteristics of the MPI parallelization of the new algorithm, and the simulation performances.",what Has implementation ?,a new algorithm,dpd - b thermostat — for molecular dynamics simulations. the presented results are using martini coarse grained water system.,False,False
"Most Semantic Web applications rely on querying graphs, typically by using SPARQL with a triple store. Increasingly, applications also analyze properties of the graph structure to compute statistical inferences. The current Semantic Web infrastructure, however, does not efficiently support such operations. This forces developers to extract the relevant data for external statistical post-processing. In this paper we propose to rethink query execution in a triple store as a highly parallelized asynchronous graph exploration on an active index data structure. This approach also allows to integrate SPARQL-querying with the sampling of graph properties. To evaluate this architecture we implemented Random Walk TripleRush, which is built on a distributed graph processing system. Our evaluations show that this architecture enables both competitive graph querying, as well as the ability to execute various types of random walks with restarts that sample interesting graph properties. Thanks to the asynchronous architecture, first results are sometimes returned in a fraction of the full execution time. We also evaluate the scalability and show that the architecture supports fast query-times on a dataset with more than a billion triples.",what Has implementation ?,SPARQL,"random walk triplerush,",False,False
"Herein, we present an approach to create a hybrid between single-atom-dispersed silver and a carbon nitride polymer. Silver tricyanomethanide (AgTCM) is used as a reactive comonomer during templated carbon nitride synthesis to introduce both negative charges and silver atoms/ions to the system. The successful introduction of the extra electron density under the formation of a delocalized joint electronic system is proven by photoluminescence measurements, X-ray photoelectron spectroscopy investigations, and measurements of surface ζ-potential. At the same time, the principal structure of the carbon nitride network is not disturbed, as shown by solid-state nuclear magnetic resonance spectroscopy and electrochemical impedance spectroscopy analysis. The synthesis also results in an improvement of the visible light absorption and the development of higher surface area in the final products. The atom-dispersed AgTCM-doped carbon nitride shows an enhanced performance in the selective hydrogenation of alkynes in comparison with the performance of other conventional Ag-based materials prepared by spray deposition and impregnation-reduction methods, here exemplified with 1-hexyne.",what substrate ?,1-hexyne,carbon nitride polymer. silver tricyanomethanide ( agtcm ),False,False
"Palladium nanoparticles supported on a mesoporous graphitic carbon nitride, Pd@mpg-C3N4, has been developed as an effective, heterogeneous catalyst for the liquid-phase semihydrogenation of phenylacetylene under mild conditions (303 K, atmospheric H2). A total conversion was achieved with high selectivity of styrene (higher than 94%) within 85 minutes. Moreover, the spent catalyst can be easily recovered by filtration and then reused nine times without apparent lose of selectivity. The generality of Pd@mpg-C3N4 catalyst for partial hydrogenation of alkynes was also checked for terminal and internal alkynes with similar performance. The Pd@mpg-C3N4 catalyst was proven to be of industrial interest.",what substrate ?,phenylacetylene,"mesoporous graphitic carbon nitride, pd @ mpg - c3n4, has been developed as an effective, heterogeneous catalyst for the liquid - phase semihydrogenation of phenylacetylene",False,True
"Abstract Cleavage of C–O bonds in lignin can afford the renewable aryl sources for fine chemicals. However, the high bond energies of these C–O bonds, especially the 4-O-5-type diaryl ether C–O bonds (~314 kJ/mol) make the cleavage very challenging. Here, we report visible-light photoredox-catalyzed C–O bond cleavage of diaryl ethers by an acidolysis with an aryl carboxylic acid and a following one-pot hydrolysis. Two molecules of phenols are obtained from one molecule of diaryl ether at room temperature. The aryl carboxylic acid used for the acidolysis can be recovered. The key to success of the acidolysis is merging visible-light photoredox catalysis using an acridinium photocatalyst and Lewis acid catalysis using Cu(TMHD) 2 . Preliminary mechanistic studies indicate that the catalytic cycle occurs via a rare selective electrophilic attack of the generated aryl carboxylic radical on the electron-rich aryl ring of the diphenyl ether. This transformation is applied to a gram-scale reaction and the model of 4-O-5 lignin linkages.",what substrate ?,Diaryl ethers,lignin,False,False
"Gold nanoparticles on a number of supporting materials, including anatase TiO2 (TiO2-A, in 40 nm and 45 μm), rutile TiO2 (TiO2-R), ZrO2, Al2O3, SiO2 , and activated carbon, were evaluated for hydrodeoxygenation of guaiacol in 6.5 MPa initial H2 pressure at 300 °C. The presence of gold nanoparticles on the supports did not show distinguishable performance compared to that of the supports alone in the conversion level and in the product distribution, except for that on a TiO2-A-40 nm. The lack of marked catalytic activity on supports other than TiO2-A-40 nm suggests that Au nanoparticles are not catalytically active on these supports. Most strikingly, the gold nanoparticles on the least-active TiO2-A-40 nm support stood out as the best catalyst exhibiting high activity with excellent stability and remarkable selectivity to phenolics from guaiacol hydrodeoxygenation. The conversion of guaiacol (∼43.1%) over gold on the TiO2-A-40 nm was about 33 times that (1.3%) over the TiO2-A-40 nm alone. The selectivity o...",what substrate ?,guaiacol,guaiacol,True,True
"Catalytic bio‐oil upgrading to produce renewable fuels has attracted increasing attention in response to the decreasing oil reserves and the increased fuel demand worldwide. Herein, the catalytic hydrodeoxygenation (HDO) of guaiacol with carbon‐supported non‐sulfided metal catalysts was investigated. Catalytic tests were performed at 4.0 MPa and temperatures ranging from 623 to 673 K. Both Ru/C and Mo/C catalysts showed promising catalytic performance in HDO. The selectivity to benzene was 69.5 and 83.5 % at 653 K over Ru/C and 10Mo/C catalysts, respectively. Phenol, with a selectivity as high as 76.5 %, was observed mainly on 1Mo/C. However, the reaction pathway over both catalysts is different. Over the Ru/C catalyst, the OCH3 bond was cleaved to form the primary intermediate catechol, whereas only traces of catechol were detected over Mo/C catalysts. In addition, two types of active sites were detected over Mo samples after reduction in H2 at 973 K. Catalytic studies showed that the demethoxylation of guaiacol is performed over residual MoOx sites with high selectivity to phenol whereas the consecutive HDO of phenol is performed over molybdenum carbide species, which is widely available only on the 10Mo/C sample. Different deactivation patterns were also observed over Ru/C and Mo/C catalysts.",what substrate ?,guaiacol,guaiacol,True,True
<p>Silica supported and unsupported PdAu single atom alloys (SAAs) were investigated for the selective hydrogenation of 1-hexyne to hexenes under mild conditions.</p>,what substrate ?,1-hexyne,,False,False
<jats:title>Abstract</jats:title><jats:p>This paper presents the results of study on titanium dioxide thin films prepared by atomic layer deposition method on a silicon substrate. The changes of surface morphology have been observed in topographic images performed with the atomic force microscope (AFM) and scanning electron microscope (SEM). Obtained roughness parameters have been calculated with XEI Park Systems software. Qualitative studies of chemical composition were also performed using the energy dispersive spectrometer (EDS). The structure of titanium dioxide was investigated by X-ray crystallography. A variety of crystalline TiO<jats:sub>2</jats:sub>was also confirmed by using the Raman spectrometer. The optical reflection spectra have been measured with UV-Vis spectrophotometry.</jats:p>,what substrate ?,Silicon,titanium dioxide,False,False
"Clustered graph visualization techniques are an easy to understand way of hiding complex parts of a visualized graph when they are not needed by the user. When visualizing RDF, there are several situations where such clusters are defined in a very natural way. Using this techniques, we can give the user optional access to some detailed information without unnecessarily occupying space in the basic view of the data. This paper describes algorithms for clustered visualization used in the Trisolda RDF visualizer. Most notable is the newly added clustered navigation technique.",what System ?,Trisolda ,trisolda rdf,False,True
"In the past decade, much effort has been put into the visual representation of ontologies. However, present visualization strategies are not equipped to handle complex ontologies with many relations, leading to visual clutter and inefficient use of space. In this paper, we propose GLOW, a method for ontology visualization based on Hierarchical Edge Bundles. Hierarchical Edge Bundles is a new visually attractive technique for displaying relations in hierarchical data, such as concept structures formed by 'subclass-of' and 'type-of' relations. We have developed a visualization library based on OWL API, as well as a plug-in for Protégé, a well-known ontology editor. The displayed adjacency relations can be selected from an ontology using a set of common configurations, allowing for intuitive discovery of information. Our evaluation demonstrates that the GLOW visualization provides better visual clarity, and displays relations and complex ontologies better than the existing Protégé visualization plug-in Jambalaya.",what System ?,GLOW ,"glow,",True,True
"In an effort to optimize visualization and editing of OWL ontologies we have developed GrOWL: a browser and visual editor for OWL that accurately visualizes the underlying DL semantics of OWL ontologies while avoiding the difficulties of the verbose OWL syntax. In this paper, we discuss GrOWL visualization model and the essential visualization techniques implemented in GrOWL.",what System ?,GrOWL ,growl :,True,True
"Objectives. Electronic laboratory reporting (ELR) reduces the time between communicable disease diagnosis and case reporting to local health departments (LHDs). However, it also imposes burdens on public health agencies, such as increases in the number of unique and duplicate case reports. We assessed how ELR affects the timeliness and accuracy of case report processing within public health agencies. Methods. Using data from May–August 2010 and January–March 2012, we assessed timeliness by calculating the time between receiving a case at the LHD and reporting the case to the state (first stage of reporting) and between submitting the report to the state and submitting it to the Centers for Disease Control and Prevention (second stage of reporting). We assessed accuracy by calculating the proportion of cases returned to the LHD for changes or additional information. We compared timeliness and accuracy for ELR and non-ELR cases. Results. ELR was associated with decreases in case processing time (median = 40 days for ELR cases vs. 52 days for non-ELR cases in 2010; median = 20 days for ELR cases vs. 25 days for non-ELR cases in 2012; both p<0.001). ELR also allowed time to reduce the backlog of unreported cases. Finally, ELR was associated with higher case reporting accuracy (in 2010, 2% of ELR case reports vs. 8% of non-ELR case reports were returned; in 2012, 2% of ELR case reports vs. 6% of non-ELR case reports were returned; both p<0.001). Conclusion. The overall impact of increased ELR is more efficient case processing at both local and state levels.",what Epidemiological surveillance software ?,Electronic Laboratory Reporting,,False,False
"Our aim was to evaluate the results of automated surveillance of Lyme neuroborreliosis (LNB) in Denmark using the national microbiology database (MiBa), and to describe the epidemiology of laboratory-confirmed LNB at a national level. MiBa-based surveillance includes electronic transfer of laboratory results, in contrast to the statutory surveillance based on manually processed notifications. Antibody index (AI) testing is the recommend laboratory test to support the diagnosis of LNB in Denmark. In the period from 2010 to 2012, 217 clinical cases of LNB were notified to the statutory surveillance system, while 533 cases were reported AI positive by the MiBa system. Thirty-five unconfirmed cases (29 AI-negative and 6 not tested) were notified, but not captured by MiBa. Using MiBa, the number of reported cases was increased almost 2.5 times. Furthermore, the reporting was timelier (median lag time: 6 vs 58 days). Average annual incidence of AI-confirmed LNB in Denmark was 3.2/100,000 population and incidences stratified by municipality ranged from none to above 10/100,000. This is the first study reporting nationwide incidence of LNB using objective laboratory criteria. Laboratory-based surveillance with electronic data-transfer was more accurate, complete and timely compared to the surveillance based on manually processed notifications. We propose using AI test results for LNB surveillance instead of clinical reporting.",what Epidemiological surveillance software ?,MiBa,miba ),True,True
"INTRODUCTION: The 2019 coronavirus disease (COVID-19) is a major global health concern. Joint efforts for effective surveillance of COVID-19 require immediate transmission of reliable data. In this regard, a standardized and interoperable reporting framework is essential in a consistent and timely manner. Thus, this research aimed at to determine data requirements towards interoperability. MATERIALS AND METHODS: In this cross-sectional and descriptive study, a combination of literature study and expert consensus approach was used to design COVID-19 Minimum Data Set (MDS). A MDS checklist was extracted and validated. The definitive data elements of the MDS were determined by applying the Delphi technique. Then, the existing messaging and data standard templates (Health Level Seven-Clinical Document Architecture [HL7-CDA] and SNOMED-CT) were used to design the surveillance interoperable framework. RESULTS: The proposed MDS was divided into administrative and clinical sections with three and eight data classes and 29 and 40 data fields, respectively. Then, for each data field, structured data values along with SNOMED-CT codes were defined and structured according HL7-CDA standard. DISCUSSION AND CONCLUSION: The absence of effective and integrated system for COVID-19 surveillance can delay critical public health measures, leading to increased disease prevalence and mortality. The heterogeneity of reporting templates and lack of uniform data sets hamper the optimal information exchange among multiple systems. Thus, developing a unified and interoperable reporting framework is more effective to prompt reaction to the COVID-19 outbreak.",what Epidemiological surveillance software ?,SNOMED,covid - 19 minimum data set ( mds ). a mds checklist was extracted and validated. the definitive data elements of the mds were determined by applying the delphi,False,False
"Hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. The need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the Open Biodiversity Knowledge Management System (OBKMS). This paper presents OpenBiodiv: An OBKMS that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. It is presented as a Linked Open Dataset generated from scientific literature. OpenBiodiv encompasses data extracted from more than 5000 scholarly articles published by Pensoft and many more taxonomic treatments extracted by Plazi from journals of other publishers. The data from both sources are converted to Resource Description Framework (RDF) and integrated in a graph database using the OpenBiodiv-O ontology and an RDF version of the Global Biodiversity Information Facility (GBIF) taxonomic backbone. Through the application of semantic technologies, the project showcases the value of open publishing of Findable, Accessible, Interoperable, Reusable (FAIR) data towards the establishment of open science practices in the biodiversity domain.",what Domain ?,Biodiversity,biodiversity,True,True
"The Visual Notation for OWL Ontologies (VOWL) is a well-specified visual language for the user-oriented representation of ontologies. It defines graphical depictions for most elements of the Web Ontology Language (OWL) that are combined to a force-directed graph layout visualizing the ontology. In contrast to related work, VOWL aims for an intuitive and comprehensive representation that is also understandable to users less familiar with ontologies. This article presents VOWL in detail and describes its implementation in two different tools: ProtegeVOWL and WebVOWL. The first is a plugin for the ontology editor Protege, the second a standalone web application. Both tools demonstrate the applicability of VOWL by means of various ontologies. In addition, the results of three user studies that evaluate the comprehensibility and usability of VOWL are summarized. They are complemented by findings from an interview with experienced ontology users and from testing the visual scope and completeness of VOWL with a benchmark ontology. The evaluations helped to improve VOWL and confirm that it produces comparatively intuitive and comprehensible ontology visualizations.",what Domain ?,ontology,owl,False,False
"Recently, the amount of semantic data available in the Web has increased dramatically. The potential of this vast amount of data is enormous but in most cases it is difficult for users to explore and use this data, especially for those without experience with Semantic Web technologies. Applying information visualization techniques to the Semantic Web helps users to easily explore large amounts of data and interact with them. In this article we devise a formal Linked Data Visualization Model (LDVM), which allows to dynamically connect data with visualizations. We report about our implementation of the LDVM comprising a library of generic visualizations that enable both users and data analysts to get an overview on, visualize and explore the Data Web and perform detailed analyzes on Linked Data.",what Domain ?,generic,"web has increased dramatically. the potential of this vast amount of data is enormous but in most cases it is difficult for users to explore and use this data, especially for those without experience with semantic",False,False
"Considering recent progress in NLP, deep learning techniques and biomedical language models there is a pressing need to generate annotated resources and comparable evaluation scenarios that enable the development of advanced biomedical relation extraction systems that extract interactions between drugs/chemical entities and genes, proteins or miRNAs. Building on the results and experience of the CHEMDNER, CHEMDNER patents and ChemProt tracks, we have posed the DrugProt track at BioCreative VII. The DrugProt track focused on the evaluation of automatic systems able to extract 13 different types of drug-genes/protein relations of importance to understand gene regulatory and pharmacological mechanisms. The DrugProt track addressed regulatory associations (direct/indirect, activator/inhibitor relations), certain types of binding associations (antagonist and agonist relations) as well as metabolic associations (substrate or product relations). To promote development of novel tools and offer a comparative evaluation scenario we have released 61,775 manually annotated gene mentions, 65,561 chemical and drug mentions and a total of 24,526 relationships manually labeled by domain experts. A total of 30 teams submitted results for the DrugProt main track, while 9 teams submitted results for the large-scale text mining subtrack that required processing of over 2,3 million records. Teams obtained very competitive results, with predictions reaching fmeasures of over 0.92 for some relation types (antagonist) and fmeasures across all relation types close to 0.8. INTRODUCTION Among the most relevant biological and pharmacological relation types are those that involve (a) chemical compounds and drugs as well as (b) gene products including genes, proteins, miRNAs. A variety of associations between chemicals and genes/proteins are described in the biomedical literature, and there is a growing interest in facilitating a more systematic extraction of these relations from the literature, either for manual database curation initiatives or to generate large knowledge graphs of importance for drug discovery, drug repurposing, building regulatory or interaction networks or to characterize off-target interactions of drugs that might be of importance to understand better adverse drug reactions. At BioCreative VI, the ChemProt track tried to promote the development of novel systems between chemicals and genes for groups of biologically related association types (ChemProt track relation groups or CPRs). Although the obtained results did have a considerable impact in the development and evaluation of new biomedical relation extraction systems, a limitation of grouping more specific relation types into broader groups was the difficulty to directly exploit the results for database curation efforts and biomedical knowledge graph mining application scenarios. The considerable interest in the integration of chemical and biomedical data for drug-discovery purposes, together with the ongoing curation of relationships between biological and chemical entities from scientific publications and patents due to the recent COVID-19 pandemic, motivated the DrugProt track of BioCreative VII, which proposed using more granular relation types. In order to facilitate the development of more granular relation extraction systems large manually annotated corpora are needed. Those corpora should include high-quality manually labled entity mentions together with exhaustive relation annotations generated by domain experts. TRACK AND CORPUS DESCRIPTION Corpus description To carry out the DrugProt track at BioCreative VII, we have released a large manually labelled corpus including annotations of mentions of chemical compounds and drugs as well as genes, proteins and miRNAs. Domain experts with experience in biomedical literature annotation and database curation annotated by hand all abstracts using the BRAT annotation interface. The manual labeling of chemicals and genes was done in separate steps and by different experts to avoid introducing biases during the text annotation process. The manual tagging of entity mentions of chemicals and drugs as well as genes, proteins and miRNAs was done following a carefully designed annotation process and in line with publicly released annotation guidelines. Gene/protein entity mentions were manually mapped to their corresponding biologic al database identifiers whenever possible and classified as either normalizable to databases (tag: GENE-Y) or non normalizable mentions (GENE-N). Teams that participated at the DrugProt track were only provided with this classification of gene mentions and not the actual database identifier to avoid usage of external knowledge bases for producing their predictions. The corpus construction process required first annotating exhaustively all chemical and gene mentions (phase 1). Afterwards the relation annotation phase followed (phase 2), were relationships between these two types of entities had to be labeled according to public available annotation guidelines. Thus, to facilitate the annotation of chemical-protein interactions, the DrugProt track organizers constructed very granular relation annotation rules described in a 33 pages annotation guidelines document. These guidelines were refined during an iterative process based on the annotation of sample documents. The guidelines provided the basic details of the chemicalprotein interaction annotation task and the conventions that had to be followed during the corpus construction process. They incorporated suggestions made by curators as well as observations of annotation inconsistencies encountered when comparing results from different human curators. In brief, DrugProt interactions covered direct interactions (when a physical contact existed between a chemical/drug and a gene/protein) as well as indirect regulatory interactions that alter either the function or the quantity of the gene/gene product. The aim of the iterative manual annotation cycle was to improve the quality and consistency of the guidelines. During the planning of the guidelines some rules had to be reformulated to make them more explicit and clear and additional rules were added wherever necessary to better cover the practical annotation scenario and for being more complete. The manual annotation task basically consisted of labeling or marking manually through a customized BRAT webinterface the interactions given the article abstracts as content. Figure 1 summarizes the DrugProt relation types included in the annotation guidelines. Fig. 1. Overview of the DrugProt relation type hierarchy. The corpus annotation carried out for the DrugProt track was exhaustive for all the types of interactions previously specified. This implied that mentions of other kind of relationships between chemicals and genes (e.g. phenotypic and biological responses) were not manually labelled. Moreover, the DrugProt relations are directed in the sense that only relations of “what a chemical does to a gene/protein"" (chemical → gene/protein direction) were annotated, and not vice versa. To establish a easy to understand relation nomenclature and avoid redundant class definitions, we reviewed several chemical repositories that included chemical – biology information. We revised DrugBank, the Therapeutic Targets Database (TTD) and ChEMBL, assay normalization ontologies (BAO) and previously existing formalizations for the annotation of relationships: the Biological Expression Language (BEL), curation guidelines for transcription regulation interactions (DNA-binding transcription factor – target gene interaction) and SIGNOR, a database of causal relationships between biological entities. Each of these resources inspired the definition of the subclasses DIRECT REGULATOR (e.g. DrugBank, ChEMBL, BAO and SIGNOR) and the INDIRECT REGULATOR (e.g. BEL, curation guidelines for transcription regulation interactions and SIGNOR). For example, DrugBank relationships for drugs included a total of 22 definitions, some of them overlapping with CHEMPROT subclasses (e.g. “Inhibitor”, “Antagonist”, “Agonist”,...), some of them being regarded as highly specific for the purpose of this task (e.g. “intercalation”, “cross-linking/alkylation”) or referring to biological roles (e.g. “Antibody”, “Incorporation into and Destabilization”) and others, partially overlapping between them (e.g. “Binder” and “Ligand”), that were merged into a single class. Concerning indirect regulatory aspects, the five classes of casual relationships between a subject and an object term defined by BEL (“decreases”, “directlyDecreases”, “increases”, “directlyIncreases” and “causesNoChange”) were highly inspiring. Subclasses definitions of pharmacological modes of action were defined according to the UPHAR/BPS Guide to Pharmacology in 2016. For the DrugProt track a very granular chemical-protein relation annotation was carried out, with the aim to cover most of the relations that are of importance from the point of view of biochemical and pharmacological/biomedical perspective. Nevertheless, for the DrugProt track only a total of 13 relation types were used, keeping those that had enough training instances/examples and sufficient manual annotation consistency. The final list of relation types used for this shared task was: INDIRECT-DOWNREGULATOR, INDIRECTUPREGULATOR, DIRECT-REGULATOR, ACTIVATOR, INHIBITOR, AGONIST, ANTAGONIST, AGONISTACTIVATOR, AGONIST-INHIBITOR, PRODUCT-OF, SUBSTRATE, SUBSTRATE_PRODUCT-OF or PART-OF. The DrugProt corpus was split randomly into training, development and test set. We also included a background and large scale background collection of records that were automatically annotated with drugs/chemicals and genes/proteins/miRNAs using an entity tagger trained on the manual DrugProt entity mentions. The background collections were merged with the test set to be able to get team predictions also for these records. Table 1 shows a su",what Relation types ?,Antagonist,antagonist and agonist relations ),False,True
"Considering recent progress in NLP, deep learning techniques and biomedical language models there is a pressing need to generate annotated resources and comparable evaluation scenarios that enable the development of advanced biomedical relation extraction systems that extract interactions between drugs/chemical entities and genes, proteins or miRNAs. Building on the results and experience of the CHEMDNER, CHEMDNER patents and ChemProt tracks, we have posed the DrugProt track at BioCreative VII. The DrugProt track focused on the evaluation of automatic systems able to extract 13 different types of drug-genes/protein relations of importance to understand gene regulatory and pharmacological mechanisms. The DrugProt track addressed regulatory associations (direct/indirect, activator/inhibitor relations), certain types of binding associations (antagonist and agonist relations) as well as metabolic associations (substrate or product relations). To promote development of novel tools and offer a comparative evaluation scenario we have released 61,775 manually annotated gene mentions, 65,561 chemical and drug mentions and a total of 24,526 relationships manually labeled by domain experts. A total of 30 teams submitted results for the DrugProt main track, while 9 teams submitted results for the large-scale text mining subtrack that required processing of over 2,3 million records. Teams obtained very competitive results, with predictions reaching fmeasures of over 0.92 for some relation types (antagonist) and fmeasures across all relation types close to 0.8. INTRODUCTION Among the most relevant biological and pharmacological relation types are those that involve (a) chemical compounds and drugs as well as (b) gene products including genes, proteins, miRNAs. A variety of associations between chemicals and genes/proteins are described in the biomedical literature, and there is a growing interest in facilitating a more systematic extraction of these relations from the literature, either for manual database curation initiatives or to generate large knowledge graphs of importance for drug discovery, drug repurposing, building regulatory or interaction networks or to characterize off-target interactions of drugs that might be of importance to understand better adverse drug reactions. At BioCreative VI, the ChemProt track tried to promote the development of novel systems between chemicals and genes for groups of biologically related association types (ChemProt track relation groups or CPRs). Although the obtained results did have a considerable impact in the development and evaluation of new biomedical relation extraction systems, a limitation of grouping more specific relation types into broader groups was the difficulty to directly exploit the results for database curation efforts and biomedical knowledge graph mining application scenarios. The considerable interest in the integration of chemical and biomedical data for drug-discovery purposes, together with the ongoing curation of relationships between biological and chemical entities from scientific publications and patents due to the recent COVID-19 pandemic, motivated the DrugProt track of BioCreative VII, which proposed using more granular relation types. In order to facilitate the development of more granular relation extraction systems large manually annotated corpora are needed. Those corpora should include high-quality manually labled entity mentions together with exhaustive relation annotations generated by domain experts. TRACK AND CORPUS DESCRIPTION Corpus description To carry out the DrugProt track at BioCreative VII, we have released a large manually labelled corpus including annotations of mentions of chemical compounds and drugs as well as genes, proteins and miRNAs. Domain experts with experience in biomedical literature annotation and database curation annotated by hand all abstracts using the BRAT annotation interface. The manual labeling of chemicals and genes was done in separate steps and by different experts to avoid introducing biases during the text annotation process. The manual tagging of entity mentions of chemicals and drugs as well as genes, proteins and miRNAs was done following a carefully designed annotation process and in line with publicly released annotation guidelines. Gene/protein entity mentions were manually mapped to their corresponding biologic al database identifiers whenever possible and classified as either normalizable to databases (tag: GENE-Y) or non normalizable mentions (GENE-N). Teams that participated at the DrugProt track were only provided with this classification of gene mentions and not the actual database identifier to avoid usage of external knowledge bases for producing their predictions. The corpus construction process required first annotating exhaustively all chemical and gene mentions (phase 1). Afterwards the relation annotation phase followed (phase 2), were relationships between these two types of entities had to be labeled according to public available annotation guidelines. Thus, to facilitate the annotation of chemical-protein interactions, the DrugProt track organizers constructed very granular relation annotation rules described in a 33 pages annotation guidelines document. These guidelines were refined during an iterative process based on the annotation of sample documents. The guidelines provided the basic details of the chemicalprotein interaction annotation task and the conventions that had to be followed during the corpus construction process. They incorporated suggestions made by curators as well as observations of annotation inconsistencies encountered when comparing results from different human curators. In brief, DrugProt interactions covered direct interactions (when a physical contact existed between a chemical/drug and a gene/protein) as well as indirect regulatory interactions that alter either the function or the quantity of the gene/gene product. The aim of the iterative manual annotation cycle was to improve the quality and consistency of the guidelines. During the planning of the guidelines some rules had to be reformulated to make them more explicit and clear and additional rules were added wherever necessary to better cover the practical annotation scenario and for being more complete. The manual annotation task basically consisted of labeling or marking manually through a customized BRAT webinterface the interactions given the article abstracts as content. Figure 1 summarizes the DrugProt relation types included in the annotation guidelines. Fig. 1. Overview of the DrugProt relation type hierarchy. The corpus annotation carried out for the DrugProt track was exhaustive for all the types of interactions previously specified. This implied that mentions of other kind of relationships between chemicals and genes (e.g. phenotypic and biological responses) were not manually labelled. Moreover, the DrugProt relations are directed in the sense that only relations of “what a chemical does to a gene/protein"" (chemical → gene/protein direction) were annotated, and not vice versa. To establish a easy to understand relation nomenclature and avoid redundant class definitions, we reviewed several chemical repositories that included chemical – biology information. We revised DrugBank, the Therapeutic Targets Database (TTD) and ChEMBL, assay normalization ontologies (BAO) and previously existing formalizations for the annotation of relationships: the Biological Expression Language (BEL), curation guidelines for transcription regulation interactions (DNA-binding transcription factor – target gene interaction) and SIGNOR, a database of causal relationships between biological entities. Each of these resources inspired the definition of the subclasses DIRECT REGULATOR (e.g. DrugBank, ChEMBL, BAO and SIGNOR) and the INDIRECT REGULATOR (e.g. BEL, curation guidelines for transcription regulation interactions and SIGNOR). For example, DrugBank relationships for drugs included a total of 22 definitions, some of them overlapping with CHEMPROT subclasses (e.g. “Inhibitor”, “Antagonist”, “Agonist”,...), some of them being regarded as highly specific for the purpose of this task (e.g. “intercalation”, “cross-linking/alkylation”) or referring to biological roles (e.g. “Antibody”, “Incorporation into and Destabilization”) and others, partially overlapping between them (e.g. “Binder” and “Ligand”), that were merged into a single class. Concerning indirect regulatory aspects, the five classes of casual relationships between a subject and an object term defined by BEL (“decreases”, “directlyDecreases”, “increases”, “directlyIncreases” and “causesNoChange”) were highly inspiring. Subclasses definitions of pharmacological modes of action were defined according to the UPHAR/BPS Guide to Pharmacology in 2016. For the DrugProt track a very granular chemical-protein relation annotation was carried out, with the aim to cover most of the relations that are of importance from the point of view of biochemical and pharmacological/biomedical perspective. Nevertheless, for the DrugProt track only a total of 13 relation types were used, keeping those that had enough training instances/examples and sufficient manual annotation consistency. The final list of relation types used for this shared task was: INDIRECT-DOWNREGULATOR, INDIRECTUPREGULATOR, DIRECT-REGULATOR, ACTIVATOR, INHIBITOR, AGONIST, ANTAGONIST, AGONISTACTIVATOR, AGONIST-INHIBITOR, PRODUCT-OF, SUBSTRATE, SUBSTRATE_PRODUCT-OF or PART-OF. The DrugProt corpus was split randomly into training, development and test set. We also included a background and large scale background collection of records that were automatically annotated with drugs/chemicals and genes/proteins/miRNAs using an entity tagger trained on the manual DrugProt entity mentions. The background collections were merged with the test set to be able to get team predictions also for these records. Table 1 shows a su",what Relation types ?,Inhibitor,antagonist and agonist relations ),False,False
"Considering recent progress in NLP, deep learning techniques and biomedical language models there is a pressing need to generate annotated resources and comparable evaluation scenarios that enable the development of advanced biomedical relation extraction systems that extract interactions between drugs/chemical entities and genes, proteins or miRNAs. Building on the results and experience of the CHEMDNER, CHEMDNER patents and ChemProt tracks, we have posed the DrugProt track at BioCreative VII. The DrugProt track focused on the evaluation of automatic systems able to extract 13 different types of drug-genes/protein relations of importance to understand gene regulatory and pharmacological mechanisms. The DrugProt track addressed regulatory associations (direct/indirect, activator/inhibitor relations), certain types of binding associations (antagonist and agonist relations) as well as metabolic associations (substrate or product relations). To promote development of novel tools and offer a comparative evaluation scenario we have released 61,775 manually annotated gene mentions, 65,561 chemical and drug mentions and a total of 24,526 relationships manually labeled by domain experts. A total of 30 teams submitted results for the DrugProt main track, while 9 teams submitted results for the large-scale text mining subtrack that required processing of over 2,3 million records. Teams obtained very competitive results, with predictions reaching fmeasures of over 0.92 for some relation types (antagonist) and fmeasures across all relation types close to 0.8. INTRODUCTION Among the most relevant biological and pharmacological relation types are those that involve (a) chemical compounds and drugs as well as (b) gene products including genes, proteins, miRNAs. A variety of associations between chemicals and genes/proteins are described in the biomedical literature, and there is a growing interest in facilitating a more systematic extraction of these relations from the literature, either for manual database curation initiatives or to generate large knowledge graphs of importance for drug discovery, drug repurposing, building regulatory or interaction networks or to characterize off-target interactions of drugs that might be of importance to understand better adverse drug reactions. At BioCreative VI, the ChemProt track tried to promote the development of novel systems between chemicals and genes for groups of biologically related association types (ChemProt track relation groups or CPRs). Although the obtained results did have a considerable impact in the development and evaluation of new biomedical relation extraction systems, a limitation of grouping more specific relation types into broader groups was the difficulty to directly exploit the results for database curation efforts and biomedical knowledge graph mining application scenarios. The considerable interest in the integration of chemical and biomedical data for drug-discovery purposes, together with the ongoing curation of relationships between biological and chemical entities from scientific publications and patents due to the recent COVID-19 pandemic, motivated the DrugProt track of BioCreative VII, which proposed using more granular relation types. In order to facilitate the development of more granular relation extraction systems large manually annotated corpora are needed. Those corpora should include high-quality manually labled entity mentions together with exhaustive relation annotations generated by domain experts. TRACK AND CORPUS DESCRIPTION Corpus description To carry out the DrugProt track at BioCreative VII, we have released a large manually labelled corpus including annotations of mentions of chemical compounds and drugs as well as genes, proteins and miRNAs. Domain experts with experience in biomedical literature annotation and database curation annotated by hand all abstracts using the BRAT annotation interface. The manual labeling of chemicals and genes was done in separate steps and by different experts to avoid introducing biases during the text annotation process. The manual tagging of entity mentions of chemicals and drugs as well as genes, proteins and miRNAs was done following a carefully designed annotation process and in line with publicly released annotation guidelines. Gene/protein entity mentions were manually mapped to their corresponding biologic al database identifiers whenever possible and classified as either normalizable to databases (tag: GENE-Y) or non normalizable mentions (GENE-N). Teams that participated at the DrugProt track were only provided with this classification of gene mentions and not the actual database identifier to avoid usage of external knowledge bases for producing their predictions. The corpus construction process required first annotating exhaustively all chemical and gene mentions (phase 1). Afterwards the relation annotation phase followed (phase 2), were relationships between these two types of entities had to be labeled according to public available annotation guidelines. Thus, to facilitate the annotation of chemical-protein interactions, the DrugProt track organizers constructed very granular relation annotation rules described in a 33 pages annotation guidelines document. These guidelines were refined during an iterative process based on the annotation of sample documents. The guidelines provided the basic details of the chemicalprotein interaction annotation task and the conventions that had to be followed during the corpus construction process. They incorporated suggestions made by curators as well as observations of annotation inconsistencies encountered when comparing results from different human curators. In brief, DrugProt interactions covered direct interactions (when a physical contact existed between a chemical/drug and a gene/protein) as well as indirect regulatory interactions that alter either the function or the quantity of the gene/gene product. The aim of the iterative manual annotation cycle was to improve the quality and consistency of the guidelines. During the planning of the guidelines some rules had to be reformulated to make them more explicit and clear and additional rules were added wherever necessary to better cover the practical annotation scenario and for being more complete. The manual annotation task basically consisted of labeling or marking manually through a customized BRAT webinterface the interactions given the article abstracts as content. Figure 1 summarizes the DrugProt relation types included in the annotation guidelines. Fig. 1. Overview of the DrugProt relation type hierarchy. The corpus annotation carried out for the DrugProt track was exhaustive for all the types of interactions previously specified. This implied that mentions of other kind of relationships between chemicals and genes (e.g. phenotypic and biological responses) were not manually labelled. Moreover, the DrugProt relations are directed in the sense that only relations of “what a chemical does to a gene/protein"" (chemical → gene/protein direction) were annotated, and not vice versa. To establish a easy to understand relation nomenclature and avoid redundant class definitions, we reviewed several chemical repositories that included chemical – biology information. We revised DrugBank, the Therapeutic Targets Database (TTD) and ChEMBL, assay normalization ontologies (BAO) and previously existing formalizations for the annotation of relationships: the Biological Expression Language (BEL), curation guidelines for transcription regulation interactions (DNA-binding transcription factor – target gene interaction) and SIGNOR, a database of causal relationships between biological entities. Each of these resources inspired the definition of the subclasses DIRECT REGULATOR (e.g. DrugBank, ChEMBL, BAO and SIGNOR) and the INDIRECT REGULATOR (e.g. BEL, curation guidelines for transcription regulation interactions and SIGNOR). For example, DrugBank relationships for drugs included a total of 22 definitions, some of them overlapping with CHEMPROT subclasses (e.g. “Inhibitor”, “Antagonist”, “Agonist”,...), some of them being regarded as highly specific for the purpose of this task (e.g. “intercalation”, “cross-linking/alkylation”) or referring to biological roles (e.g. “Antibody”, “Incorporation into and Destabilization”) and others, partially overlapping between them (e.g. “Binder” and “Ligand”), that were merged into a single class. Concerning indirect regulatory aspects, the five classes of casual relationships between a subject and an object term defined by BEL (“decreases”, “directlyDecreases”, “increases”, “directlyIncreases” and “causesNoChange”) were highly inspiring. Subclasses definitions of pharmacological modes of action were defined according to the UPHAR/BPS Guide to Pharmacology in 2016. For the DrugProt track a very granular chemical-protein relation annotation was carried out, with the aim to cover most of the relations that are of importance from the point of view of biochemical and pharmacological/biomedical perspective. Nevertheless, for the DrugProt track only a total of 13 relation types were used, keeping those that had enough training instances/examples and sufficient manual annotation consistency. The final list of relation types used for this shared task was: INDIRECT-DOWNREGULATOR, INDIRECTUPREGULATOR, DIRECT-REGULATOR, ACTIVATOR, INHIBITOR, AGONIST, ANTAGONIST, AGONISTACTIVATOR, AGONIST-INHIBITOR, PRODUCT-OF, SUBSTRATE, SUBSTRATE_PRODUCT-OF or PART-OF. The DrugProt corpus was split randomly into training, development and test set. We also included a background and large scale background collection of records that were automatically annotated with drugs/chemicals and genes/proteins/miRNAs using an entity tagger trained on the manual DrugProt entity mentions. The background collections were merged with the test set to be able to get team predictions also for these records. Table 1 shows a su",what Relation types ?,Activator,antagonist and agonist relations ),False,False
"Knowledge about software used in scientific investigations is important for several reasons, for instance, to enable an understanding of provenance and methods involved in data handling. However, software is usually not formally cited, but rather mentioned informally within the scholarly description of the investigation, raising the need for automatic information extraction and disambiguation. Given the lack of reliable ground truth data, we present SoMeSci-Software Mentions in Science-a gold standard knowledge graph of software mentions in scientific articles. It contains high quality annotations (IRR: K=.82) of 3756 software mentions in 1367 PubMed Central articles. Besides the plain mention of the software, we also provide relation labels for additional information, such as the version, the developer, a URL or citations. Moreover, we distinguish between different types, such as application, plugin or programming environment, as well as different types of mentions, such as usage or creation. To the best of our knowledge, SoMeSci is the most comprehensive corpus about software mentions in scientific articles, providing training samples for Named Entity Recognition, Relation Extraction, Entity Disambiguation, and Entity Linking. Finally, we sketch potential use cases and provide baseline results.",what Relation types ?,Developer,"application,",False,False
"This paper analyses the relationship between GDP and carbon dioxide emissions for Mauritius and vice-versa in a historical perspective. Using rigorous econometrics analysis, our results suggest that the carbon dioxide emission trajectory is closely related to the GDP time path. We show that emissions elasticity on income has been increasing over time. By estimating the EKC for the period 1975-2009, we were unable to prove the existence of a reasonable turning point and thus no EKC “U” shape was obtained. Our results suggest that Mauritius could not curb its carbon dioxide emissions in the last three decades. Thus, as hypothesized, the cost of degradation associated with GDP grows over time and it suggests that the economic and human activities are having increasingly negative environmental impacts on the country as cpmpared to their economic prosperity.",what Type of data ?,time,econometrics,False,False
"This study examines the impact of various factors such as gross domestic product (GDP) per capita, energy use per capita and trade openness on carbon dioxide (CO 2 ) emission per capita in the Central and Eastern European Countries. The extended environmental Kuznets curve (EKC) was employed, utilizing the available panel data from 1980 to 2002 for Bulgaria, Hungary, Romania and Turkey. The results confirm the existence of an EKC for the region such that CO 2 emission per capita decreases over time as the per capita GDP increases. Energy use per capita is a significant factor that causes pollution in the region, indicating that the region produces environmentally unclean energy. The trade openness variable implies that globalization has not facilitated the emission level in the region. The results imply that the region needs environmentally cleaner technologies in energy production to achieve sustainable development. Copyright © 2008 John Wiley & Sons, Ltd and ERP Environment.",what Type of data ?,Panel,panel,True,True
"In this article, we attempt to use panel unit root and panel cointegration tests as well as the fully-modified ordinary least squares (OLS) approach to examine the relationships among carbon dioxide emissions, energy use and gross domestic product for 22 Organization for Economic Cooperation and Development (OECD) countries (Annex II Parties) over the 1971–2000 period. Furthermore, in order to investigate these results for other direct greenhouse gases (GHGs), we have estimated the Environmental Kuznets Curve (EKC) hypothesis by using the total GHG, methane, and nitrous oxide. The empirical results support that energy use still plays an important role in explaining the GHG emissions for OECD countries. In terms of the EKC hypothesis, the results showed that a quadratic relationship was found to exist in the long run. Thus, other countries could learn from developed countries in this regard and try to smooth the EKC curve at relatively less cost.",what Type of data ?,Panel,panel,True,True
"We explore the emissions income relationship for CO2 in OECD countries using various modelling strategies.Even for this relatively homogeneous sample, we find that the inverted-U-shaped curve is quite sensitive to the degree of heterogeneity included in the panel estimations.This finding is robust, not only across different model specifications but also across estimation techniques, including the more flexible non-parametric approach.Differences in restrictions applied in panel estimations are therefore responsible for the widely divergent findings for an inverted-U shape for CO2.Our findings suggest that allowing for enough heterogeneity is essential to prevent spurious correlation from reduced-form panel estimations.Moreover, this inverted U for CO2 is likely to exist for many, but not for all, countries.",what Type of data ?,Panel,panel,True,True
"This paper examines the dynamic causal relationship between carbon dioxide emissions, energy consumption, economic growth, foreign trade and urbanization using time series data for the period of 1960-2009. Short-run unidirectional causalities are found from energy consumption and trade openness to carbon dioxide emissions, from trade openness to energy consumption, from carbon dioxide emissions to economic growth, and from economic growth to trade openness. The test results also support the evidence of existence of long-run relationship among the variables in the form of Equation (1) which also conform the results of bounds and Johansen conintegration tests. It is found that over time higher energy consumption in Japan gives rise to more carbon dioxide emissions as a result the environment will be polluted more. But in respect of economic growth, trade openness and urbanization the environmental quality is found to be normal good in the long-run.",what Type of data ?,Time series,time series,True,True
"There has been a growing interest in the design and synthesis of non-fullerene acceptors for organic solar cells that may overcome the drawbacks of the traditional fullerene-based acceptors. Herein, two novel push-pull (acceptor-donor-acceptor) type small-molecule acceptors, that is, ITDI and CDTDI, with indenothiophene and cyclopentadithiophene as the core units and 2-(3-oxo-2,3-dihydroinden-1-ylidene)malononitrile (INCN) as the end-capping units, are designed and synthesized for non-fullerene polymer solar cells (PSCs). After device optimization, PSCs based on ITDI exhibit good device performance with a power conversion efficiency (PCE) as high as 8.00%, outperforming the CDTDI-based counterparts fabricated under identical condition (2.75% PCE). We further discuss the performance of these non-fullerene PSCs by correlating the energy level and carrier mobility with the core of non-fullerene acceptors. These results demonstrate that indenothiophene is a promising electron-donating core for high-performance non-fullerene small-molecule acceptors.",what Acceptor ?,ITDI,"cdtdi, with indenothiophene and cyclopentadithiophene",False,False
"Low bandgap n-type organic semiconductor (n-OS) ITIC has attracted great attention for the application as an acceptor with medium bandgap p-type conjugated polymer as donor in nonfullerene polymer solar cells (PSCs) because of its attractive photovoltaic performance. Here we report a modification on the molecular structure of ITIC by side-chain isomerization with meta-alkyl-phenyl substitution, m-ITIC, to further improve its photovoltaic performance. In a comparison with its isomeric counterpart ITIC with para-alkyl-phenyl substitution, m-ITIC shows a higher film absorption coefficient, a larger crystalline coherence, and higher electron mobility. These inherent advantages of m-ITIC resulted in a higher power conversion efficiency (PCE) of 11.77% for the nonfullerene PSCs with m-ITIC as acceptor and a medium bandgap polymer J61 as donor, which is significantly improved over that (10.57%) of the corresponding devices with ITIC as acceptor. To the best of our knowledge, the PCE of 11.77% is one of the highest values reported in the literature to date for nonfullerene PSCs. More importantly, the m-ITIC-based device shows less thickness-dependent photovoltaic behavior than ITIC-based devices in the active-layer thickness range of 80-360 nm, which is beneficial for large area device fabrication. These results indicate that m-ITIC is a promising low bandgap n-OS for the application as an acceptor in PSCs, and the side-chain isomerization could be an easy and convenient way to further improve the photovoltaic performance of the donor and acceptor materials for high efficiency PSCs.",what Acceptor ?,m-ITIC,j61,False,False
"We develop an efficient fused-ring electron acceptor (ITIC-Th) based on indacenodithieno[3,2-b]thiophene core and thienyl side-chains for organic solar cells (OSCs). Relative to its counterpart with phenyl side-chains (ITIC), ITIC-Th shows lower energy levels (ITIC-Th: HOMO = -5.66 eV, LUMO = -3.93 eV; ITIC: HOMO = -5.48 eV, LUMO = -3.83 eV) due to the σ-inductive effect of thienyl side-chains, which can match with high-performance narrow-band-gap polymer donors and wide-band-gap polymer donors. ITIC-Th has higher electron mobility (6.1 × 10(-4) cm(2) V(-1) s(-1)) than ITIC (2.6 × 10(-4) cm(2) V(-1) s(-1)) due to enhanced intermolecular interaction induced by sulfur-sulfur interaction. We fabricate OSCs by blending ITIC-Th acceptor with two different low-band-gap and wide-band-gap polymer donors. In one case, a power conversion efficiency of 9.6% was observed, which rivals some of the highest efficiencies for single junction OSCs based on fullerene acceptors.",what Acceptor ?,ITIC-Th,itic - th ),False,False
"<p>A nonfullerene electron acceptor (IEIC) based on indaceno[1,2-<italic>b</italic>:5,6-<italic>b</italic>′]dithiophene and 2-(3-oxo-2,3-dihydroinden-1-ylidene)malononitrile was designed and synthesized, and fullerene-free polymer solar cells based on the IEIC acceptor showed power conversion efficiencies of up to 6.31%.</p>",what Acceptor ?,IEIC,ieic ),True,True
"Two cheliform non-fullerene acceptors, DTPC-IC and DTPC-DFIC, based on a highly electron-rich core, dithienopicenocarbazole (DTPC), are synthesized, showing ultra-narrow bandgaps (as low as 1.21 eV). The two-dimensional nitrogen-containing conjugated DTPC possesses strong electron-donating capability, which induces intense intramolecular charge transfer and intermolecular π-π stacking in derived acceptors. The solar cell based on DTPC-DFIC and a spectrally complementary polymer donor, PTB7-Th, showed a high power conversion efficiency of 10.21% and an extremely low energy loss of 0.45 eV, which is the lowest among reported efficient OSCs.",what Acceptor ?,DTPC-DFIC,,False,False
"Abstract Objective: Paclitaxel (PTX)-loaded polymer (Poly(lactic-co-glycolic acid), PLGA)-based nanoformulation was developed with the objective of formulating cremophor EL-free nanoformulation intended for intravenous use. Significance: The polymeric PTX nanoparticles free from the cremophor EL will help in eliminating the shortcomings of the existing delivery system as cremophor EL causes serious allergic reactions to the subjects after intravenous use. Methods and results: Paclitaxel-loaded nanoparticles were formulated by nanoprecipitation method. The diminutive nanoparticles (143.2 nm) with uniform size throughout (polydispersity index, 0.115) and high entrapment efficiency (95.34%) were obtained by employing the Box–Behnken design for the optimization of the formulation with the aid of desirability approach-based numerical optimization technique. Optimized levels for each factor viz. polymer concentration (X1), amount of organic solvent (X2), and surfactant concentration (X3) were 0.23%, 5 ml %, and 1.13%, respectively. The results of the hemocompatibility studies confirmed the safety of PLGA-based nanoparticles for intravenous administration. Pharmacokinetic evaluations confirmed the longer retention of PTX in systemic circulation. Conclusion: In a nutshell, the developed polymeric nanoparticle formulation of PTX precludes the inadequacy of existing PTX formulation and can be considered as superior alternative carrier system of the same.",what Uses drug ?,Paclitaxel,paclitaxel,True,True
"Abstract Melanotransferrin antibody (MA) and tamoxifen (TX) were conjugated on etoposide (ETP)-entrapped solid lipid nanoparticles (ETP-SLNs) to target the blood–brain barrier (BBB) and glioblastom multiforme (GBM). MA- and TX-conjugated ETP-SLNs (MA–TX–ETP–SLNs) were used to infiltrate the BBB comprising a monolayer of human astrocyte-regulated human brain-microvascular endothelial cells (HBMECs) and to restrain the proliferation of malignant U87MG cells. TX-grafted ETP-SLNs (TX–ETP–SLNs) significantly enhanced the BBB permeability coefficient for ETP and raised the fluorescent intensity of calcein-AM when compared with ETP-SLNs. In addition, surface MA could increase the BBB permeability coefficient for ETP about twofold. The viability of HBMECs was higher than 86%, suggesting a high biocompatibility of MA–TX–ETP-SLNs. Moreover, the efficiency in antiproliferation against U87MG cells was in the order of MA–TX–ETP-SLNs > TX–ETP-SLNs > ETP-SLNs > SLNs. The capability of MA–TX–ETP-SLNs to target HBMECs and U87MG cells during internalization was verified by immunochemical staining of expressed melanotransferrin. MA–TX–ETP-SLNs can be a potent pharmacotherapy to deliver ETP across the BBB to GBM.",what Uses drug ?,Tamoxifen,etoposide,False,False
"Nanocrystal formulation has become a viable solution for delivering poorly soluble drugs including chemotherapeutic agents. The purpose of this study was to examine cellular uptake of paclitaxel nanocrystals by confocal imaging and concentration measurement. It was found that drug nanocrystals could be internalized by KB cells at much higher concentrations than a conventional, solubilized formulation. The imaging and quantitative results suggest that nanocrystals could be directly taken up by cells as solid particles, likely via endocytosis. Moreover, it was found that polymer treatment to drug nanocrystals, such as surface coating and lattice entrapment, significantly influenced the cellular uptake. While drug molecules are in the most stable physical state, nanocrystals of a poorly soluble drug are capable of achieving concentrated intracellular presence enabling needed therapeutic effects.",what Uses drug ?,Paclitaxel,paclitaxel,True,True
"Radioresistant hypoxic cells may contribute to the failure of radiation therapy in controlling certain tumors. Some studies have suggested the radiosensitizing effect of paclitaxel. The poly(D,L-lactide-co-glycolide)(PLGA) nanoparticles containing paclitaxel were prepared by o/w emulsification-solvent evaporation method. The physicochemical characteristics of the nanoparticles (i.e. encapsulation efficiency, particle size distribution, morphology, in vitro release) were studied. The morphology of the two human tumor cell lines: a carcinoma cervicis (HeLa) and a hepatoma (HepG2), treated with paclitaxel-loaded nanoparticles was photomicrographed. Flow cytometry was used to quantify the number of the tumor cells held in the G2/M phase of the cell cycle. The cellular uptake of nanoparticles was evaluated by transmission electronic microscopy. Cell viability was determined by the ability of single cell to form colonies in vitro. The prepared nanoparticles were spherical in shape with size between 200nm and 800nm. The encapsulation efficiency was 85.5％. The release behaviour of paclitaxel from the nanoparticles exhibited a biphasic pattern characterised by a fast initial release during the first 24 h, followed by a slower and continuous release. Co-culture of the two tumor cell lines with paclitaxel-loaded nanoparticles demonstrated that the cell morphology was changed and the released paclitaxel retained its bioactivity to block cells in the G2/M phase. The cellular uptake of nanoparticles was observed. The free paclitaxel and paclitaxel-loaded nanoparticles effectively sensitized hypoxic HeLa and HepG2 cells to radiation. Under this experimental condition, the radiosensitization of paclitaxel-loaded nanoparticles was more significant than that of free paclitaxel.Keywords: Paclitaxel；Drug delivery；Nanoparticle；Radiotherapy；Hypoxia；Human tumor cells；cellular uptake",what Uses drug ?,Paclitaxel,paclitaxel.,True,True
"Alzheimer's disease is a growing concern in the modern world. As the currently available medications are not very promising, there is an increased need for the fabrication of newer drugs. Curcumin is a plant derived compound which has potential activities beneficial for the treatment of Alzheimer's disease. Anti-amyloid activity and anti-oxidant activity of curcumin is highly beneficial for the treatment of Alzheimer's disease. The insolubility of curcumin in water restricts its use to a great extend, which can be overcome by the synthesis of curcumin nanoparticles. In our work, we have successfully synthesized water-soluble PLGA coated- curcumin nanoparticles and characterized it using different techniques. As drug targeting to diseases of cerebral origin are difficult due to the stringency of blood-brain barrier, we have coupled the nanoparticle with Tet-1 peptide, which has the affinity to neurons and possess retrograde transportation properties. Our results suggest that curcumin encapsulated-PLGA nanoparticles are able to destroy amyloid aggregates, exhibit anti-oxidative property and are non-cytotoxic. The encapsulation of the curcumin in PLGA does not destroy its inherent properties and so, the PLGA-curcumin nanoparticles can be used as a drug with multiple functions in treating Alzheimer's disease proving it to be a potential therapeutic tool against this dreaded disease.",what Uses drug ?,Curcumin,curcumin,True,True
"Biotherapeutics such as peptides possess strong potential for the treatment of intractable neurological disorders. However, because of their low stability and the impermeability of the blood-brain barrier (BBB), biotherapeutics are difficult to transport into brain parenchyma via intravenous injection. Herein, we present a novel poly(ethylene glycol)-poly(d,l-lactic-co-glycolic acid) polymersome-based nanomedicine with self-assembled bilayers, which was functionalized with lactoferrin (Lf-POS) to facilitate the transport of a neuroprotective peptide into the brain. The apparent diffusion coefficient (D*) of H(+) through the polymersome membrane was 5.659 × 10(-26) cm(2) s(-1), while that of liposomes was 1.017 × 10(-24) cm(2) s(-1). The stability of the polymersome membrane was much higher than that of liposomes. The uptake of polymersomes by mouse brain capillary endothelial cells proved that the optimal density of lactoferrin was 101 molecules per polymersome. Fluorescence imaging indicated that Lf101-POS was effectively transferred into the brain. In pharmacokinetics, compared with transferrin-modified polymersomes and cationic bovine serum albumin-modified polymersomes, Lf-POS obtained the greatest BBB permeability surface area and percentage of injected dose per gram (%ID per g). Furthermore, Lf-POS holding S14G-humanin protected against learning and memory impairment induced by amyloid-β25-35 in rats. Western blotting revealed that the nanomedicine provided neuroprotection against over-expression of apoptotic proteins exhibiting neurofibrillary tangle pathology in neurons. The results indicated that polymersomes can be exploited as a promising non-invasive nanomedicine capable of mediating peptide therapeutic delivery and controlling the release of drugs to the central nervous system.",what Uses drug ?,Humanin,lactoferrin,False,False
"The Fe(3)O(4) nanoparticles, tailored with maleimidyl 3-succinimidopropionate ligands, were conjugated with paclitaxel molecules that were attached with a poly(ethylene glycol) (PEG) spacer through a phosphodiester moiety at the (C-2')-OH position. The average number of paclitaxel molecules/nanoparticles was determined as 83. These nanoparticles liberated paclitaxel molecules upon exposure to phosphodiesterase.",what Uses drug ?,Paclitaxel,paclitaxel,True,True
"Breast cancer is a major form of cancer, with a high mortality rate in women. It is crucial to achieve more efficient and safe anticancer drugs. Recent developments in medical nanotechnology have resulted in novel advances in cancer drug delivery. Cisplatin, doxorubicin, and 5-fluorouracil are three important anti-cancer drugs which have poor water-solubility. In this study, we used cisplatin, doxorubicin, and 5-fluorouracil-loaded polycaprolactone-polyethylene glycol (PCL-PEG) nanoparticles to improve the stability and solubility of molecules in drug delivery systems. The nanoparticles were prepared by a double emulsion method and characterized with Fourier Transform Infrared (FTIR) spectroscopy and Hydrogen-1 nuclear magnetic resonance (1HNMR). Cells were treated with equal concentrations of cisplatin, doxorubicin and 5-fluorouracil-loaded PCL-PEG nanoparticles, and free cisplatin, doxorubicin and 5-fluorouracil. The 3-[4,5-dimethylthiazol-2yl]-2,5-diphenyl tetrazolium bromide (MTT) assay confirmed that cisplatin, doxorubicin, and 5-fluorouracil-loaded PCL-PEG nanoparticles enhanced cytotoxicity and drug delivery in T47D and MCF7 breast cancer cells. However, the IC50 value of doxorubicin was lower than the IC50 values of both cisplatin and 5-fluorouracil, where the difference was statistically considered significant (p˂0.05). However, the IC50 value of all drugs on T47D were lower than those on MCF7.",what Uses drug ?,Cisplatin,,False,False
"Background: Paclitaxel (PTX) is one of the most important and effective anticancer drugs for the treatment of human cancer. However, its low solubility and severe adverse effects limited clinical use. To overcome this limitation, nanotechnology has been used to overcome tumors due to its excellent antimicrobial activity. Objective: This study was to demonstrate the anticancer properties of functionalization silver nanoparticles loaded with paclitaxel (Ag@PTX) induced A549 cells apoptosis through ROS-mediated signaling pathways. Methods: The Ag@PTX nanoparticles were charged with a zeta potential of about -17 mv and characterized around 2 nm with a narrow size distribution. Results: Ag@PTX significantly decreased the viability of A549 cells and possessed selectivity between cancer and normal cells. Ag@PTX induced A549 cells apoptosis was confirmed by nuclear condensation, DNA fragmentation, and activation of caspase-3. Furthermore, Ag@PTX enhanced the anti-cancer activity of A549 cells through ROS-mediated p53 and AKT signalling pathways. Finally, in a xenograft nude mice model, Ag@PTX suppressed the growth of tumors. Conclusion: Our findings suggest that Ag@PTX may be a candidate as a chemopreventive agent and could be a highly efficient way to achieve anticancer synergism for human cancers.",what Uses drug ?,Paclitaxel,paclitaxel,True,True
"Breast cancer is a major form of cancer, with a high mortality rate in women. It is crucial to achieve more efficient and safe anticancer drugs. Recent developments in medical nanotechnology have resulted in novel advances in cancer drug delivery. Cisplatin, doxorubicin, and 5-fluorouracil are three important anti-cancer drugs which have poor water-solubility. In this study, we used cisplatin, doxorubicin, and 5-fluorouracil-loaded polycaprolactone-polyethylene glycol (PCL-PEG) nanoparticles to improve the stability and solubility of molecules in drug delivery systems. The nanoparticles were prepared by a double emulsion method and characterized with Fourier Transform Infrared (FTIR) spectroscopy and Hydrogen-1 nuclear magnetic resonance (1HNMR). Cells were treated with equal concentrations of cisplatin, doxorubicin and 5-fluorouracil-loaded PCL-PEG nanoparticles, and free cisplatin, doxorubicin and 5-fluorouracil. The 3-[4,5-dimethylthiazol-2yl]-2,5-diphenyl tetrazolium bromide (MTT) assay confirmed that cisplatin, doxorubicin, and 5-fluorouracil-loaded PCL-PEG nanoparticles enhanced cytotoxicity and drug delivery in T47D and MCF7 breast cancer cells. However, the IC50 value of doxorubicin was lower than the IC50 values of both cisplatin and 5-fluorouracil, where the difference was statistically considered significant (p˂0.05). However, the IC50 value of all drugs on T47D were lower than those on MCF7.",what Uses drug ?,Doxorubicin,,False,False
"In this study, we examine the relationship between the physical structure and dissolution behavior of olanzapine (OLZ) prepared via hot-melt extrusion in three polymers [polyvinylpyrrolidone (PVP) K30, polyvinylpyrrolidone-co-vinyl acetate (PVPVA) 6:4, and Soluplus® (SLP)]. In particular, we examine whether full amorphicity is necessary to achieve a favorable dissolution profile. Drug–polymer miscibility was estimated using melting point depression and Hansen solubility parameters. Solid dispersions were characterized using differential scanning calorimetry, X-ray powder diffraction, and scanning electron microscopy. All the polymers were found to be miscible with OLZ in a decreasing order of PVP>PVPVA>SLP. At a lower extrusion temperature (160°C), PVP generated fully amorphous dispersions with OLZ, whereas the formulations with PVPVA and SLP contained 14%–16% crystalline OLZ. Increasing the extrusion temperature to 180°C allowed the preparation of fully amorphous systems with PVPVA and SLP. Despite these differences, the dissolution rates of these preparations were comparable, with PVP showing a lower release rate despite being fully amorphous. These findings suggested that, at least in the particular case of OLZ, the absence of crystalline material may not be critical to the dissolution performance. We suggest alternative key factors determining dissolution, particularly the dissolution behavior of the polymers themselves.",what Uses drug ?,Olanzapine,olanzapine,True,True
"Abstract— The effect of topically active 2‐hydroxypropyl‐β‐cyclodextrin (HP‐β‐CyD) eye‐drop formulations containing solutions of acetazolamide, ethoxyzolamide or timolol on the intra‐ocular pressure (IOP) was investigated in normotensive conscious rabbits. Both acetazolamide and ethoxyzolamide were active but their IOP‐lowering effect was less than that of timolol. The IOP‐lowering effects of acetazolamide and ethoxyzolamide and that of timolol appeared to be to some extent additive. Combination of acetazolamide and timolol or ethoxyzolamide and timolol in one HP‐β‐CyD formulation resulted in a significant increase in the duration of activity compared with HP‐β‐CyD formulations containing only acetazolamide, ethoxyzolamide or timolol. Also, it was possible to increase the IOP‐lowering effect of acetazolamide by formulating the drug as a suspension in an aqueous HP‐β‐CyD vehicle.",what Uses drug ?,Acetazolamide,,False,False
"UNLABELLED Interactivity between humans and smart systems, including wearable, body-attachable, or implantable platforms, can be enhanced by realization of multifunctional human-machine interfaces, where a variety of sensors collect information about the surrounding environment, intentions, or physiological conditions of the human to which they are attached. Here, we describe a stretchable, transparent, ultrasensitive, and patchable strain sensor that is made of a novel sandwich-like stacked piezoresisitive nanohybrid film of single-wall carbon nanotubes (SWCNTs) and a conductive elastomeric composite of polyurethane (PU)-poly(3,4-ethylenedioxythiophene) polystyrenesulfonate ( PEDOT PSS). This sensor, which can detect small strains on human skin, was created using environmentally benign water-based solution processing. We attributed the tunability of strain sensitivity (i.e., gauge factor), stability, and optical transparency to enhanced formation of percolating networks between conductive SWCNTs and PEDOT phases at interfaces in the stacked PU-PEDOT:PSS/SWCNT/PU-PEDOT:PSS structure. The mechanical stability, high stretchability of up to 100%, optical transparency of 62%, and gauge factor of 62 suggested that when attached to the skin of the face, this sensor would be able to detect small strains induced by emotional expressions such as laughing and crying, as well as eye movement, and we confirmed this experimentally.",what Sensing material ?,SWCNT/PU-PEDOT:PSS,polyurethane,False,False
"Graphene is widely regarded as one of the most promising materials for sensor applications. Here, we demonstrate that a pristine graphene can detect gas molecules at extremely low concentrations with detection limits as low as 158 parts-per-quadrillion (ppq) for a range of gas molecules at room temperature. The unprecedented sensitivity was achieved by applying our recently developed concept of continuous in situ cleaning of the sensing material with ultraviolet light. The simplicity of the concept, together with graphene’s flexibility to be used on various platforms, is expected to intrigue more investigations to develop ever more sensitive sensors.",what Sensing material ?,Graphene,graphene,True,True
"In this article, cupric oxide (CuO) leafletlike nanosheets have been synthesized by a facile, low-cost, and surfactant-free method, and they have further been successfully developed for sensitive and selective determination of hydrogen sulfide (H2S) with high recovery ability. The experimental results have revealed that the sensitivity and recovery time of the present H2S gas sensor are strongly dependent on the working temperature. The best H2S sensing performance has been achieved with a low detection limit of 2 ppb and broad linear range from 30 ppb to 1.2 ppm. The gas sensor is reversible, with a quick response time of 4 s and a short recovery time of 9 s. In addition, negligible responses can be observed exposed to 100-fold concentrations of other gases which may exist in the atmosphere such as nitrogen (N2), oxygen (O2), nitric oxide (NO), cabon monoxide (CO), nitrogen dioxide (NO2), hydrogen (H2), and so on, indicating relatively high selectivity of the present H2S sensor. The H2S sensor based on t...",what Sensing material ?,CuO,cupric oxide,False,False
"Despite their successful use in many conscientious studies involving outdoor learning applications, mobile learning systems still have certain limitations. For instance, because students cannot obtain real-time, contextaware content in outdoor locations such as historical sites, endangered animal habitats, and geological landscapes, they are unable to search, collect, share, and edit information by using information technology. To address such concerns, this work proposes an environment of ubiquitous learning with educational resources (EULER) based on radio frequency identification (RFID), augmented reality (AR), the Internet, ubiquitous computing, embedded systems, and database technologies. EULER helps teachers deliver lessons on site and cultivate student competency in adopting information technology to improve learning. To evaluate its effectiveness, we used the proposed EULER for natural science learning at the Guandu Nature Park in Taiwan. The participants were elementary school teachers and students. The analytical results revealed that the proposed EULER improves student learning. Moreover, the largely positive feedback from a post-study survey confirms the effectiveness of EULER in supporting outdoor learning and its ability to attract the interest of students.",what Result ?,Positive,euler ) based on radio frequency identification,False,False
"Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.",what Result ?,F1 score,conll - 2003 dataset and surpasses the previously reported state of the art performance on the ontonotes 5. 0,False,False
"A rapidly growing amount of content posted online, such as food recipes, opens doors to new exciting applications at the intersection of vision and language. In this work, we aim to estimate the calorie amount of a meal directly from an image by learning from recipes people have published on the Internet, thus skipping time-consuming manual data annotation. Since there are few large-scale publicly available datasets captured in unconstrained environments, we propose the pic2kcal benchmark comprising 308 000 images from over 70 000 recipes including photographs, ingredients, and instructions. To obtain nutritional information of the ingredients and automatically determine the ground-truth calorie value, we match the items in the recipes with structured information from a food item database. We evaluate various neural networks for regression of the calorie quantity and extend them with the multi-task paradigm. Our learning procedure combines the calorie estimation with prediction of proteins, carbohydrates, and fat amounts as well as a multi-label ingredient classification. Our experiments demonstrate clear benefits of multi-task learning for calorie estimation, surpassing the single-task calorie regression by 9.9%. To encourage further research on this task, we make the code for generating the dataset and the models publicly available.",what Result ?,Prediction of proteins,pic2kcal,False,False
"A rapidly growing amount of content posted online, such as food recipes, opens doors to new exciting applications at the intersection of vision and language. In this work, we aim to estimate the calorie amount of a meal directly from an image by learning from recipes people have published on the Internet, thus skipping time-consuming manual data annotation. Since there are few large-scale publicly available datasets captured in unconstrained environments, we propose the pic2kcal benchmark comprising 308 000 images from over 70 000 recipes including photographs, ingredients, and instructions. To obtain nutritional information of the ingredients and automatically determine the ground-truth calorie value, we match the items in the recipes with structured information from a food item database. We evaluate various neural networks for regression of the calorie quantity and extend them with the multi-task paradigm. Our learning procedure combines the calorie estimation with prediction of proteins, carbohydrates, and fat amounts as well as a multi-label ingredient classification. Our experiments demonstrate clear benefits of multi-task learning for calorie estimation, surpassing the single-task calorie regression by 9.9%. To encourage further research on this task, we make the code for generating the dataset and the models publicly available.",what Result ?,Ingredient classification,pic2kcal,False,False
"In wireless sensor networks, nodes in the area of interest must report sensing readings to the sink, and this report always satisfies the report frequency required by the sink. This paper proposes a link-aware clustering mechanism, called LCM, to determine an energy-efficient and reliable routing path. The LCM primarily considers node status and link condition, and uses a novel clustering metric called the predicted transmission count (PTX), to evaluate the qualification of nodes for clusterheads and gateways to construct clusters. Each clusterhead or gateway candidate depends on the PTX to derive its priority, and the candidate with the highest priority becomes the clusterhead or gateway. Simulation results validate that the proposed LCM significantly outperforms the clustering mechanisms using random selection and by considering only link quality and residual energy in the packet delivery ratio, energy consumption, and delivery latency.",what Protocol ?,LCM,"lcm,",True,True
"The ability to extract topological regularity out of large randomly deployed sensor networks holds the promise to maximally leverage correlation for data aggregation and also to assist with sensor localization and hierarchy creation. This paper focuses on extracting such regular structures from physical topology through the development of a distributed clustering scheme. The topology adaptive spatial clustering (TASC) algorithm presented here is a distributed algorithm that partitions the network into a set of locally isotropic, non-overlapping clusters without prior knowledge of the number of clusters, cluster size and node coordinates. This is achieved by deriving a set of weights that encode distance measurements, connectivity and density information within the locality of each node. The derived weights form the terrain for holding a coordinated leader election in which each node selects the node closer to the center of mass of its neighborhood to become its leader. The clustering algorithm also employs a dynamic density reachability criterion that groups nodes according to their neighborhood's density properties. Our simulation results show that the proposed algorithm can trace locally isotropic structures in non-isotropic network and cluster the network with respect to local density attributes. We also found out that TASC exhibits consistent behavior in the presence of moderate measurement noise levels",what Protocol ?,TASC,distributed,False,False
"Sensor webs consisting of nodes with limited battery power and wireless communications are deployed to collect useful information from the field. Gathering sensed information in an energy efficient manner is critical to operate the sensor network for a long period of time. In W. Heinzelman et al. (Proc. Hawaii Conf. on System Sci., 2000), a data collection problem is defined where, in a round of communication, each sensor node has a packet to be sent to the distant base station. If each node transmits its sensed data directly to the base station then it will deplete its power quickly. The LEACH protocol presented by W. Heinzelman et al. is an elegant solution where clusters are formed to fuse data before transmitting to the base station. By randomizing the cluster heads chosen to transmit to the base station, LEACH achieves a factor of 8 improvement compared to direct transmissions, as measured in terms of when nodes die. In this paper, we propose PEGASIS (power-efficient gathering in sensor information systems), a near optimal chain-based protocol that is an improvement over LEACH. In PEGASIS, each node communicates only with a close neighbor and takes turns transmitting to the base station, thus reducing the amount of energy spent per round. Simulation results show that PEGASIS performs better than LEACH by about 100 to 300% when 1%, 20%, 50%, and 100% of nodes die for different network sizes and topologies.",what Protocol ?,PEGASIS,leach,False,False
"Future large-scale sensor networks may comprise thousands of wirelessly connected sensor nodes that could provide an unimaginable opportunity to interact with physical phenomena in real time. However, the nodes are typically highly resource-constrained. Since the communication task is a significant power consumer, various attempts have been made to introduce energy-awareness at different levels within the communication stack. Clustering is one such attempt to control energy dissipation for sensor data dissemination in a multihop fashion. The Time-Controlled Clustering Algorithm (TCCA) is proposed to realize a network-wide energy reduction. A realistic energy dissipation model is derived probabilistically to quantify the sensor network's energy consumption using the proposed clustering algorithm. A discrete-event simulator is developed to verify the mathematical model and to further investigate TCCA in other scenarios. The simulator is also extended to include the rest of the communication stack to allow a comprehensive evaluation of the proposed algorithm.",what Protocol ?,TCCA,tcca ),True,True
"Wireless sensor networks are expected to find wide applicability and increasing deployment in the near future. In this paper, we propose a formal classification of sensor networks, based on their mode of functioning, as proactive and reactive networks. Reactive networks, as opposed to passive data collecting proactive networks, respond immediately to changes in the relevant parameters of interest. We also introduce a new energy efficient protocol, TEEN (Threshold sensitive Energy Efficient sensor Network protocol) for reactive networks. We evaluate the performance of our protocol for a simple temperature sensing application. In terms of energy efficiency, our protocol has been observed to outperform existing conventional sensor network protocols.",what Protocol ?,TEEN,teen,True,True
"In the last few years, several studies have found an inverted-U relationship between per capita income and environmental degradation. This relationship, known as the environmental Kuznets curve (EKC), suggests that environmental degradation increases in the early stages of growth, but it eventually decreases as income exceeds a threshold level. However, this paper investigation relationship between per capita CO2 emission, growth economics and trade liberalization based on econometric techniques of unit root test, co-integration and a panel data set during the period 1960-1996 for BRICS countries. Data properties were analyzed to determine their stationarity using the LLC , IPS , ADF and PP unit root tests which indicated that the series are I(1). We find a cointegration relationship between per capita CO2 emission, growth economics and trade liberalization by applying Kao panel cointegration test. The evidence indi cates that in the long-run trade liberalization has a positive significant impact on CO2 emissions and impact of trade liberalization on emissions growth depends on the level of income Our findings suggest that there is a quadratic relationship between relationship between real GDP and CO2 emissions for the region as a whole. The estimated long-run coefficients of real GDP and its square satisfy the EKC hypothesis in all of studied countries. Our estimation shows that the inflection point or optimal point real GDP per capita is about 5269.4 dollars. The results show that on average, sample countries are on the positive side of the inverted U curve. The turning points are very low in some cases and very high in other cases, hence providing poor evidence in support of the EKC hypothesis. Thus, our findings suggest that all BRICS countries need to sacrifice economic growth to decrease their emission levels",what Methodology ?,Kao Panel,cointegration,False,False
"The objective of this study is to analyse the long-run dynamic relationship of carbon dioxide emissions, real gross domestic product (GDP), the square of real GDP, energy consumption, trade and tourism under an Environmental Kuznets Curve (EKC) model for the Organization for Economic Co-operation and Development (OECD) member countries. Since we find the presence of cross-sectional dependence within the panel time-series data, we apply second-generation unit root tests, cointegration test and causality test which can deal with cross-sectional dependence problems. The cross-sectionally augmented Dickey-Fuller (CADF) and the cross-sectionally augmented Im-Pesaran-Shin (CIPS) unit root tests indicate that the analysed variables become stationary at their first differences. The Lagrange multiplier bootstrap panel cointegration test shows the existence of a long-run relationship between the analysed variables. The dynamic ordinary least squares (DOLS) estimation technique indicates that energy consumption and tourism contribute to the levels of gas emissions, while increases in trade lead to environmental improvements. In addition, the EKC hypothesis cannot be supported as the sign of coefficients on GDP and GDP2 is negative and positive, respectively. Moreover, the Dumitrescu–Hurlin causality tests exploit a variety of causal relationship between the analysed variables. The OECD countries are suggested to invest in improving energy efficiency, regulate necessary environmental protection policies for tourism sector in specific and promote trading activities through several types of encouragement act.",what Methodology ?,DOLS,cointegration,False,False
"This paper reexamines the causality between energy consumption and economic growth with both bivariate and multivariate models by applying the recently developed methods of cointegration and Hsiao`s version of the Granger causality to transformed U.S. data for the period 1947-1990. The Phillips-Perron (PP) tests reveal that the original series are not stationary and, therefore, a first differencing is performed to secure stationarity. The study finds no causal linkages between energy consumption and economic growth. Energy and gross national product (GNP) each live a life of its own. The results of this article are consistent with some of the past studies that find no relationship between energy and GNP but are contrary to some other studies that find GNP unidirectionally causes energy consumption. Both the bivariate and trivariate models produce the similar results. We also find that there is no causal relationship between energy consumption and industrial production. The United States is basically a service-oriented economy and changes in energy consumption can cause little or no changes in GNP. In other words, an implementation of energy conservation policy may not impair economic growth. 27 refs., 5 tabs.",what Methodology ?,Cointegration,granger causality,False,False
"Eye localization is an important part in face recognition system, because its precision closely affects the performance of face recognition. Although various methods have already achieved high precision on the face images with high quality, their precision will drop on low quality images. In this paper, we propose a robust eye localization method for low quality face images to improve the eye detection rate and localization precision. First, we propose a probabilistic cascade (P-Cascade) framework, in which we reformulate the traditional cascade classifier in a probabilistic way. The P-Cascade can give chance to each image patch contributing to the final result, regardless the patch is accepted or rejected by the cascade. Second, we propose two extensions to further improve the robustness and precision in the P-Cascade framework. There are: (1) extending feature set, and (2) stacking two classifiers in multiple scales. Extensive experiments on JAFFE, BioID, LFW and a self-collected video surveillance database show that our method is comparable to state-of-the-art methods on high quality images and can work well on low quality images. This work supplies a solid base for face recognition applications under unconstrained or surveillance environments.",what Methods ?,Probabilistic Cascade ,probabilistic cascade ( p - cascade ),False,True
"Eye localization is an important part in face recognition system, because its precision closely affects the performance of face recognition. Although various methods have already achieved high precision on the face images with high quality, their precision will drop on low quality images. In this paper, we propose a robust eye localization method for low quality face images to improve the eye detection rate and localization precision. First, we propose a probabilistic cascade (P-Cascade) framework, in which we reformulate the traditional cascade classifier in a probabilistic way. The P-Cascade can give chance to each image patch contributing to the final result, regardless the patch is accepted or rejected by the cascade. Second, we propose two extensions to further improve the robustness and precision in the P-Cascade framework. There are: (1) extending feature set, and (2) stacking two classifiers in multiple scales. Extensive experiments on JAFFE, BioID, LFW and a self-collected video surveillance database show that our method is comparable to state-of-the-art methods on high quality images and can work well on low quality images. This work supplies a solid base for face recognition applications under unconstrained or surveillance environments.",what Methods ?,Probabilistic cascade ,probabilistic cascade ( p - cascade ),False,True
"Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most robust, fast and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, this paper proposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-of-the-art performance in the problem of facial feature detection. The code is available at www.humansensing.cs. cmu.edu/intraface.",what Methods ?,SDM,supervised descent method ( sdm ),False,True
"In this paper, we take a look at an enhanced approach for eye detection under difficult acquisition circumstances such as low-light, distance, pose variation, and blur. We present a novel correlation filter based eye detection pipeline that is specifically designed to reduce face alignment errors, thereby increasing eye localization accuracy and ultimately face recognition accuracy. The accuracy of our eye detector is validated using data derived from the Labeled Faces in the Wild (LFW) and the Face Detection on Hard Datasets Competition 2011 (FDHD) sets. The results on the LFW dataset also show that the proposed algorithm exhibits enhanced performance, compared to another correlation filter based detector, and that a considerable increase in face recognition accuracy may be achieved by focusing more effort on the eye localization stage of the face recognition process. Our results on the FDHD dataset show that our eye detector exhibits superior performance, compared to 11 different state-of-the-art algorithms, on the entire set of difficult data without any per set modifications to our detection or preprocessing algorithms. The immediate application of eye detection is automatic face recognition, though many good applications exist in other areas, including medical research, training simulators, communication systems for the disabled, and automotive engineering.",what Methods ?,Novel correlation filter ,correlation filter,False,False
"ABSTRACT On December 31, 2019, the World Health Organization was notified about a cluster of pneumonia of unknown aetiology in the city of Wuhan, China. Chinese authorities later identified a new coronavirus (2019-nCoV) as the causative agent of the outbreak. As of January 23, 2020, 655 cases have been confirmed in China and several other countries. Understanding the transmission characteristics and the potential for sustained human-to-human transmission of 2019-nCoV is critically important for coordinating current screening and containment strategies, and determining whether the outbreak constitutes a public health emergency of international concern (PHEIC). We performed stochastic simulations of early outbreak trajectories that are consistent with the epidemiological findings to date. We found the basic reproduction number, R 0 , to be around 2.2 (90% high density interval 1.4—3.8), indicating the potential for sustained human-to-human transmission. Transmission characteristics appear to be of a similar magnitude to severe acute respiratory syndrome-related coronavirus (SARS-CoV) and the 1918 pandemic influenza. These findings underline the importance of heightened screening, surveillance and control efforts, particularly at airports and other travel hubs, in order to prevent further international spread of 2019-nCoV.",what Methods ?,Stochastic simulations of early outbreak trajectories,stochastic simulations,False,False
"This paper addresses the problem of facial landmark localization and tracking from a single camera. We present a two-stage cascaded deformable shape model to effectively and efficiently localize facial landmarks with large head pose variations. For face detection, we propose a group sparse learning method to automatically select the most salient facial landmarks. By introducing 3D face shape model, we use procrustes analysis to achieve pose-free facial landmark initialization. For deformation, the first step uses mean-shift local search with constrained local model to rapidly approach the global optimum. The second step uses component-wise active contours to discriminatively refine the subtle shape variation. Our framework can simultaneously handle face detection, pose-free landmark localization and tracking in real time. Extensive experiments are conducted on both laboratory environmental face databases and face-in-the-wild databases. All results demonstrate that our approach has certain advantages over state-of-the-art methods in handling pose variations.",what Methods ?,Pose-free,,False,False
"This paper empirically examines the suitability of monetary union in East African community members namely, Burundi, Kenya, Rwanda, Tanzania and Uganda, on the basis of business cycle synchronization. This research considers annual GDP (gross domestic product) data from IMF (international monetary fund) for the period of 1980 to 2010. In order to extract the business cycles and trends, the study uses HP (Hodrick-Prescott) and the BP (band pass) filters. After identifying the cycles and trends of the business cycle, the study considers cross country correlation analysis and analysis of variance technique to examine whether EAC (East African community) countries are characterized by synchronized business cycles or not. The results show that four EAC countries (Burundi, Kenya, Tanzania and Uganda) among five countries are having similar pattern of business cycle and trend from the last ten years of the formation of the EAC. The research concludes that these countries, except Rwanda, do not differ significantly in transitory or cycle components but do differ in permanent components especially in growth trend. Key words: Business cycle synchronization, optimum currency area, East African community, monetary union, development.",what Countries ?,East African Community,"kenya, rwanda, tanzania and uganda,",False,False
"Do changes in monetary policy affect inflation and output in the East African Community (EAC)? We find that (i) Monetary Transmission Mechanism (MTM) tends to be generally weak when using standard statistical inferences, but somewhat strong when using non-standard inference methods; (ii) when MTM is present, the precise transmission channels and their importance differ across countries; and (iii) reserve money and the policy rate, two frequently used instruments of monetary policy, sometimes move in directions that exert offsetting expansionary and contractionary effects on inflation - posing challenges to harmonization of monetary policies across the EAC and transition to a future East African Monetary Union. The paper offers some suggestions for strengthening the MTM in the EAC.",what Countries ?,East African Community,east african community,True,True
"This paper investigates the short-run and long-run causality issues between electricity consumption and economic growth in Turkey by using the co-integration and vector error-correction models with structural breaks. It employs annual data covering the period 1968–2005. The study also explores the causal relationship between these variables in terms of the three error-correction based Granger causality models. The empirical results are as follows: i) Both variables are nonstationary in levels and stationary in the first differences with/without structural breaks, ii) there exists a longrun relationship between variables, iii) there is unidirectional causality running from the electricity consumption to economic growth. The overall results indicate that “growth hypothesis” for electricity consumption and growth nexus holds in Turkey. Thus, energy conservation policies, such as rationing electricity consumption, may harm economic growth in Turkey.",what Countries ?,Turkey,turkey,True,True
"We develop a model in which governments' financing needs exceed the socially optimal level because public resources are diverted to serve the narrow interests of the group in power. From a social welfare perspective, this results in undue pressure on the central bank to extract seigniorage. Monetary policy also suffers from an expansive bias, owing to the authorities' inability to precommit to price stability. Such a conjecture about the fiscal-monetary policy mix appears quite relevant in Africa, with deep implications for the incentives of fiscally heterogeneous countries to form a currency union. We calibrate the model to data for West Africa and use it to assess proposed ECOWAS monetary unions. Fiscal heterogeneity indeed appears critical in shaping regional currency blocs that would be mutually beneficial for all their members. In particular, Nigeria's membership in the configurations currently envisaged would not be in the interests of other ECOWAS countries unless it were accompanied by effective containment on Nigeria's financing needs.",what Countries ?,ECOWAS,"africa,",False,False
"Through a quantitative content analysis, this study applies situational crisis communication theory (SCCT) to investigate how 13 corporate and government organizations responded to the first phase of the 2009 flu pandemic. The results indicate that government organizations emphasized providing instructing information to their primary publics such as guidelines about how to respond to the crisis. On the other hand, organizations representing corporate interests emphasized reputation management in their crisis responses, frequently adopting denial, diminish, and reinforce response strategies. In addition, both government and corporate organizations used social media more often than traditional media in responding to the crisis. Finally, the study expands SCCT's response options.",what Technology ?,social media,social media,True,True
"Social media for emergency management has emerged as a vital resource for government agencies across the globe. In this study, we explore social media strategies employed by governments to respond to major weather-related events. Using social media monitoring software, we analyze how social media is used in six cities following storms in the winter of 2012. We listen, monitor, and assess online discourse available on the full range of social media outlets (e.g., Twitter, Facebook, blogs). To glean further insight, we conduct a survey and extract themes from citizen comments and government's response. We conclude with recommendations on how practitioners can develop social media strategies that enable citizen participation in emergency management.",what Technology ?,social media,social media,True,True
"Distributed group support systems are likely to be widely used in the future as a means for dispersed groups of people to work together through computer networks. They combine characteristics of computer-mediated communication systems with the specialized tools and processes developed in the context of group decision support systems, to provide communications, a group memory, and tools and structures to coordinate the group process and analyze data. These tools and structures can take a wide variety of forms in order to best support computer-mediated interaction for different types of tasks and groups. This article summarizes five case studies of different distributed group support systems developed by the authors and their colleagues over the last decade to support different types of tasks and to accommodate fairly large numbers of participants (tens to hundreds). The case studies are placed within conceptual frameworks that aid in classifying and comparing such systems. The results of the case studies demonstrate that design requirements and the associated research issues for group support systems an be very different in the distributed environment compared to the decision room approach.",what Technology ?,Distributed group support systems,computer networks.,False,False
"The notion of communities getting together during a disaster to help each other is common. However, how does this communal activity happen within the online world? Here we examine this issue using the Communities of Practice (CoP) approach. We extend CoP to multiple CoP (MCoPs) and examine the role of social media applications in disaster management, extending work done by Ahmed (2011). Secondary data in the form of newspaper reports during 2010 to 2011 were analysed to understand how social media, particularly Facebook and Twitter, facilitated the process of communication among various communities during the Queensland floods in 2010. The results of media-content analysis along with the findings of relevant literature were used to extend our existing understanding on various communities of practice involved in disaster management, their communication tasks and the role of Twitter and Facebook as common conducive platforms of communication during disaster management alongside traditional communication channels.",what Technology ?,Facebook and Twitter,social media,False,False
"The project DEKO (Detection of artificial objects in sea areas) is integrated in the German DeMarine-Security project and focuses on the detection and classification of ships and offshore artificial objects relying on TerraSAR-X as well as on RapidEye multispectral optical images. The objectives are 1/ the development of reliable detection algorithms and 2/ the definition of effective, customized service concepts. In addition to an earlier publication, we describe in the following paper some selected results of our work. The algorithms for TerraSAR-X have been extended to a processing chain including all needed steps for ship detection and ship signature analysis, with an emphasis on object segmentation. For Rapid Eye imagery, a ship detection algorithm has been developed. Finally, some applications are described: Ship monitoring in the Strait of Dover based on TerraSAR-X StripMap using AIS information for verification, analyzing TerraSAR-X HighResolution scenes of an industrial harbor and finally an example of surveying a wind farm using change detection.",what Satellite sensor ?,RapidEye,terrasar - x,False,False
"With the increasement of spatial resolution of remote sensing, the ship detection methods for low-resolution images are no longer suitable. In this study, a ship target automatic detection method for high-resolution remote sensing is proposed, which mainly contains steps of Otsu binary segmentation, morphological operation, calculation of target features and target judgment. The results show that almost all of the offshore ships can be detected, and the total detection rates are 94% and 91% with the experimental Google Earth data and GF-1 data respectively. The ship target automatic detection method proposed in this study is more suitable for detecting ship targets offshore rather than anchored along the dock.",what Satellite sensor ?,Google Earth,google earth,True,True
"In this letter, we present a new method to detect inshore ships using shape and context information. We first propose a new energy function based on an active contour model to segment water and land and minimize it with an iterative global optimization method. The proposed energy performs well on the different intensity distributions between water and land and produces a result that can be well used in shape and context analyses. In the segmented image, ships are detected with successive shape analysis, including shape analysis in the localization of ship head and region growing in computing the width and length of ship. Finally, to locate ships accurately and remove the false alarms, we unify them with a binary linear programming problem by utilizing the context information. Experiments on QuickBird images show the robustness and precision of our method.",what Satellite sensor ?,QuickBird,quickbird,True,True
"The idea of exploiting Genetic Programming (GP) to estimate software development effort is based on the observation that the effort estimation problem can be formulated as an optimization problem. Indeed, among the possible models, we have to identify the one providing the most accurate estimates. To this end a suitable measure to evaluate and compare different models is needed. However, in the context of effort estimation there does not exist a unique measure that allows us to compare different models but several different criteria (e.g., MMRE, Pred(25), MdMRE) have been proposed. Aiming at getting an insight on the effects of using different measures as fitness function, in this paper we analyzed the performance of GP using each of the five most used evaluation criteria. Moreover, we designed a Multi-Objective Genetic Programming (MOGP) based on Pareto optimality to simultaneously optimize the five evaluation measures and analyzed whether MOGP is able to build estimation models more accurate than those obtained using GP. The results of the empirical analysis, carried out using three publicly available datasets, showed that the choice of the fitness function significantly affects the estimation accuracy of the models built with GP and the use of some fitness functions allowed GP to get estimation accuracy comparable with the ones provided by MOGP.",what Algorithm(s) ?,MOGP,estimates.,False,False
"The problem known as CAITO refers to the determination of an order to integrate and test classes and aspects that minimizes stubbing costs. Such problem is NP-hard and to solve it efficiently, search based algorithms have been used, mainly evolutionary ones. However, the problem is very complex since it involves different factors that may influence the stubbing process, such as complexity measures, contractual issues and so on. These factors are usually in conflict and different possible solutions for the problem exist. To deal properly with this problem, this work explores the use of multi-objective optimization algorithms. The paper presents results from the application of two evolutionary algorithms - NSGA-II and SPEA2 - to the CAITO problem in four real systems, implemented in AspectJ. Both multi-objective algorithms are evaluated and compared with the traditional Tarjan's algorithm and with a mono-objective genetic algorithm. Moreover, it is shown how the tester can use the found solutions, according to the test goals.",what Algorithm(s) ?,NSGA-II,stubbing costs.,False,False
"The success of the team allocation in a agile software development project is essential. The agile team allocation is a NP-hard problem, since it comprises the allocation of self-organizing and cross-functional teams. Many researchers have driven efforts to apply Computational Intelligence techniques to solve this problem. This work presents a hybrid approach based on NSGA-II multi-objective metaheuristic and Mamdani Fuzzy Inference Systems to solve the agile team allocation problem, together with an initial evaluation of its use in a real environment.",what Algorithm(s) ?,NSGA-II,"team allocation in a agile software development project is essential. the agile team allocation is a np - hard problem, since it comprises the allocation of self - organizing and cross - functional teams.",False,False
"Drug repositioning is the only feasible option to immediately address the COVID-19 global challenge. We screened a panel of 48 FDA-approved drugs against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) which were preselected by an assay of SARS-CoV. We identified 24 potential antiviral drug candidates against SARS-CoV-2 infection. Some drug candidates showed very low 50% inhibitory concentrations (IC 50 s), and in particular, two FDA-approved drugs—niclosamide and ciclesonide—were notable in some respects.",what Has participant ?,SARS-CoV-2,panel,False,False
"A novel coronavirus, named SARS-CoV-2, emerged in 2019 from Hubei region in China and rapidly spread worldwide. As no approved therapeutics exists to treat Covid-19, the disease associated to SARS-Cov-2, there is an urgent need to propose molecules that could quickly enter into clinics. Repurposing of approved drugs is a strategy that can bypass the time consuming stages of drug development. In this study, we screened the Prestwick Chemical Library® composed of 1,520 approved drugs in an infected cell-based assay. 90 compounds were identified. The robustness of the screen was assessed by the identification of drugs, such as Chloroquine derivatives and protease inhibitors, already in clinical trials. The hits were sorted according to their chemical composition and their known therapeutic effect, then EC50 and CC50 were determined for a subset of compounds. Several drugs, such as Azithromycine, Opipramol, Quinidine or Omeprazol present antiviral potency with 2<EC50<20µM. By providing new information on molecules inhibiting SARS-CoV-2 replication in vitro, this study could contribute to the short-term repurposing of drugs against Covid-19.",what Has participant ?,virus,,False,False
"Drug repositioning is the only feasible option to immediately address the COVID-19 global challenge. We screened a panel of 48 FDA-approved drugs against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) which were preselected by an assay of SARS-CoV. We identified 24 potential antiviral drug candidates against SARS-CoV-2 infection. Some drug candidates showed very low 50% inhibitory concentrations (IC 50 s), and in particular, two FDA-approved drugs—niclosamide and ciclesonide—were notable in some respects.",what Has participant ?,antiviral drug,panel,False,False
"The knowledge representation community has built general-purpose ontologies which contain large amounts of commonsense knowledge over relevant aspects of the world, including useful visual information, e.g.: ""a ball is used by a football player"", ""a tennis player is located at a tennis court"". Current state-of-the-art approaches for visual recognition do not exploit these rule-based knowledge sources. Instead, they learn recognition models directly from training examples. In this paper, we study how general-purpose ontologies—specifically, MIT's ConceptNet ontology—can improve the performance of state-of-the-art vision systems. As a testbed, we tackle the problem of sentence-based image retrieval. Our retrieval approach incorporates knowledge from ConceptNet on top of a large pool of object detectors derived from a deep learning technique. In our experiments, we show that ConceptNet can improve performance on a common benchmark dataset. Key to our performance is the use of the ESPGAME dataset to select visually relevant relations from ConceptNet. Consequently, a main conclusion of this work is that general-purpose commonsense ontologies improve performance on visual reasoning tasks when properly filtered to select meaningful visual relations.",what Machine Learning Input ?,image,performance,False,False
"Deep neural networks (DNNs) have become the gold standard for solving challenging classification problems, especially given complex sensor inputs (e.g., images and video). While DNNs are powerful, they are also brittle, and their inner workings are not fully understood by humans, leading to their use as “black-box” models. DNNs often generalize poorly when provided new data sampled from slightly shifted distributions; DNNs are easily manipulated by adversarial examples; and the decision-making process of DNNs can be difficult for humans to interpret. To address these challenges, we propose integrating DNNs with external sources of semantic knowledge. Large quantities of meaningful, formalized knowledge are available in knowledge graphs and other databases, many of which are publicly obtainable. But at present, these sources are inaccessible to deep neural methods, which can only exploit patterns in the signals they are given to classify. In this work, we conduct experiments on the ADE20K dataset, using scene classification as an example task where combining DNNs with external knowledge graphs can result in more robust and explainable models. We align the atomic concepts present in ADE20K (i.e., objects) to WordNet, a hierarchically-organized lexical database. Using this knowledge graph, we expand the concept categories which can be identified in ADE20K and relate these concepts in a hierarchical manner. The neural architecture we present performs scene classification using these concepts, illuminating a path toward DNNs which can efficiently exploit high-level knowledge in place of excessive quantities of direct sensory input. We hypothesize and experimentally validate that incorporating background knowledge via an external knowledge graph into a deep learning-based model should improve the explainability and robustness of the model.",what Machine Learning Input ?,image,external,False,False
"We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a key-value memory, in a cloze-style setting. Instead of relying only on document-to-question interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.",what Machine Learning Input ?,text,,False,False
"A wireless network consisting of a large number of small sensors with low-power transceivers can be an effective tool for gathering data in a variety of environments. The data collected by each sensor is communicated through the network to a single processing center that uses all reported data to determine characteristics of the environment or detect an event. The communication or message passing process must be designed to conserve the limited energy resources of the sensors. Clustering sensors into groups, so that sensors communicate information only to clusterheads and then the clusterheads communicate the aggregated information to the processing center, may save energy. In this paper, we propose a distributed, randomized clustering algorithm to organize the sensors in a wireless sensor network into clusters. We then extend this algorithm to generate a hierarchy of clusterheads and observe that the energy savings increase with the number of levels in the hierarchy. Results in stochastic geometry are used to derive solutions for the values of parameters of our algorithm that minimize the total energy spent in the network when all sensors report data through the clusterheads to the processing center.",what Method ?,Distributed,"distributed, randomized",False,True
"Science communication only reaches certain segments of society. Various underserved audiences are detached from it and feel left out, which is a challenge for democratic societies that build on informed participation in deliberative processes. While only recently researchers and practitioners have addressed the question on the detailed composition of the not reached groups, even less is known about the emotional impact on underserved audiences: feelings and emotions can play an important role in how science communication is received, and “feeling left out” can be an important aspect of exclusion. In this exploratory study, we provide insights from interviews and focus groups with three different underserved audiences in Germany. We found that on the one hand, material exclusion factors such as available infrastructure or financial means as well as specifically attributable factors such as language skills, are influencing the audience composition of science communication. On the other hand, emotional exclusion factors such as fear, habitual distance, and self- as well as outside-perception also play an important role. Therefore, simply addressing material aspects can only be part of establishing more inclusive science communication practices. Rather, being aware of emotions and feelings can serve as a point of leverage for science communication in reaching out to underserved audiences.",what Method ?,interviews,focus groups,False,False
"The ability to extract topological regularity out of large randomly deployed sensor networks holds the promise to maximally leverage correlation for data aggregation and also to assist with sensor localization and hierarchy creation. This paper focuses on extracting such regular structures from physical topology through the development of a distributed clustering scheme. The topology adaptive spatial clustering (TASC) algorithm presented here is a distributed algorithm that partitions the network into a set of locally isotropic, non-overlapping clusters without prior knowledge of the number of clusters, cluster size and node coordinates. This is achieved by deriving a set of weights that encode distance measurements, connectivity and density information within the locality of each node. The derived weights form the terrain for holding a coordinated leader election in which each node selects the node closer to the center of mass of its neighborhood to become its leader. The clustering algorithm also employs a dynamic density reachability criterion that groups nodes according to their neighborhood's density properties. Our simulation results show that the proposed algorithm can trace locally isotropic structures in non-isotropic network and cluster the network with respect to local density attributes. We also found out that TASC exhibits consistent behavior in the presence of moderate measurement noise levels",what Method ?,Distributed,distributed,True,True
"Th e aim of this study is to define whether using exercise-based games increase the performance of learning. For this reason, two basic questions were tried to be answered in the study. First, is there any diff erence in learning between the group that was given exercisebased games and the group that was not? Second, is there any diff erence in learning between the group that used exercise-based games at end of the process of learning and the group that was not applied this but taken the questions of exercises in game material? Th is research has been conducted within the subject of Testing and Evaluation in the program of Kocaeli University Primary Maths Teacher’s College. Experimental design with a pre test-post test control group was used in this study. Experimental process based on game material was used in 120 minutes at the end of a 3-week-teaching period. Th e reliability values (KR-20) of the two tests were found to be .79 and .71 which were used to evaluate learning level. Th e study has reached a conclusion that game materials used at the end of learning process have increased the learning levels of teacher candidates. However, the similar learning levels have been observed among students who were taken printed exercises instead of using learning game method to reinforce the traditional learning in the research. Th is means that in method of applying teaching games in addition to the traditional teaching, there isn’t any diff erence of learning eff iciency of students answered the questions based on competition and fun and the group who only answered the questions. Th is study is expected to contribute defining in which situations games are eff ective.",what Method ?,experiment,experimental design with a pre test - post test control group,False,True
"Dense stereo correspondence enabling reconstruction of depth information in a scene is of great importance in the field of computer vision. Recently, some local solutions based on matching cost filtering with an edge-preserving filter have been proved to be capable of achieving more accuracy than global approaches. Unfortunately, the computational complexity of these algorithms is quadratically related to the window size used to aggregate the matching costs. The recent trend has been to pursue higher accuracy with greater efficiency in execution. Therefore, this paper proposes a new cost-aggregation module to compute the matching responses for all the image pixels at a set of sampling points generated by a hierarchical clustering algorithm. The complexity of this implementation is linear both in the number of image pixels and the number of clusters. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art local methods in terms of both accuracy and speed. Moreover, performance tests indicate that parameters such as the height of the hierarchical binary tree and the spatial and range standard deviations have a significant influence on time consumption and the accuracy of disparity maps.",what Method ?,Local,dense stereo correspondence,False,False
"Adaptive support weight (ASW) methods represent the state of the art in local stereo matching, while the bilateral filter-based ASW method achieves outstanding performance. However, this method fails to resolve the ambiguity induced by nearby pixels at different disparities but with similar colors. In this paper, we introduce a novel trilateral filter (TF)-based ASW method that remedies such ambiguities by considering the possible disparity discontinuities through color discontinuity boundaries, i.e., the boundary strength between two pixels, which is measured by a local energy model. We also present a recursive TF-based ASW method whose computational complexity is O(N) for the cost aggregation step, and O(NLog2(N)) for boundary detection, where N denotes the input image size. This complexity is thus independent of the support window size. The recursive TF-based method is a nonlocal cost aggregation strategy. The experimental evaluation on the Middlebury benchmark shows that the proposed method, whose average error rate is 4.95%, outperforms other local methods in terms of accuracy. Equally, the average runtime of the proposed TF-based cost aggregation is roughly 260 ms on a 3.4-GHz Inter Core i7 CPU, which is comparable with state-of-the-art efficiency.",what Method ?,Local,,False,False
"Abstract The problem of deciding an optimal fleet (the type of ships and the number of each type) in a real liner shipping problem is considered. The liner shipping problem is a multi-trip vehicle routing problem, and consists of deciding weekly routes for the selected ships. A solution method consisting of three phases is presented. In phase 1, all feasible single routes are generated for the largest ship available. Some of these routes will use only a small portion of the ship’s capacity and can be performed by smaller ships at less cost. This fact is used when calculating the cost of each route. In phase 2, the single routes generated in phase 1 are combined into multiple routes. By solving a set partitioning problem (phase 3), where the columns are the routes generated in phases 1 and 2, we find both the optimal fleet and the coherent routes for the fleet.",what Method ?,set partitioning,set partitioning problem,False,True
"Central to the cluster-based routing protocols is the cluster head (CH) selection procedure that allows even distribution of energy consumption among the sensors, and therefore prolonging the lifespan of a sensor network. We propose a distributed CH selection algorithm that takes into account the distances from sensors to a base station that optimally balances the energy consumption among the sensors. NS-2 simulations show that our proposed scheme outperforms existing algorithms in terms of the average node lifespan and the time to first node death.",what Method ?,Distributed,distributed,True,True
"One challenge in field-based marine microbial ecology is to achieve sufficient spatial resolution to obtain representative information about microbial distributions and biogeochemical processes. The challenges are exacerbated when conducting rate measurements of biological processes due to potential perturbations during sampling and incubation. Here we present the first application of a robotic microlaboratory, the 4 L-submersible incubation device (SID), for conducting in situ measurements of the rates of biological nitrogen (N2) fixation (BNF). The free-drifting autonomous instrument obtains samples from the water column that are incubated in situ after the addition of 15N2 tracer. After each of up to four consecutive incubation experiments, the 4-L sample is filtered and chemically preserved. Measured BNF rates from two deployments of the SID in the oligotrophic North Pacific ranged from 0.8 to 2.8 nmol N L?1 day?1, values comparable with simultaneous rate measurements obtained using traditional conductivity–temperature–depth (CTD)–rosette sampling followed by on-deck or in situ incubation. Future deployments of the SID will help to better resolve spatial variability of oceanic BNF, particularly in areas where recovery of seawater samples by CTD compromises their integrity, e.g. anoxic habitats.",what Method ?,Submersible incubation device,robotic,False,False
"Background. The epidemic outbreak cased by coronavirus 2019-nCoV is of great interest to researches because of the high rate of spread of the infection and the significant number of fatalities. A detailed scientific analysis of the phenomenon is yet to come, but the public is already interested in the questions of the duration of the epidemic, the expected number of patients and deaths. For long time predictions, the complicated mathematical models are necessary which need many efforts for unknown parameters identification and calculations. In this article, some preliminary estimates will be presented. Objective. Since the reliable long time data are available only for mainland China, we will try to predict the epidemic characteristics only in this area. We will estimate some of the epidemic characteristics and present the most reliable dependences for victim numbers, infected and removed persons versus time. Methods. In this study we use the known SIR model for the dynamics of an epidemic, the known exact solution of the linear equations and statistical approach developed before for investigation of the children disease, which occurred in Chernivtsi (Ukraine) in 1988-1989. Results. The optimal values of the SIR model parameters were identified with the use of statistical approach. The numbers of infected, susceptible and removed persons versus time were predicted. Conclusions. Simple mathematical model was used to predict the characteristics of the epidemic caused by coronavirus 2019-nCoV in mainland China. The further research should focus on updating the predictions with the use of fresh data and using more complicated mathematical models.",what Method ?,SIR model,sir model,True,True
"Abstract Since its inception in 2007, DBpedia has been constantly releasing open data in RDF, extracted from various Wikimedia projects using a complex software system called the DBpedia Information Extraction Framework (DIEF). For the past 12 years, the software received a plethora of extensions by the community, which positively affected the size and data quality. Due to the increase in size and complexity, the release process was facing huge delays (from 12 to 17 months cycle), thus impacting the agility of the development. In this paper, we describe the new DBpedia release cycle including our innovative release workflow, which allows development teams (in particular those who publish large, open data) to implement agile, cost-efficient processes and scale up productivity. The DBpedia release workflow has been re-engineered, its new primary focus is on productivity and agility , to address the challenges of size and complexity. At the same time, quality is assured by implementing a comprehensive testing methodology. We run an experimental evaluation and argue that the implemented measures increase agility and allow for cost-effective quality-control and debugging and thus achieve a higher level of maintainability. As a result, DBpedia now publishes regular (i.e. monthly) releases with over 21 billion triples with minimal publishing effort .",what Method ?,Release cycle,testing,False,False
"CUBIC is a congestion control protocol for TCP (transmission control protocol) and the current default TCP algorithm in Linux. The protocol modifies the linear window growth function of existing TCP standards to be a cubic function in order to improve the scalability of TCP over fast and long distance networks. It also achieves more equitable bandwidth allocations among flows with different RTTs (round trip times) by making the window growth to be independent of RTT -- thus those flows grow their congestion window at the same rate. During steady state, CUBIC increases the window size aggressively when the window is far from the saturation point, and the slowly when it is close to the saturation point. This feature allows CUBIC to be very scalable when the bandwidth and delay product of the network is large, and at the same time, be highly stable and also fair to standard TCP flows. The implementation of CUBIC in Linux has gone through several upgrades. This paper documents its design, implementation, performance and evolution as the default TCP algorithm of Linux.",what Method ?,Congestion Window,linear window growth function,False,False
"This paper addresses a fleet-sizing problem in the context of the truck-rental industry. Specifically, trucks that vary in capacity and age are utilized over space and time to meet customer demand. Operational decisions (including demand allocation and empty truck repositioning) and tactical decisions (including asset procurements and sales) are explicitly examined in a linear programming model to determine the optimal fleet size and mix. The method uses a time-space network, common to fleet-management problems, but also includes capital cost decisions, wherein assets of different ages carry different costs, as is common to replacement analysis problems. A two-phase solution approach is developed to solve large-scale instances of the problem. Phase I allocates customer demand among assets through Benders decomposition with a demand-shifting algorithm assuring feasibility in each subproblem. Phase II uses the initial bounds and dual variables from Phase I and further improves the solution convergence without increasing computer memory requirements through the use of Lagrangian relaxation. Computational studies are presented to show the effectiveness of the approach for solving large problems within reasonable solution gaps.",what Method ?,Benders decomposition,linear programming,False,False
"Although plastic is considered an indispensable commodity, plastic pollution is a major concern around the world due to its rapid accumulation rate, complexity, and lack of management. Some political policies, such as the Chinese import ban on plastic waste, force us to think about a long-term solution to eliminate plastic wastes. Converting waste plastics into liquid and gaseous fuels is considered a promising technique to eliminate the harm to the environment and decrease the dependence on fossil fuels, and recycling waste plastic by converting it into monomers is another effective solution to the plastic pollution problem. This paper presents the critical situation of plastic pollution, various methods of plastic depolymerization based on different kinds of polymers defined in the Society of the Plastics Industry (SPI) Resin Identification Coding System, and the opportunities and challenges in the future.",what Method ?,depolymerization,,False,False
"A multinational company uses a personal computer to schedule a fleet of coastal tankers and barges transporting liquid bulk products among plants, distribution centres (tank farms), and industrial customers. A simple spreadsheet interface cloaks a sophisticated optimization-based decision support system and makes this system useable via a varity of natural languages. The dispatchers, whose native language is not English, and some of whom presumably speak no English at all, communicate via the spreadsheet, and view recommended schedules displayed in Gantt charts both internationally familiar tools. Inside the spreadsheet, a highly detailed simulation can generate every feasible alternate vessel employment schedule, and an integer linear set partitioning model selects one schedule for each vessel so that all loads and deliveries are completed at minimal cost while satisfying all operational requirements. The optimized fleet employment schedule is displyed graphically with hourly time resolution over a planning horizon of 2-3 weeks. Each vessel will customarily make several voyages and many port calls to load and unload products during this time.",what Method ?,Simulation,spreadsheet,False,False
"Applications such as robot navigation and augmented reality require high-accuracy dense disparity maps in real-time and online. Due to time constraint, most realtime stereo applications rely on local winner-take-all optimization in the disparity computation process. These local approaches are generally outperformed by offline global optimization based algorithms. However, recent research shows that, through carefully selecting and aggregating the matching costs of neighboring pixels, the disparity maps produced by a local approach can be more accurate than those generated by many global optimization techniques. We are therefore motivated to investigate whether these cost aggregation approaches can be adopted in real-time stereo applications and, if so, how well they perform under the real-time constraint. The evaluation is conducted on a real-time stereo platform, which utilizes the processing power of programmable graphics hardware. Several recent cost aggregation approaches are also implemented and optimized for graphics hardware so that real-time speed can be achieved. The performances of these aggregation approaches in terms of both processing speed and result quality are reported.",what Method ?,Local,offline global optimization,False,False
"With the rapid growth of online social media content, and the impact these have made on people’s behavior, many researchers have been interested in studying these media platforms. A major part of their work focused on sentiment analysis and opinion mining. These refer to the automatic identification of opinions of people toward specific topics by analyzing their posts and publications. Multi-class sentiment analysis, in particular, addresses the identification of the exact sentiment conveyed by the user rather than the overall sentiment polarity of his text message or post. That being the case, we introduce a task different from the conventional multi-class classification, which we run on a data set collected from Twitter. We refer to this task as “quantification.” By the term “quantification,” we mean the identification of all the existing sentiments within an online post (i.e., tweet) instead of attributing a single sentiment label to it. For this sake, we propose an approach that automatically attributes different scores to each sentiment in a tweet, and selects the sentiments with the highest scores which we judge as conveyed in the text. To reach this target, we added to our previously introduced tool SENTA the necessary components to run and perform such a task. Throughout this work, we present the added components; we study the feasibility of quantification, and propose an approach to perform it on a data set made of tweets for 11 different sentiment classes. The data set was manually labeled and the results of the automatic analysis were checked against the human annotation. Our experiments show the feasibility of this task and reach an F1 score equal to 45.9%.",what Method ?,conventional multi-class classification,automatic,False,False
"Automatic subject indexing has been a longstanding goal of digital curators to facilitate effective retrieval access to large collections of both online and offline information resources. Controlled vocabularies are often used for this purpose, as they standardise annotation practices and help users to navigate online resources through following interlinked topical concepts. However, to this date, the assignment of suitable text annotations from a controlled vocabulary is still largely done manually, or at most (semi-)automatically, even though effective machine learning tools are already in place. This is because existing procedures require a sufficient amount of training data and they have to be adapted to each vocabulary, language and application domain anew. In this paper, we argue that there is a third solution to subject indexing which harnesses cross-domain knowledge graphs. Our KINDEX approach fuses distributed knowledge graph information from different sources. Experimental evaluation shows that the approach achieves good accuracy scores by exploiting correspondence links of publicly available knowledge graphs.",what Method ?,KINDEX approach,kindex,False,False
"The objective of this paper is to suggest practical optimization models for routing strategies for liner fleets. Many useful routing and scheduling problems have been studied in the transportation literature. As for ship scheduling or routing problems, relatively less effort has been devoted, in spite of the fact that sea transportation involves large capital and operating costs. This paper suggests two optimization models that can be useful to liner shipping companies. One is a linear programming model of profit maximization, which provides an optimal routing mix for each ship available and optimal service frequencies for each candidate route. The other model is a mixed integer programming model with binary variables which not only provides optimal routing mixes and service frequencies but also best capital investment alternatives to expand fleet capacity. This model is a cost minimization model.",what Method ?,Linear programming,linear programming,True,True
"Science communication only reaches certain segments of society. Various underserved audiences are detached from it and feel left out, which is a challenge for democratic societies that build on informed participation in deliberative processes. While only recently researchers and practitioners have addressed the question on the detailed composition of the not reached groups, even less is known about the emotional impact on underserved audiences: feelings and emotions can play an important role in how science communication is received, and “feeling left out” can be an important aspect of exclusion. In this exploratory study, we provide insights from interviews and focus groups with three different underserved audiences in Germany. We found that on the one hand, material exclusion factors such as available infrastructure or financial means as well as specifically attributable factors such as language skills, are influencing the audience composition of science communication. On the other hand, emotional exclusion factors such as fear, habitual distance, and self- as well as outside-perception also play an important role. Therefore, simply addressing material aspects can only be part of establishing more inclusive science communication practices. Rather, being aware of emotions and feelings can serve as a point of leverage for science communication in reaching out to underserved audiences.",what Method ?,focus group,focus groups,False,True
"Background: Estimating key infectious disease parameters from the COVID-19 outbreak is quintessential for modelling studies and guiding intervention strategies. Whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for COVID-19 have not been provided. Methods: We used outbreak data from clusters in Singapore and Tianjin, China to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. From those estimates we obtained the proportions pre-symptomatic transmission and reproduction numbers. Results: The mean generation interval was 5.20 (95%CI 3.78-6.78) days for Singapore and 3.95 (95%CI 3.01-4.91) days for Tianjin, China when relying on a previously reported incubation period with mean 5.2 and SD 2.8 days. The proportion of pre-symptomatic transmission was 48% (95%CI 32-67%) for Singapore and 62% (95%CI 50-76%) for Tianjin, China. Estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution. Conclusions: Estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network. Detailed contact tracing information is essential for correctly estimating these quantities.",what Method ?,serial interval,serial interval,True,True
"With the rapid growth of online social media content, and the impact these have made on people’s behavior, many researchers have been interested in studying these media platforms. A major part of their work focused on sentiment analysis and opinion mining. These refer to the automatic identification of opinions of people toward specific topics by analyzing their posts and publications. Multi-class sentiment analysis, in particular, addresses the identification of the exact sentiment conveyed by the user rather than the overall sentiment polarity of his text message or post. That being the case, we introduce a task different from the conventional multi-class classification, which we run on a data set collected from Twitter. We refer to this task as “quantification.” By the term “quantification,” we mean the identification of all the existing sentiments within an online post (i.e., tweet) instead of attributing a single sentiment label to it. For this sake, we propose an approach that automatically attributes different scores to each sentiment in a tweet, and selects the sentiments with the highest scores which we judge as conveyed in the text. To reach this target, we added to our previously introduced tool SENTA the necessary components to run and perform such a task. Throughout this work, we present the added components; we study the feasibility of quantification, and propose an approach to perform it on a data set made of tweets for 11 different sentiment classes. The data set was manually labeled and the results of the automatic analysis were checked against the human annotation. Our experiments show the feasibility of this task and reach an F1 score equal to 45.9%.",what Method ?,Multi-class sentiment analysis,automatic,False,False
"This article describes an experiment to measure the impact of open access (OA) publishing of academic books. During a period of nine months, three sets of 100 books were disseminated through an institutional repository, the Google Book Search program, or both channels. A fourth set of 100 books was used as control group. OA publishing enhances discovery and online consultation. Within the context of the experiment, no relation could be found between OA publishing and citation rates. Contrary to expectations, OA publishing does not stimulate or diminish sales figures. The Google Book Search program is superior to the repository.",what Method ?,experiment,experiment,True,True
"Low-Energy Adaptive Clustering Hierarchy (LEACH) is one of the most popular distributed cluster-based routing protocols in wireless sensor networks. Clustering algorithm of the LEACH is simple but offers no guarantee about even distribution of cluster heads over the network. And it assumes that each cluster head transmits data to sink over a single hop. In this paper, we propose a new method for selecting cluster heads to evenly distribute cluster heads. It avoids creating redundant cluster heads within a small geographical range. Simulation results show that our scheme reduces energy dissipation and prolongs network lifetime as compared with LEACH.",what Method ?,Distributed,leach ),False,False
"Global optimisation algorithms for stereo dense depth map estimation have demonstrated how to outperform other stereo algorithms such as local methods or dynamic programming. The energy minimisation framework, using Markov random fields model and solved using graph cuts or belief propagation, has especially obtained good results. The main drawback of these methods is that, although they achieve accurate reconstruction, they are not suited for real-time applications. Subsampling the input images does not reduce the complexity of the problem because it also reduces the resolution of the output in the disparity space. Nonetheless, some real-time applications such as navigation would tolerate the reduction of the depth map resolutions (width and height) while maintaining the resolution in the disparity space (number of labels). In this study a new multiresolution energy minimisation framework for real-time robotics applications is proposed where a global optimisation algorithm is applied. A reduction by a factor R of the final depth map's resolution is considered and a speed of up to 50 times has been achieved. Using high-resolution stereo pair input images guarantees that a high resolution on the disparity dimension is preserved. The proposed framework has shown how to obtain real-time performance while keeping accurate results in the Middlebury test data set.",what Method ?,Global,markov,False,False
"The outbreak of coronavirus disease 2019 (COVID-19) which originated in Wuhan, China, constitutes a public health emergency of international concern with a very high risk of spread and impact at the global level. We developed data-driven susceptible-exposed-infectious-quarantine-recovered (SEIQR) models to simulate the epidemic with the interventions of social distancing and epicenter lockdown. Population migration data combined with officially reported data were used to estimate model parameters, and then calculated the daily exported infected individuals by estimating the daily infected ratio and daily susceptible population size. As of Jan 01, 2020, the estimated initial number of latently infected individuals was 380.1 (95%-CI: 379.8~381.0). With 30 days of substantial social distancing, the reproductive number in Wuhan and Hubei was reduced from 2.2 (95%-CI: 1.4~3.9) to 1.58 (95%-CI: 1.34~2.07), and in other provinces from 2.56 (95%-CI: 2.43~2.63) to 1.65 (95%-CI: 1.56~1.76). We found that earlier intervention of social distancing could significantly limit the epidemic in mainland China. The number of infections could be reduced up to 98.9%, and the number of deaths could be reduced by up to 99.3% as of Feb 23, 2020. However, earlier epicenter lockdown would partially neutralize this favorable effect. Because it would cause in situ deteriorating, which overwhelms the improvement out of the epicenter. To minimize the epidemic size and death, stepwise implementation of social distancing in the epicenter city first, then in the province, and later the whole nation without the epicenter lockdown would be practical and cost-effective.",what Method ?,data-driven susceptible-exposed-infectious-quarantine-recovered (SEIQR) models,seiqr ),False,False
"We present a ship scheduling problem concerned with the pickup and delivery of bulk cargoes within given time windows. As the ports are closed for service at night and during weekends, the wide time windows can be regarded as multiple time windows. Another issue is that the loading/discharging times of cargoes may take several days. This means that a ship will stay idle much of the time in port, and the total time at port will depend on the ship's arrival time. Ship scheduling is associated with uncertainty due to bad weather at sea and unpredictable service times in ports. Our objective is to make robust schedules that are less likely to result in ships staying idle in ports during the weekend, and impose penalty costs for arrivals at risky times (i.e., close to weekends). A set partitioning approach is proposed to solve the problem. The columns correspond to feasible ship schedules that are found a priori. They are generated taking the uncertainty and multiple time windows into account. The computational results show that we can increase the robustness of the schedules at the sacrifice of increased transportation costs. © 2002 Wiley Periodicals, Inc. Naval Research Logistics 49: 611–625, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/nav.10033",what Method ?,set partitioning,partitioning,False,False
"A large community of research has been developed in recent years to analyze social media and social networks, with the aim of understanding, discovering insights, and exploiting the available information. The focus has shifted from conventional polarity classification to contemporary application-oriented fine-grained aspects such as, emotions, sarcasm, stance, rumor, and hate speech detection in the user-generated content. Detecting a sarcastic tone in natural language hinders the performance of sentiment analysis tasks. The majority of the studies on automatic sarcasm detection emphasize on the use of lexical, syntactic, or pragmatic features that are often unequivocally expressed through figurative literary devices such as words, emoticons, and exclamation marks. In this paper, we propose a deep learning model called sAtt-BLSTM convNet that is based on the hybrid of soft attention-based bidirectional long short-term memory (sAtt-BLSTM) and convolution neural network (convNet) applying global vectors for word representation (GLoVe) for building semantic word embeddings. In addition to the feature maps generated by the sAtt-BLSTM, punctuation-based auxiliary features are also merged into the convNet. The robustness of the proposed model is investigated using balanced (tweets from benchmark SemEval 2015 Task 11) and unbalanced (approximately 40000 random tweets using the Sarcasm Detector tool with 15000 sarcastic and 25000 non-sarcastic messages) datasets. An experimental study using the training- and test-set accuracy metrics is performed to compare the proposed deep neural model with convNet, LSTM, and bidirectional LSTM with/without attention and it is observed that the novel sAtt-BLSTM convNet model outperforms others with a superior sarcasm-classification accuracy of 97.87% for the Twitter dataset and 93.71% for the random-tweet dataset.",what Method ?,Sarcasm Detector tool,sarcasm detector tool,True,True
"Since 1995, Dikerogammarus villosus Sowinski, a Ponto-Caspian amphi- pod species, has been invading most of Western Europe' s hydrosystems. D. villosus geographic extension and quickly increasing population density has enabled it to become a major component of macrobenthic assemblages in recipient ecosystems. The ecological characteristics of D. villosus on a mesohabitat scale were investigated at a station in the Moselle River. This amphipod is able to colonize a wide range of sub- stratum types, thus posing a threat to all freshwater ecosystems. Rivers whose domi- nant substratum is cobbles and which have tree roots along the banks could harbour particularly high densities of D. villosus. A relationship exists between substratum par- ticle size and the length of the individuals, and spatial segregation according to length was shown. This allows the species to limit intra-specific competition between genera- tions while facilitating reproduction. A strong association exists between D. villosus and other Ponto-Caspian species, such as Dreissena polymorpha and Corophium cur- vispinum, in keeping with Invasional Meltdown Theory. Four taxa (Coenagrionidae, Calopteryx splendens, Corophium curvispinum and Gammarus pulex ) exhibited spa- tial niches that overlap significantly that of D. villosus. According to the predatory be- haviour of the newcomer, their populations may be severely impacted.",what Habitat ?,Freshwater,,False,False
"Many introduced species have negative impacts on native species, but some develop positive interactions with both native species and other invaders. Facilitation between invaders may lead to an overall acceleration in invasion success and impacts. Mechanisms of facilitation include habitat alteration, or ecosystem engineering, and trophic interactions. In marine systems, only a handful of positive effects have been reported for invading species. In an unusual NE Pacific marine assemblage dominated by 5 conspicuous invaders and 2 native species, we identified positive effects of the most abundant invader, the Asian hornsnail Batillaria attramentaria, on all other species. B. attramentaria reached densities >1400 m -2 , providing an average of 600 cm of hard substrate per m 2 on this mudflat. Its shells were used as habitat almost exclusively by the introduced Atlantic slipper shell Crepidula convexa, the introduced Asian anemone Diadumene lineata, and 2 native hermit crabs Pagurus hirsutiusculus and P. granosimanus. In addition, manipulative experiments showed that the abundance of the mudsnail Nassarius fraterculus and percentage cover of the eelgrass Zostera japonica, both introduced from the NW Pacific, increased significantly in the presence of B. attramentaria. The most likely mechanisms for these facilitations are indirect grazing effects and bioturbation, respectively. Since the precise arrival dates of all these invaders are unknown, the role of B. attramentaria's positive interactions in their initial invasion success is unknown. Nevertheless, by providing habitat for 2 non-native epibionts and 2 native species, and by facilitating 2 other invaders, the non-native B. attramentaria enhances the level of invasion by all 6 species.",what Habitat ?,Marine,1400,False,False
"Besides exacerbated exploitation, pollution, flow alteration and habitats degradation, freshwater biodiversity is also threatened by biological invasions. This paper addresses how native aquatic macrophyte communities are affected by the non-native species Urochloa arrecta, a current successful invader in Brazilian freshwater systems. We compared the native macrophytes colonizing patches dominated and non-dominated by this invader species. We surveyed eight streams in Northwest Parana State (Brazil). In each stream, we recorded native macrophytes' richness and biomass in sites where U. arrecta was dominant and in sites where it was not dominant or absent. No native species were found in seven, out of the eight investigated sites where U. arrecta was dominant. Thus, we found higher native species richness, Shannon index and native biomass values in sites without dominance of U. arrecta than in sites dominated by this invader. Although difficult to conclude about causes of such differences, we infer that the elevated biomass production by this grass might be the primary reason for alterations in invaded environments and for the consequent impacts on macrophytes' native communities. However, biotic resistance offered by native richer sites could be an alternative explanation for our results. To mitigate potential impacts and to prevent future environmental perturbations, we propose mechanical removal of the invasive species and maintenance or restoration of riparian vegetation, for freshwater ecosystems have vital importance for the maintenance of ecological services and biodiversity and should be preserved.",what Habitat ?,Freshwater,shannon index,False,False
"Despite long-standing interest of terrestrial ecologists, freshwater ecosystems are a fertile, yet unappreciated, testing ground for applying community phylogenetics to uncover mechanisms of species assembly. We quantify phylogenetic clustering and overdispersion of native and non-native fishes of a large river basin in the American Southwest to test for the mechanisms (environmental filtering versus competitive exclusion) and spatial scales influencing community structure. Contrary to expectations, non-native species were phylogenetically clustered and related to natural environmental conditions, whereas native species were not phylogenetically structured, likely reflecting human-related changes to the basin. The species that are most invasive (in terms of ecological impacts) tended to be the most phylogenetically divergent from natives across watersheds, but not within watersheds, supporting the hypothesis that Darwin's naturalization conundrum is driven by the spatial scale. Phylogenetic distinctiveness may facilitate non-native establishment at regional scales, but environmental filtering restricts local membership to closely related species with physiological tolerances for current environments. By contrast, native species may have been phylogenetically clustered in historical times, but species loss from contemporary populations by anthropogenic activities has likely shaped the phylogenetic signal. Our study implies that fundamental mechanisms of community assembly have changed, with fundamental consequences for the biogeography of both native and non-native species.",what Habitat ?,Freshwater,,False,False
"In recent decades the grass Phragmites australis has been aggressively in- vading coastal, tidal marshes of North America, and in many areas it is now considered a nuisance species. While P. australis has historically been restricted to the relatively benign upper border of brackish and salt marshes, it has been expanding seaward into more phys- iologically stressful regions. Here we test a leading hypothesis that the spread of P. australis is due to anthropogenic modification of coastal marshes. We did a field experiment along natural borders between stands of P. australis and the other dominant grasses and rushes (i.e., matrix vegetation) in a brackish marsh in Rhode Island, USA. We applied a pulse disturbance in one year by removing or not removing neighboring matrix vegetation and adding three levels of nutrients (specifically nitrogen) in a factorial design, and then we monitored the aboveground performance of P. australis and the matrix vegetation. Both disturbances increased the density, height, and biomass of shoots of P. australis, and the effects of fertilization were more pronounced where matrix vegetation was removed. Clear- ing competing matrix vegetation also increased the distance that shoots expanded and their reproductive output, both indicators of the potential for P. australis to spread within and among local marshes. In contrast, the biomass of the matrix vegetation decreased with increasing severity of disturbance. Disturbance increased the total aboveground production of plants in the marsh as matrix vegetation was displaced by P. australis. A greenhouse experiment showed that, with increasing nutrient levels, P. australis allocates proportionally more of its biomass to aboveground structures used for spread than to belowground struc- tures used for nutrient acquisition. Therefore, disturbances that enrich nutrients or remove competitors promote the spread of P. australis by reducing belowground competition for nutrients between P. australis and the matrix vegetation, thus allowing P. australis, the largest plant in the marsh, to expand and displace the matrix vegetation. Reducing nutrient load and maintaining buffers of matrix vegetation along the terrestrial-marsh ecotone will, therefore, be important methods of control for this nuisance species.",what Habitat ?,Terrestrial,plants,False,False
"Summary Biological invasions threaten ecosystem integrity and biodiversity, with numerous adverse implications for native flora and fauna. Established populations of two notorious freshwater invaders, the snail Tarebia granifera and the fish Pterygoplichthys disjunctivus, have been reported on three continents and are frequently predicted to be in direct competition with native species for dietary resources. Using comparisons of species' isotopic niche widths and stable isotope community metrics, we investigated whether the diets of the invasive T. granifera and P. disjunctivus overlapped with those of native species in a highly invaded river. We also attempted to resolve diet composition for both species, providing some insight into the original pathway of invasion in the Nseleni River, South Africa. Stable isotope metrics of the invasive species were similar to or consistently mid-range in comparison with their native counterparts, with the exception of markedly more uneven spread in isotopic space relative to indigenous species. Dietary overlap between the invasive P. disjunctivus and native fish was low, with the majority of shared food resources having overlaps of <0.26. The invasive T. granifera showed effectively no overlap with the native planorbid snail. However, there was a high degree of overlap between the two invasive species (˜0.86). Bayesian mixing models indicated that detrital mangrove Barringtonia racemosa leaves contributed the largest proportion to P. disjunctivus diet (0.12–0.58), while the diet of T. granifera was more variable with high proportions of detrital Eichhornia crassipes (0.24–0.60) and Azolla filiculoides (0.09–0.33) as well as detrital Barringtonia racemosa leaves (0.00–0.30). Overall, although the invasive T. granifera and P. disjunctivus were not in direct competition for dietary resources with native species in the Nseleni River system, their spread in isotopic space suggests they are likely to restrict energy available to higher consumers in the food web. Establishment of these invasive populations in the Nseleni River is thus probably driven by access to resources unexploited or unavailable to native residents.",what Habitat ?,Freshwater,0. 26.,False,False
"Propagule pressure is fundamental to invasion success, yet our understanding of its role in the marine domain is limited. Few studies have manipulated or controlled for propagule supply in the field, and consequently there is little empirical data to test for non-linearities or interactions with other processes. Supply of non-indigenous propagules is most likely to be elevated in urban estuaries, where vessels congregate and bring exotic species on fouled hulls and in ballast water. These same environments are also typically subject to elevated levels of disturbance from human activities, creating the potential for propagule pressure and disturbance to interact. By applying a controlled dose of free-swimming larvae to replicate assemblages, we were able to quantify a dose-response relationship at much finer spatial and temporal scales than previously achieved in the marine environment. We experimentally crossed controlled levels of propagule pressure and disturbance in the field, and found that both were required for invasion to occur. Only recruits that had settled onto bare space survived beyond three months, precluding invader persistence in undisturbed communities. In disturbed communities initial survival on bare space appeared stochastic, such that a critical density was required before the probability of at least one colony surviving reached a sufficient level. Those that persisted showed 75% survival over the following three months, signifying a threshold past which invaders were resilient to chance mortality. Urban estuaries subject to anthropogenic disturbance are common throughout the world, and similar interactions may be integral to invasion dynamics in these ecosystems.",what Habitat ?,Marine,,False,False
"Hypoxia is increasing in marine and estuarine systems worldwide, primarily due to anthropogenic causes. Periodic hypoxia represents a pulse disturbance, with the potential to restruc- ture estuarine biotic communities. We chose the shallow, epifaunal community in the lower Chesa- peake Bay, Virginia, USA, to test the hypothesis that low dissolved oxygen (DO) (<4 mg l -1 ) affects community dynamics by reducing the cover of spatial dominants, creating space both for less domi- nant native species and for invasive species. Settling panels were deployed at shallow depths in spring 2000 and 2001 at Gloucester Point, Virginia, and were manipulated every 2 wk from late June to mid-August. Manipulation involved exposing epifaunal communities to varying levels of DO for up to 24 h followed by redeployment in the York River. Exposure to low DO affected both species com- position (presence or absence) and the abundance of the organisms present. Community dominance shifted away from barnacles as level of hypoxia increased. Barnacles were important spatial domi- nants which reduced species diversity when locally abundant. The cover of Hydroides dianthus, a native serpulid polychaete, doubled when exposed to periodic hypoxia. Increased H. dianthus cover may indicate whether a local region has experienced periodic, local DO depletion and thus provide an indicator of poor water-quality conditions. In 2001, the combined cover of the invasive and crypto- genic species in this community, Botryllus schlosseri (tunicate), Molgula manhattensis (tunicate), Ficopomatus enigmaticus (polychaete) and Diadumene lineata (anemone), was highest on the plates exposed to moderately low DO (2 mg l -1 < DO < 4 mg l -1 ). All 4 of these species are now found world- wide and exhibit life histories well adapted for establishment in foreign habitats. Low DO events may enhance success of invasive species, which further stress marine and estuarine ecosystems.",what Habitat ?,Marine,,False,False
"Many ecosystems receive a steady stream of non-native species. How biotic resistance develops over time in these ecosystems will depend on how established invaders contribute to subsequent resistance. If invasion success and defence capacity (i.e. contribution to resistance) are correlated, then community resistance should increase as species accumulate. If successful invaders also cause most impact (through replacing native species with low defence capacity) then the effect will be even stronger. If successful invaders instead have weak defence capacity or even facilitative attributes, then resistance should decrease with time, as proposed by the invasional meltdown hypothesis. We analysed 1157 introductions of freshwater fish in Swedish lakes and found that species' invasion success was positively correlated with their defence capacity and impact, suggesting that these communities will develop stronger resistance over time. These insights can be used to identify scenarios where invading species are expected to cause large impact.",what Habitat ?,Freshwater,1157,False,False
"The expression of defensive morphologies in prey often is correlated with predator abundance or diversity over a range of temporal and spatial scales. These patterns are assumed to reflect natural selection via differential predation on genetically determined, fixed phenotypes. Phenotypic variation, however, also can reflect within-generation developmental responses to environmental cues (phenotypic plasticity). For example, water-borne effluents from predators can induce the production of defensive morphologies in many prey taxa. This phenomenon, however, has been examined only on narrow scales. Here, we demonstrate adaptive phenotypic plasticity in prey from geographically separated populations that were reared in the presence of an introduced predator. Marine snails exposed to predatory crab effluent in the field increased shell thickness rapidly compared with controls. Induced changes were comparable to (i) historical transitions in thickness previously attributed to selection by the invading predator and (ii) present-day clinal variation predicted from water temperature differences. Thus, predator-induced phenotypic plasticity may explain broad-scale geographic and temporal phenotypic variation. If inducible defenses are heritable, then selection on the reaction norm may influence coevolution between predator and prey. Trade-offs may explain why inducible rather than constitutive defenses have evolved in several gastropod species.",what Habitat ?,Marine,"defensive morphologies in prey often is correlated with predator abundance or diversity over a range of temporal and spatial scales. these patterns are assumed to reflect natural selection via differential predation on genetically determined, fixed phenotypes. phenotypic variation, however, also can reflect within - generation developmental responses to environmental cues ( phenotypic plasticity ). for example, water - borne effluents from predators can induce the production of defensive morphologies in many prey taxa. this phenomenon, however, has been examined only on narrow scales. here, we demonstrate adaptive phenotypic plasticity",False,False
"1. Biological invasion theory predicts that the introduction and establishment of non-native species is positively correlated with propagule pressure. Releases of pet and aquarium fishes to inland waters has a long history; however, few studies have examined the demographic basis of their importation and incidence in the wild. 2. For the 1500 grid squares (10×10 km) that make up England, data on human demographics (population density, numbers of pet shops, garden centres and fish farms), the numbers of non-native freshwater fishes (from consented licences) imported in those grid squares (i.e. propagule pressure), and the reported incidences (in a national database) of non-native fishes in the wild were used to examine spatial relationships between the occurrence of non-native fishes and the demographic factors associated with propagule pressure, as well as to test whether the demographic factors are statistically reliable predictors of the incidence of non-native fishes, and as such surrogate estimators of propagule pressure. 3. Principal coordinates of neighbour matrices analyses, used to generate spatially explicit models, and confirmatory factor analysis revealed that spatial distributions of non-native species in England were significantly related to human population density, garden centre density and fish farm density. Human population density and the number of fish imports were identified as the best predictors of propagule pressure. 4. Human population density is an effective surrogate estimator of non-native fish propagule pressure and can be used to predict likely areas of non-native fish introductions. In conjunction with fish movements, where available, human population densities can be used to support biological invasion monitoring programmes across Europe (and perhaps globally) and to inform management decisions as regards the prioritization of areas for the control of non-native fish introductions. © Crown copyright 2010. Reproduced with the permission of her Majesty's Stationery Office. Published by John Wiley & Sons, Ltd.",what Habitat ?,Freshwater,,False,False
"The objective of this study was to test if morphological differences in pumpkinseed Lepomis gibbosus found in their native range (eastern North America) that are linked to feeding regime, competition with other species, hydrodynamic forces and habitat were also found among stream- and lake- or reservoir-dwelling fish in Iberian systems. The species has been introduced into these systems, expanding its range, and is presumably well adapted to freshwater Iberian Peninsula ecosystems. The results show a consistent pattern for size of lateral fins, with L. gibbosus that inhabit streams in the Iberian Peninsula having longer lateral fins than those inhabiting reservoirs or lakes. Differences in fin placement, body depth and caudal peduncle dimensions do not differentiate populations of L. gibbosus from lentic and lotic water bodies and, therefore, are not consistent with functional expectations. Lepomis gibbosus from lotic and lentic habitats also do not show a consistent pattern of internal morphological differentiation, probably due to the lack of lotic-lentic differences in prey type. Overall, the univariate and multivariate analyses show that most of the external and internal morphological characters that vary among populations do not differentiate lotic from lentic Iberian populations. The lack of expected differences may be a consequence of the high seasonal flow variation in Mediterranean streams, and the resultant low- or no-flow conditions during periods of summer drought.",what Habitat ?,Freshwater,drought.,False,False
"The relative importance of plasticity vs. adaptation for the spread of invasive species has rarely been studied. We examined this question in a clonal population of invasive freshwater snails (Potamopyrgus antipodarum) from the western United States by testing whether observed plasticity in life history traits conferred higher fitness across a range of temperatures. We raised isofemale lines from three populations from different climate regimes (high- and low-elevation rivers and an estuary) in a split-brood, common-garden design in three temperatures. We measured life history and growth traits and calculated population growth rate (as a measure of fitness) using an age-structured projection matrix model. We found a strong effect of temperature on all traits, but no evidence for divergence in the average level of traits among populations. Levels of genetic variation and significant reaction norm divergence for life history traits suggested some role for adaptation. Plasticity varied among traits and was lowest for size and reproductive traits compared to age-related traits and fitness. Plasticity in fitness was intermediate, suggesting that invasive populations are not general-purpose genotypes with respect to the range of temperatures studied. Thus, by considering plasticity in fitness and its component traits, we have shown that trait plasticity alone does not yield the same fitness across a relevant set of temperature conditions.",what Habitat ?,Freshwater,temperatures.,False,False
"Species invasion is one of the leading mechanisms of global environmental change, particularly in freshwater ecosystems. We used the Food and Agriculture Organization's Database of Invasive Aquatic...",what Habitat ?,Freshwater,freshwater,True,True
"Species that are frequently introduced to an exotic range have a high potential of becoming invasive. Besides propagule pressure, however, no other generally strong determinant of invasion success is known. Although evidence has accumulated that human affiliates (domesticates, pets, human commensals) also have high invasion success, existing studies do not distinguish whether this success can be completely explained by or is partly independent of propagule pressure. Here, we analyze both factors independently, propagule pressure and human affiliation. We also consider a third factor directly related to humans, hunting, and 17 traits on each species' population size and extent, diet, body size, and life history. Our dataset includes all 2362 freshwater fish, mammals, and birds native to Europe or North America. In contrast to most previous studies, we look at the complete invasion process consisting of (1) introduction, (2) establishment, and (3) spread. In this way, we not only consider which of the introduced species became invasive but also which species were introduced. Of the 20 factors tested, propagule pressure and human affiliation were the two strongest determinants of invasion success across all taxa and steps. This was true for multivariate analyses that account for intercorrelations among variables as well as univariate analyses, suggesting that human affiliation influenced invasion success independently of propagule pressure. Some factors affected the different steps of the invasion process antagonistically. For example, game species were much more likely to be introduced to an exotic continent than nonhunted species but tended to be less likely to establish themselves and spread. Such antagonistic effects show the importance of considering the complete invasion process.",what Measure of invasion success ?,Establishment,"establishment, and ( 3 ) spread.",False,True
"Abstract: Roads are believed to be a major contributing factor to the ongoing spread of exotic plants. We examined the effect of road improvement and environmental variables on exotic and native plant diversity in roadside verges and adjacent semiarid grassland, shrubland, and woodland communities of southern Utah ( U.S.A. ). We measured the cover of exotic and native species in roadside verges and both the richness and cover of exotic and native species in adjacent interior communities ( 50 m beyond the edge of the road cut ) along 42 roads stratified by level of road improvement ( paved, improved surface, graded, and four‐wheel‐drive track ). In roadside verges along paved roads, the cover of Bromus tectorum was three times as great ( 27% ) as in verges along four‐wheel‐drive tracks ( 9% ). The cover of five common exotic forb species tended to be lower in verges along four‐wheel‐drive tracks than in verges along more improved roads. The richness and cover of exotic species were both more than 50% greater, and the richness of native species was 30% lower, at interior sites adjacent to paved roads than at those adjacent to four‐wheel‐drive tracks. In addition, environmental variables relating to dominant vegetation, disturbance, and topography were significantly correlated with exotic and native species richness and cover. Improved roads can act as conduits for the invasion of adjacent ecosystems by converting natural habitats to those highly vulnerable to invasion. However, variation in dominant vegetation, soil moisture, nutrient levels, soil depth, disturbance, and topography may render interior communities differentially susceptible to invasions originating from roadside verges. Plant communities that are both physically invasible ( e.g., characterized by deep or fertile soils ) and disturbed appear most vulnerable. Decision‐makers considering whether to build, improve, and maintain roads should take into account the potential spread of exotic plants.",what Measure of invasion success ?,Cover of exotic species,improvement,False,False
"What determines the number of alien species in a given region? ‘Native biodiversity’ and ‘human impact’ are typical answers to this question. Indeed, studies comparing different regions have frequently found positive relationships between number of alien species and measures of both native biodiversity (e.g. the number of native species) and human impact (e.g. human population). These relationships are typically explained by biotic acceptance or resistance, i.e. by influence of native biodiversity and human impact on the second step of the invasion process, establishment. The first step of the invasion process, introduction, has often been ignored. Here we investigate whether relationships between number of alien mammals and native biodiversity or human impact in 43 European countries are mainly shaped by differences in number of introduced mammals or establishment success. Our results suggest that correlation between number of native and established mammals is spurious, as it is simply explainable by the fact that both quantities are linked to country area. We also demonstrate that countries with higher human impact host more alien mammals than other countries because they received more introductions than other countries. Differences in number of alien mammals cannot be explained by differences in establishment success. Our findings highlight importance of human activities and question, at least for mammals in Europe, importance of biotic acceptance and resistance.",what Measure of invasion success ?,Establishment success,establishment,False,False
"Genetic diversity is supposed to support the colonization success of expanding species, in particular in situations where microsite availability is constrained. Addressing the role of genetic diversity in plant invasion experimentally requires its manipulation independent of propagule pressure. To assess the relative importance of these components for the invasion of Senecio vernalis, we created propagule mixtures of four levels of genotype diversity by combining seeds across remote populations, across proximate populations, within single populations and within seed families. In a first container experiment with constant Festuca rupicola density as matrix, genotype diversity was crossed with three levels of seed density. In a second experiment, we tested for effects of establishment limitation and genotype diversity by manipulating Festuca densities. Increasing genetic diversity had no effects on abundance and biomass of S. vernalis but positively affected the proportion of large individuals to small individuals. Mixtures composed from proximate populations had a significantly higher proportion of large individuals than mixtures composed from within seed families only. High propagule pressure increased emergence and establishment of S. vernalis but had no effect on individual growth performance. Establishment was favoured in containers with Festuca, but performance of surviving seedlings was higher in open soil treatments. For S. vernalis invasion, we found a shift in driving factors from density dependence to effects of genetic diversity across life stages. While initial abundance was mostly linked to the amount of seed input, genetic diversity, in contrast, affected later stages of colonization probably via sampling effects and seemed to contribute to filtering the genotypes that finally grew up. In consequence, when disentangling the mechanistic relationships of genetic diversity, seed density and microsite limitation in colonization of invasive plants, a clear differentiation between initial emergence and subsequent survival to juvenile and adult stages is required.",what Measure of invasion success ?,Abundance,establishment,False,False
"Abstract: We developed a method to predict the potential of non‐native reptiles and amphibians (herpetofauna) to establish populations. This method may inform efforts to prevent the introduction of invasive non‐native species. We used boosted regression trees to determine whether nine variables influence establishment success of introduced herpetofauna in California and Florida. We used an independent data set to assess model performance. Propagule pressure was the variable most strongly associated with establishment success. Species with short juvenile periods and species with phylogenetically more distant relatives in regional biotas were more likely to establish than species that start breeding later and those that have close relatives. Average climate match (the similarity of climate between native and non‐native range) and life form were also important. Frogs and lizards were the taxonomic groups most likely to establish, whereas a much lower proportion of snakes and turtles established. We used results from our best model to compile a spreadsheet‐based model for easy use and interpretation. Probability scores obtained from the spreadsheet model were strongly correlated with establishment success as were probabilities predicted for independent data by the boosted regression tree model. However, the error rate for predictions made with independent data was much higher than with cross validation using training data. This difference in predictive power does not preclude use of the model to assess the probability of establishment of herpetofauna because (1) the independent data had no information for two variables (meaning the full predictive capacity of the model could not be realized) and (2) the model structure is consistent with the recent literature on the primary determinants of establishment success for herpetofauna. It may still be difficult to predict the establishment probability of poorly studied taxa, but it is clear that non‐native species (especially lizards and frogs) that mature early and come from environments similar to that of the introduction region have the highest probability of establishment.",what Measure of invasion success ?,Establishment,establishment,True,True
"1 The cultivation and dissemination of alien ornamental plants increases their potential to invade. More specifically, species with bird‐dispersed seeds can potentially infiltrate natural nucleation processes in savannas. 2 To test (i) whether invasion depends on facilitation by host trees, (ii) whether propagule pressure determines invasion probability, and (iii) whether alien host plants are better facilitators of alien fleshy‐fruited species than indigenous species, we mapped the distribution of alien fleshy‐fruited species planted inside a military base, and compared this with the distribution of alien and native fleshy‐fruited species established in the surrounding natural vegetation. 3 Abundance and diversity of fleshy‐fruited plant species was much greater beneath tree canopies than in open grassland and, although some native fleshy‐fruited plants were found both beneath host trees and in the open, alien fleshy‐fruited plants were found only beneath trees. 4 Abundance of fleshy‐fruited alien species in the natural savanna was positively correlated with the number of individuals of those species planted in the grounds of the military base, while the species richness of alien fleshy‐fruited taxa decreased with distance from the military base, supporting the notion that propagule pressure is a fundamental driver of invasions. 5 There were more fleshy‐fruited species beneath native Acacia tortilis than beneath alien Prosopis sp. trees of the equivalent size. Although there were significant differences in native plant assemblages beneath these hosts, the proportion of alien to native fleshy‐fruited species did not differ with host. 6 Synthesis. Birds facilitate invasion of a semi‐arid African savanna by alien fleshy‐fruited plants, and this process does not require disturbance. Instead, propagule pressure and a few simple biological observations define the probability that a plant will invade, with alien species planted in gardens being a major source of propagules. Some invading species have the potential to transform this savanna by overtopping native trees, leading to ecosystem‐level impacts. Likewise, the invasion of the open savanna by alien host trees (such as Prosopis sp.) may change the diversity, abundance and species composition of the fleshy‐fruited understorey. These results illustrate the complex interplay between propagule pressure, facilitation, and a range of other factors in biological invasions.",what Measure of invasion success ?,Abundance,plants,False,False
"Questions: How did post-wildfire understorey plant community response, including exotic species response, differ between pre-fire treated areas that were less severely burned, and pre-fire untreated areas that were more severely burned? Were these differences consistent through time? Location: East-central Arizona, southwestern US. Methods: We used a multi-year data set from the 2002 Rodeo–Chediski Fire to detect post-fire trends in plant community response in burned ponderosa pine forests. Within the burn perimeter, we examined the effects of pre-fire fuels treatments on post-fire vegetation by comparing paired treated and untreated sites on the Apache-Sitgreaves National Forest. We sampled these paired sites in 2004, 2005 and 2011. Results: There were significant differences in pre-fire treated and untreated plant communities by species composition and abundance in 2004 and 2005, but these communities were beginning to converge in 2011. Total understorey plant cover was significantly higher in untreated areas for all 3 yr. Plant cover generally increased between 2004 and 2005 and markedly decreased in 2011, with the exception of shrub cover, which steadily increased through time. The sharp decrease in forb and graminoid cover in 2011 is likely related to drought conditions since the fire. Annual/biennial forb and graminoid cover decreased relative to perennial cover through time, consistent with the initial floristics hypothesis. Exotic plant response was highly variable and not limited to the immediate post-fire, annual/biennial community. Despite low overall exotic forb and graminoid cover for all years (<2.5%), several exotic species increased in frequency, and the relative proportion of exotic to native cover increased through time. Conclusions: Pre-treatment fuel reduction treatments helped maintain foundation overstorey species and associated native plant communities following this large wildfire. The overall low cover of exotic species on these sites supports other findings that the disturbance associated with high-severity fire does not always result in exotic species invasions. The increase in relative cover and frequency though time indicates that some species are proliferating, and continued monitoring is recommended. Patterns of exotic species invasions after severe burning are not easily predicted, and are likely more dependent on site-specific factors such as propagules, weather patterns and management.",what Measure of invasion success ?,Cover of exotic species ,,False,False
"Airborne hyperspectral data have been available to researchers since the early 1980s and their use for geologic applications is well documented. The launch of the National Aeronautics and Space Administration Earth Observing 1 Hyperion sensor in November 2000 marked the establishment of a test bed for spaceborne hyperspectral capabilities. Hyperion covers the 0.4-2.5-/spl mu/m range with 242 spectral bands at approximately 10-nm spectral resolution and 30-m spatial resolution. Analytical Imaging and Geophysics LLC and the Commonwealth Scientific and Industrial Research Organisation have been involved in efforts to evaluate, validate, and demonstrate Hyperions's utility for geologic mapping in a variety of sites in the United States and around the world. Initial results over several sites with established ground truth and years of airborne hyperspectral data show that Hyperion data from the shortwave infrared spectrometer can be used to produce useful geologic (mineralogic) information. Minerals mapped include carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite. Hyperion data collected under optimum conditions (summer season, bright targets, well-exposed geology) indicate that Hyperion data meet prelaunch specifications and allow subtle distinctions such as determining the difference between calcite and dolomite and mapping solid solution differences in micas caused by substitution in octahedral molecular sites. Comparison of airborne hyperspectral data [from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)] to the Hyperion data establishes that Hyperion provides similar basic mineralogic information, with the principal limitation being limited mapping of fine spectral detail under less-than-optimum acquisition conditions (winter season, dark targets) based on lower signal-to-noise ratios. Case histories demonstrate the analysis methodologies and level of information available from the Hyperion data. They also show the viability of Hyperion as a means of extending hyperspectral mineral mapping to areas not accessible to aircraft sensors. The analysis results demonstrate that spaceborne hyperspectral sensors can produce useful mineralogic information, but also indicate that SNR improvements are required for future spaceborne sensors to allow the same level of mapping that is currently possible from airborne sensors such as AVIRIS.",what Minerals Mapped/ Identified ?,Buddingtonite,"carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite.",False,True
"Airborne hyperspectral data have been available to researchers since the early 1980s and their use for geologic applications is well documented. The launch of the National Aeronautics and Space Administration Earth Observing 1 Hyperion sensor in November 2000 marked the establishment of a test bed for spaceborne hyperspectral capabilities. Hyperion covers the 0.4-2.5-/spl mu/m range with 242 spectral bands at approximately 10-nm spectral resolution and 30-m spatial resolution. Analytical Imaging and Geophysics LLC and the Commonwealth Scientific and Industrial Research Organisation have been involved in efforts to evaluate, validate, and demonstrate Hyperions's utility for geologic mapping in a variety of sites in the United States and around the world. Initial results over several sites with established ground truth and years of airborne hyperspectral data show that Hyperion data from the shortwave infrared spectrometer can be used to produce useful geologic (mineralogic) information. Minerals mapped include carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite. Hyperion data collected under optimum conditions (summer season, bright targets, well-exposed geology) indicate that Hyperion data meet prelaunch specifications and allow subtle distinctions such as determining the difference between calcite and dolomite and mapping solid solution differences in micas caused by substitution in octahedral molecular sites. Comparison of airborne hyperspectral data [from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)] to the Hyperion data establishes that Hyperion provides similar basic mineralogic information, with the principal limitation being limited mapping of fine spectral detail under less-than-optimum acquisition conditions (winter season, dark targets) based on lower signal-to-noise ratios. Case histories demonstrate the analysis methodologies and level of information available from the Hyperion data. They also show the viability of Hyperion as a means of extending hyperspectral mineral mapping to areas not accessible to aircraft sensors. The analysis results demonstrate that spaceborne hyperspectral sensors can produce useful mineralogic information, but also indicate that SNR improvements are required for future spaceborne sensors to allow the same level of mapping that is currently possible from airborne sensors such as AVIRIS.",what Minerals Mapped/ Identified ?,Dolomite,"carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite.",False,False
"Satellite-based hyperspectral imaging became a reality in November 2000 with the successful launch and operation of the Hyperion system on board the EO-1 platform. Hyperion is a pushbroom imager with 220 spectral bands in the 400-2500 nm wavelength range, a 30 meter pixel size and a 7.5 km swath. Pre-launch characterization of Hyperion measured low signal to noise (SNR<40:1) for the geologically significant shortwave infrared (SWIR) wavelength region (2000-2500 nm). The impact of this low SNR on Hyperion's capacity to resolve spectral detail was evaluated for the Mount Fitton test site in South Australia, which comprises a diverse range of minerals with narrow, diagnostic absorption bands in the SWIR. Following radiative transfer correction of the Hyperion radiance at sensor data to surface radiance (apparent reflectance), diagnostic spectral signatures were clearly apparent, including: green vegetation; talc; dolomite; chlorite; white mica and possibly tremolite. Even though the derived surface composition maps generated from these image endmembers were noisy (both random and column), they were nonetheless spatially coherent and correlated well with the known geology. In addition, the Hyperion data were used to measure and map spectral shifts of <10 nm in the SWIR related to white mica chemical variations.",what Minerals Mapped/ Identified ?,Dolomite,220 spectral bands in the 400 - 2500 nm,False,False
"Airborne hyperspectral data have been available to researchers since the early 1980s and their use for geologic applications is well documented. The launch of the National Aeronautics and Space Administration Earth Observing 1 Hyperion sensor in November 2000 marked the establishment of a test bed for spaceborne hyperspectral capabilities. Hyperion covers the 0.4-2.5-/spl mu/m range with 242 spectral bands at approximately 10-nm spectral resolution and 30-m spatial resolution. Analytical Imaging and Geophysics LLC and the Commonwealth Scientific and Industrial Research Organisation have been involved in efforts to evaluate, validate, and demonstrate Hyperions's utility for geologic mapping in a variety of sites in the United States and around the world. Initial results over several sites with established ground truth and years of airborne hyperspectral data show that Hyperion data from the shortwave infrared spectrometer can be used to produce useful geologic (mineralogic) information. Minerals mapped include carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite. Hyperion data collected under optimum conditions (summer season, bright targets, well-exposed geology) indicate that Hyperion data meet prelaunch specifications and allow subtle distinctions such as determining the difference between calcite and dolomite and mapping solid solution differences in micas caused by substitution in octahedral molecular sites. Comparison of airborne hyperspectral data [from the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS)] to the Hyperion data establishes that Hyperion provides similar basic mineralogic information, with the principal limitation being limited mapping of fine spectral detail under less-than-optimum acquisition conditions (winter season, dark targets) based on lower signal-to-noise ratios. Case histories demonstrate the analysis methodologies and level of information available from the Hyperion data. They also show the viability of Hyperion as a means of extending hyperspectral mineral mapping to areas not accessible to aircraft sensors. The analysis results demonstrate that spaceborne hyperspectral sensors can produce useful mineralogic information, but also indicate that SNR improvements are required for future spaceborne sensors to allow the same level of mapping that is currently possible from airborne sensors such as AVIRIS.",what Minerals Mapped/ Identified ?,Silica,"carbonates, chlorite, epidote, kaolinite, alunite, buddingtonite, muscovite, hydrothermal silica, and zeolite.",False,True
Chinese Named entity recognition is one of the most important tasks in NLP. The paper mainly describes our work on NER tasks. The paper built up a system under the framework of Conditional Random Fields (CRFs) model. With an improved tag set the system gets an F-value of 93.49 using SIGHAN2007 MSRA corpus.,what Language/domain ?,Chinese,chinese,True,True
"Drug abuse pertains to the consumption of a substance that may induce adverse effects to a person. In international security studies, drug trafficking has become an important topic. In this regard, drug-related crimes are identified as an extremely significant challenge faced by any community. Several techniques for investigations in the crime domain have been implemented by many researchers. However, most of these researchers focus on extracting general crime entities. The number of studies that focus on the drug crime domain is relatively limited. This paper mainly aims to propose a rule-based named entity recognition model for drug-related crime news documents. In this work, a set of heuristic and grammatical rules is used to extract named entities, such as types of drugs, amount of drugs, price of drugs, drug hiding methods, and the nationality of the suspect. A set of grammatical and heuristic rules is established based on part-ofspeech information, developed gazetteers, and indicator word lists. The combined approach of heuristic and grammatical rules achieves a good performance with an overall precision of 86%, a recall of 87%, and an F1-measure of 87%. Results indicate that the ensemble of both heuristic and grammatical rules improves the extraction effectiveness in terms of macro-F1 for all entities.",what Language/domain ?,Drug-related crime news documents,,False,False
"One difficulty with machine learning for information extraction is the high cost of collecting labeled examples. Active Learning can make more efficient use of the learner's time by asking them to label only instances that are most useful for the trainer. In random sampling approach, unlabeled data is selected for annotation at random and thus can't yield the desired results. In contrast, active learning selects the useful data from a huge pool of unlabeled data for the classifier. The strategies used often classify the corpus tokens (or, data points) into wrong classes. The classifier is confused between two categories if the token is located near the margin. We propose a novel method for solving this problem and show that it favorably results in the increased performance. Our approach is based on the supervised machine learning algorithm, namely Support Vector Machine (SVM). The proposed approach is applied for solving the problem of named entity recognition (NER) in two Indian languages, namely Hindi and Bengali. Results show that proposed active learning based technique indeed improves the performance of the system.",what Language/domain ?,Hindi and Bengali,hindi and bengali.,True,True
"Natural language processing (NLP) is widely applied in biological domains to retrieve information from publications. Systems to address numerous applications exist, such as biomedical named entity recognition (BNER), named entity normalization (NEN) and protein-protein interaction extraction (PPIE). High-quality datasets can assist the development of robust and reliable systems; however, due to the endless applications and evolving techniques, the annotations of benchmark datasets may become outdated and inappropriate. In this study, we first review commonlyused BNER datasets and their potential annotation problems such as inconsistency and low portability. Then, we introduce a revised version of the JNLPBA dataset that solves potential problems in the original and use state-of-the-art named entity recognition systems to evaluate its portability to different kinds of biomedical literature, including protein-protein interaction and biology events. Lastly, we introduce an ensembled biomedical entity dataset (EBED) by extending the revised JNLPBA dataset with PubMed Central full-text paragraphs, figure captions and patent abstracts. This EBED is a multi-task dataset that covers annotations including gene, disease and chemical entities. In total, it contains 85000 entity mentions, 25000 entity mentions with database identifiers and 5000 attribute tags. To demonstrate the usage of the EBED, we review the BNER track from the AI CUP Biomedical Paper Analysis challenge. Availability: The revised JNLPBA dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/Re vised_JNLPBA.zip. The EBED dataset is available at https://iasl-btm.iis.sinica.edu.tw/BNER/Content/AICUP _EBED_dataset.rar. Contact: Email: thtsai@g.ncu.edu.tw, Tel. 886-3-4227151 ext. 35203, Fax: 886-3-422-2681 Email: hsu@iis.sinica.edu.tw, Tel. 886-2-2788-3799 ext. 2211, Fax: 886-2-2782-4814 Supplementary information: Supplementary data are available at Briefings in Bioinformatics online.",what Data domains ?,Protein-Protein Interaction Extraction (PPIE),"gene,",False,False
"We present the BioCreative VII Task 3 which focuses on drug names extraction from tweets. Recognized to provide unique insights into population health, detecting health related tweets is notoriously challenging for natural language processing tools. Tweets are written about any and all topics, most of them not related to health. Additionally, they are written with little regard for proper grammar, are inherently colloquial, and are almost never proof-read. Given a tweet, task 3 consists of detecting if the tweet has a mention of a drug name and, if so, extracting the span of the drug mention. We made available 182,049 tweets publicly posted by 212 Twitter users with all drugs mentions manually annotated. This corpus exhibits the natural and strongly imbalanced distribution of positive tweets, with only 442 tweets (0.2%) mentioning a drug. This task was an opportunity for participants to evaluate methods robust to classimbalance beyond the simple lexical match. A total of 65 teams registered, and 16 teams submitted a system run. We summarize the corpus and the tools created for the challenge, which is freely available at https://biocreative.bioinformatics.udel.edu/tasks/biocreativevii/track-3/. We analyze the methods and the results of the competing systems with a focus on learning from classimbalanced data. Keywords—social media; pharmacovigilance; named entity recognition; drug name extraction; class-imbalance.",what Data domains ?,Social media,"population health,",False,False
"A considerable effort has been made to extract biological and chemical entities, as well as their relationships, from the scientific literature, either manually through traditional literature curation or by using information extraction and text mining technologies. Medicinal chemistry patents contain a wealth of information, for instance to uncover potential biomarkers that might play a role in cancer treatment and prognosis. However, current biomedical annotation databases do not cover such information, partly due to limitations of publicly available biomedical patent mining software. As part of the BioCreative V CHEMDNER patents track, we present the results of the first named entity recognition (NER) assignment carried out to detect mentions of chemical compounds and genes/proteins in running patent text. More specifically, this task aimed to evaluate the performance of automatic name recognition strategies capable of isolating chemical names and gene and gene product mentions from surrounding text within patent titles and abstracts. A total of 22 unique teams submitted results for at least one of the three CHEMDNER subtasks. The first subtask, called the CEMP (chemical entity mention in patents) task, focused on the detection of chemical named entity mentions in patents, requesting teams to return the start and end indices corresponding to all the chemical entities found in a given record. A total of 21 teams submitted 93 runs, for this subtask. The top performing team reached an f-measure of 0.89 with a precision of 0.87 and a recall of 0.91. The CPD (chemical passage detection) task required the classification of patent titles and abstracts whether they do or do not contain chemical compound mentions. Nine teams returned predictions for this task (40 runs). The top run in terms of Matthew’s correlation coefficient (MCC) had a score of 0.88, the highest sensitivity ? Corresponding author",what Data domains ?,MEDICINAL CHEMISTRY,"biological and chemical entities, as well as their relationships, from the scientific literature, either manually through traditional literature curation or by using information extraction and text mining technologies. medicinal chemistry",False,True
"Abstract Background Biology-focused databases and software define bioinformatics and their use is central to computational biology. In such a complex and dynamic field, it is of interest to understand what resources are available, which are used, how much they are used, and for what they are used. While scholarly literature surveys can provide some insights, large-scale computer-based approaches to identify mentions of bioinformatics databases and software from primary literature would automate systematic cataloguing, facilitate the monitoring of usage, and provide the foundations for the recovery of computational methods for analysing biological data, with the long-term aim of identifying best/common practice in different areas of biology. Results We have developed bioNerDS, a named entity recogniser for the recovery of bioinformatics databases and software from primary literature. We identify such entities with an F-measure ranging from 63% to 91% at the mention level and 63-78% at the document level, depending on corpus. Not attaining a higher F-measure is mostly due to high ambiguity in resource naming, which is compounded by the on-going introduction of new resources. To demonstrate the software, we applied bioNerDS to full-text articles from BMC Bioinformatics and Genome Biology. General mention patterns reflect the remit of these journals, highlighting BMC Bioinformatics’s emphasis on new tools and Genome Biology’s greater emphasis on data analysis. The data also illustrates some shifts in resource usage: for example, the past decade has seen R and the Gene Ontology join BLAST and GenBank as the main components in bioinformatics processing. Conclusions We demonstrate the feasibility of automatically identifying resource names on a large-scale from the scientific literature and show that the generated data can be used for exploration of bioinformatics database and software usage. For example, our results help to investigate the rate of change in resource usage and corroborate the suspicion that a vast majority of resources are created, but rarely (if ever) used thereafter. bioNerDS is available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""http://bionerds.sourceforge.net/"" ext-link-type=""uri"">http://bionerds.sourceforge.net/</jats:ext-link>.",what Data domains ?,Biology,biology -,True,True
"The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.",what Data domains ?,Clinical,clinical records,False,True
"The Critical Assessment of Information Extraction systems in Biology (BioCreAtIvE) challenge evaluation tasks collectively represent a community-wide effort to evaluate a variety of text-mining and information extraction systems applied to the biological domain. The BioCreative IV Workshop included five independent subject areas, including Track 3, which focused on named-entity recognition (NER) for the Comparative Toxicogenomics Database (CTD; http://ctdbase.org). Previously, CTD had organized document ranking and NER-related tasks for the BioCreative Workshop 2012; a key finding of that effort was that interoperability and integration complexity were major impediments to the direct application of the systems to CTD's text-mining pipeline. This underscored a prevailing problem with software integration efforts. Major interoperability-related issues included lack of process modularity, operating system incompatibility, tool configuration complexity and lack of standardization of high-level inter-process communications. One approach to potentially mitigate interoperability and general integration issues is the use of Web services to abstract implementation details; rather than integrating NER tools directly, HTTP-based calls from CTD's asynchronous, batch-oriented text-mining pipeline could be made to remote NER Web services for recognition of specific biological terms using BioC (an emerging family of XML formats) for inter-process communications. To test this concept, participating groups developed Representational State Transfer /BioC-compliant Web services tailored to CTD's NER requirements. Participants were provided with a comprehensive set of training materials. CTD evaluated results obtained from the remote Web service-based URLs against a test data set of 510 manually curated scientific articles. Twelve groups participated in the challenge. Recall, precision, balanced F-scores and response times were calculated. Top balanced F-scores for gene, chemical and disease NER were 61, 74 and 51%, respectively. Response times ranged from fractions-of-a-second to over a minute per article. We present a description of the challenge and summary of results, demonstrating how curation groups can effectively use interoperable NER technologies to simplify text-mining pipeline implementation. Database URL: http://ctdbase.org/",what Coarse-grained Entity type ?,Disease,chemical,False,False
"Gene function curation via Gene Ontology (GO) annotation is a common task among Model Organism Database groups. Owing to its manual nature, this task is considered one of the bottlenecks in literature curation. There have been many previous attempts at automatic identification of GO terms and supporting information from full text. However, few systems have delivered an accuracy that is comparable with humans. One recognized challenge in developing such systems is the lack of marked sentence-level evidence text that provides the basis for making GO annotations. We aim to create a corpus that includes the GO evidence text along with the three core elements of GO annotations: (i) a gene or gene product, (ii) a GO term and (iii) a GO evidence code. To ensure our results are consistent with real-life GO data, we recruited eight professional GO curators and asked them to follow their routine GO annotation protocols. Our annotators marked up more than 5000 text passages in 200 articles for 1356 distinct GO terms. For evidence sentence selection, the inter-annotator agreement (IAA) results are 9.3% (strict) and 42.7% (relaxed) in F1-measures. For GO term selection, the IAAs are 47% (strict) and 62.9% (hierarchical). Our corpus analysis further shows that abstracts contain ∼10% of relevant evidence sentences and 30% distinct GO terms, while the Results/Experiment section has nearly 60% relevant sentences and >70% GO terms. Further, of those evidence sentences found in abstracts, less than one-third contain enough experimental detail to fulfill the three core criteria of a GO annotation. This result demonstrates the need of using full-text articles for text mining GO annotations. Through its use at the BioCreative IV GO (BC4GO) task, we expect our corpus to become a valuable resource for the BioNLP research community. Database URL: http://www.biocreative.org/resources/corpora/bc-iv-go-task-corpus/.",what Coarse-grained Entity type ?,GO Term,gene function,False,False
"The BioCreative NLM-Chem track calls for a community effort to fine-tune automated recognition of chemical names in biomedical literature. Chemical names are one of the most searched biomedical entities in PubMed and – as highlighted during the COVID-19 pandemic – their identification may significantly advance research in multiple biomedical subfields. While previous community challenges focused on identifying chemical names mentioned in titles and abstracts, the full text contains valuable additional detail. We organized the BioCreative NLM-Chem track to call for a community effort to address automated chemical entity recognition in full-text articles. The track consisted of two tasks: 1) Chemical Identification task, and 2) Chemical Indexing prediction task. For the Chemical Identification task, participants were expected to predict with high accuracy all chemicals mentioned in recently published full-text articles, both span (i.e., named entity recognition) and normalization (i.e., entity linking) using MeSH. For the Chemical Indexing task, participants identified which chemicals should be indexed as topics for the article's topic terms in the NLM article and indexing, i.e., appear in the listing of MeSH terms for the document. This manuscript summarizes the BioCreative NLM-Chem track. We received a total of 88 submissions in total from 17 teams worldwide. The highest performance achieved for the Chemical Identification task was 0.8672 f-score (0.8759 precision, 0.8587 recall) for strict NER performance and 0.8136 f-score (0.8621 precision, 0.7702 recall) for strict normalization performance. The highest performance achieved for the Chemical Indexing task was 0.4825 f-score (0.4397 precision, 0.5344 recall). The NLM-Chem track dataset and other challenge materials are publicly available at https://ftp.ncbi.nlm.nih.gov/pub/lu/BC7-NLM-Chem-track/. This community challenge demonstrated 1) the current substantial achievements in deep learning technologies can be utilized to further improve automated prediction accuracy, and 2) the Chemical Indexing task is substantially more challenging. We look forward to further development of biomedical text mining methods to respond to the rapid growth of biomedical literature. Keywords— biomedical text mining; natural language processing; artificial intelligence; machine learning; deep learning; text mining; chemical entity recognition; chemical indexing",what Coarse-grained Entity type ?,Chemical,chemical,True,True
"Software contributions to academic research are relatively invisible, especially to the formalized scholarly reputation system based on bibliometrics. In this article, we introduce a gold‐standard dataset of software mentions from the manual annotation of 4,971 academic PDFs in biomedicine and economics. The dataset is intended to be used for automatic extraction of software mentions from PDF format research publications by supervised learning at scale. We provide a description of the dataset and an extended discussion of its creation process, including improved text conversion of academic PDFs. Finally, we reflect on our challenges and lessons learned during the dataset creation, in hope of encouraging more discussion about creating datasets for machine learning use.",what Entity types ?,Version,software,False,False
"Cross-domain named entity recognition (NER) models are able to cope with the scarcity issue of NER samples in target domains. However, most of the existing NER benchmarks lack domain-specialized entity types or do not focus on a certain domain, leading to a less effective cross-domain evaluation. To address these obstacles, we introduce a cross-domain NER dataset (CrossNER), a fully-labeled collection of NER data spanning over five diverse domains with specialized entity categories for different domains. Additionally, we also provide a domain-related corpus since using it to continue pre-training language models (domain-adaptive pre-training) is effective for the domain adaptation. We then conduct comprehensive experiments to explore the effectiveness of leveraging different levels of the domain corpus and pre-training strategies to do domain-adaptive pre-training for the cross-domain task. Results show that focusing on the fractional corpus containing domain-specialized entities and utilizing a more challenging pre-training strategy in domain-adaptive pre-training are beneficial for the NER domain adaptation, and our proposed method can consistently outperform existing cross-domain NER baselines. Nevertheless, experiments also illustrate the challenge of this cross-domain NER task. We hope that our dataset and baselines will catalyze research in the NER domain adaptation area. The code and data are available at this https URL.",what Entity types ?,Task,domain -,False,False
"Abstract Background Biology-focused databases and software define bioinformatics and their use is central to computational biology. In such a complex and dynamic field, it is of interest to understand what resources are available, which are used, how much they are used, and for what they are used. While scholarly literature surveys can provide some insights, large-scale computer-based approaches to identify mentions of bioinformatics databases and software from primary literature would automate systematic cataloguing, facilitate the monitoring of usage, and provide the foundations for the recovery of computational methods for analysing biological data, with the long-term aim of identifying best/common practice in different areas of biology. Results We have developed bioNerDS, a named entity recogniser for the recovery of bioinformatics databases and software from primary literature. We identify such entities with an F-measure ranging from 63% to 91% at the mention level and 63-78% at the document level, depending on corpus. Not attaining a higher F-measure is mostly due to high ambiguity in resource naming, which is compounded by the on-going introduction of new resources. To demonstrate the software, we applied bioNerDS to full-text articles from BMC Bioinformatics and Genome Biology. General mention patterns reflect the remit of these journals, highlighting BMC Bioinformatics’s emphasis on new tools and Genome Biology’s greater emphasis on data analysis. The data also illustrates some shifts in resource usage: for example, the past decade has seen R and the Gene Ontology join BLAST and GenBank as the main components in bioinformatics processing. Conclusions We demonstrate the feasibility of automatically identifying resource names on a large-scale from the scientific literature and show that the generated data can be used for exploration of bioinformatics database and software usage. For example, our results help to investigate the rate of change in resource usage and corroborate the suspicion that a vast majority of resources are created, but rarely (if ever) used thereafter. bioNerDS is available at <jats:ext-link xmlns:xlink=""http://www.w3.org/1999/xlink"" xlink:href=""http://bionerds.sourceforge.net/"" ext-link-type=""uri"">http://bionerds.sourceforge.net/</jats:ext-link>.",what Entity types ?,Biology-focused databases and software,biology -,False,False
"One of the biomedical entity types of relevance for medicine or biosciences are chemical compounds and drugs. The correct detection these entities is critical for other text mining applications building on them, such as adverse drug-reaction detection, medication-related fake news or drug-target extraction. Although a significant effort was made to detect mentions of drugs/chemicals in English texts, so far only very limited attempts were made to recognize them in medical documents in other languages. Taking into account the growing amount of medical publications and clinical records written in Spanish, we have organized the first shared task on detecting drug and chemical entities in Spanish medical documents. Additionally, we included a clinical concept-indexing sub-track asking teams to return SNOMED-CT identifiers related to drugs/chemicals for a collection of documents. For this task, named PharmaCoNER, we generated annotation guidelines together with a corpus of 1,000 manually annotated clinical case studies. A total of 22 teams participated in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system runs). Top scoring teams used sophisticated deep learning approaches yielding very competitive results with F-measures above 0.91. These results indicate that there is a real interest in promoting biomedical text mining efforts beyond English. We foresee that the PharmaCoNER annotation guidelines, corpus and participant systems will foster the development of new resources for clinical and biomedical text mining systems of Spanish medical data.",what Entity types ?,Chemical,chemical compounds,False,True
"This paper presents the fourth edition of the Bacteria Biotope task at BioNLP Open Shared Tasks 2019. The task focuses on the extraction of the locations and phenotypes of microorganisms from PubMed abstracts and full-text excerpts, and the characterization of these entities with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology). The task is motivated by the importance of the knowledge on biodiversity for fundamental research and applications in microbiology. The paper describes the different proposed subtasks, the corpus characteristics, and the challenge organization. We also provide an analysis of the results obtained by participants, and inspect the evolution of the results since the last edition in 2016.",what Entity types ?,Microorganism,microorganisms,False,True
"This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2016, which follows the previous 2013 and 2011 editions. The task focuses on the extraction of the locations (biotopes and geographical places) of bacteria from PubMe abstracts and the characterization of bacteria and their associated habitats with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology). The task is motivated by the importance of the knowledge on bacteria habitats for fundamental research and applications in microbiology. The paper describes the different proposed subtasks, the corpus characteristics, the challenge organization, and the evaluation metrics. We also provide an analysis of the results obtained by participants.",what Entity types ?,Geographical places,biotopes and geographical places ),False,True
"This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2016, which follows the previous 2013 and 2011 editions. The task focuses on the extraction of the locations (biotopes and geographical places) of bacteria from PubMe abstracts and the characterization of bacteria and their associated habitats with respect to reference knowledge sources (NCBI taxonomy, OntoBiotope ontology). The task is motivated by the importance of the knowledge on bacteria habitats for fundamental research and applications in microbiology. The paper describes the different proposed subtasks, the corpus characteristics, the challenge organization, and the evaluation metrics. We also provide an analysis of the results obtained by participants.",what Entity types ?,Habitat,biotopes and geographical places ),False,False
"We consider a distribution problem in which a product has to be shipped from a supplier to several retailers over a given time horizon. Each retailer defines a maximum inventory level. The supplier monitors the inventory of each retailer and determines its replenishment policy, guaranteeing that no stockout occurs at the retailer (vendor-managed inventory policy). Every time a retailer is visited, the quantity delivered by the supplier is such that the maximum inventory level is reached (deterministic order-up-to level policy). Shipments from the supplier to the retailers are performed by a vehicle of given capacity. The problem is to determine for each discrete time instant the quantity to ship to each retailer and the vehicle route. We present a mixed-integer linear programming model and derive new additional valid inequalities used to strengthen the linear relaxation of the model. We implement a branch-and-cut algorithm to solve the model optimally. We then compare the optimal solution of the problem with the optimal solution of two problems obtained by relaxing in different ways the deterministic order-up-to level policy. Computational results are presented on a set of randomly generated problem instances.",what Demand ?,Deterministic,,False,False
"An industrial gases tanker vehicle visitsn customers on a tour, with a possible ( n + 1)st customer added at the end. The amount of needed product at each customer is a known random process, typically a Wiener process. The objective is to adjust dynamically the amount of product provided on scene to each customer so as to minimize total expected costs, comprising costs of earliness, lateness, product shortfall, and returning to the depot nonempty. Earliness costs are computed by invocation of an annualized incremental cost argument. Amounts of product delivered to each customer are not known until the driver is on scene at the customer location, at which point the customer is either restocked to capacity or left with some residual empty capacity, the policy determined by stochastic dynamic programming. The methodology has applications beyond industrial gases.",what Demand ?,Stochastic,stochastic dynamic programming.,False,True
"We consider a new approach to stochastic inventory/routing that approximates the future costs of current actions using optimal dual prices of a linear program. We obtain two such linear programs by formulating the control problem as a Markov decision process and then replacing the optimal value function with the sum of single-customer inventory value functions. The resulting approximation yields statewise lower bounds on optimal infinite-horizon discounted costs. We present a linear program that takes into account inventory dynamics and economics in allocating transportation costs for stochastic inventory routing. On test instances we find that these allocations do not introduce any error in the value function approximations relative to the best approximations that can be achieved without them. Also, unlike other approaches, we do not restrict the set of allowable vehicle itineraries in any way. Instead, we develop an efficient algorithm to both generate and eliminate itineraries during solution of the linear programs and control policy. In simulation experiments, the price-directed policy outperforms other policies from the literature.",what Demand ?,Stochastic,markov decision process and then replacing the optimal value function with the sum of single - customer inventory value functions. the resulting approximation yields statewise lower bounds on optimal infinite - horizon discounted costs. we present a linear program that takes into account inventory dynamics and economics in allocating transportation costs for stochastic inventory routing. on test instances we find that these allocations don't introduce any error,False,True
"Eye localization is necessary for face recognition and related application areas. Most of eye localization algorithms reported thus far still need to be improved about precision and computational time for successful applications. In this paper, we propose an improved eye localization method based on multi-scale Gator feature vector models. The proposed method first tries to locate eyes in the downscaled face image by utilizing Gabor Jet similarity between Gabor feature vector at an initial eye coordinates and the eye model bunch of the corresponding scale. The proposed method finally locates eyes in the original input face image after it processes in the same way recursively in each scaled face image by using the eye coordinates localized in the downscaled image as initial eye coordinates. Experiments verify that our proposed method improves the precision rate without causing much computational overhead compared with other eye localization methods reported in the previous researches.",what Challenges ?,pose,eye localization,False,False
"In this paper, we present an enhanced pictorial structure (PS) model for precise eye localization, a fundamental problem involved in many face processing tasks. PS is a computationally efficient framework for part-based object modelling. For face images taken under uncontrolled conditions, however, the traditional PS model is not flexible enough for handling the complicated appearance and structural variations. To extend PS, we 1) propose a discriminative PS model for a more accurate part localization when appearance changes seriously, 2) introduce a series of global constraints to improve the robustness against scale, rotation and translation, and 3) adopt a heuristic prediction method to address the difficulty of eye localization with partial occlusion. Experimental results on the challenging LFW (Labeled Face in the Wild) database show that our model can locate eyes accurately and efficiently under a broad range of uncontrolled variations involving poses, expressions, lightings, camera qualities, occlusions, etc.",what Challenges ?,Uncontrolled,"eye localization,",False,False
"Multi-agent systems (MASs) have received tremendous attention from scholars in different disciplines, including computer science and civil engineering, as a means to solve complex problems by subdividing them into smaller tasks. The individual tasks are allocated to autonomous entities, known as agents. Each agent decides on a proper action to solve the task using multiple inputs, e.g., history of actions, interactions with its neighboring agents, and its goal. The MAS has found multiple applications, including modeling complex systems, smart grids, and computer networks. Despite their wide applicability, there are still a number of challenges faced by MAS, including coordination between agents, security, and task allocation. This survey provides a comprehensive discussion of all aspects of MAS, starting from definitions, features, applications, challenges, and communications to evaluation. A classification on MAS applications and challenges is provided along with references for further studies. We expect this paper to serve as an insightful and comprehensive resource on the MAS for researchers and practitioners in the area.",what Challenges ?,Security,task allocation.,False,False
"In vendor-managed inventory replenishment, the vendor decides when to make deliveries to customers, how much to deliver, and how to combine shipments using the available vehicles. This gives rise to the inventory-routing problem in which the goal is to coordinate inventory replenishment and transportation to minimize costs. The problem tackled in this paper is the stochastic inventory-routing problem, where stochastic demands are specified through general discrete distributions. The problem is formulated as a discounted infinite-horizon Markov decision problem. Heuristics based on finite scenario trees are developed. Computational results confirm the efficiency of these heuristics.",what approach ?,scenario tree,,False,False
"We investigate the one warehouse multiretailer distribution problem with traveling salesman tour vehicle routing costs. We model the system in the framework of the more general production/distribution system with arbitrary non-negative monotone joint order costs. We develop polynomial time heuristics whose policy costs are provably close to the cost of an optimal policy. In particular, we show that given a submodular function which is close to the true order cost then we can find a power-of-two policy whose cost is only moderately greater than the cost of an optimal policy. Since such submodular approximations exist for traveling salesman tour vehicle routing costs we present a detailed description of heuristics for the one warehouse multiretailer distribution problem. We formulate a nonpolynomial dynamic program that computes optimal power-of-two policies for the one warehouse multiretailer system assuming only that the order costs are non-negative monotone. Finally, we perform computational tests which compare our heuristics to optimal power of two policies for problems of up to sixteen retailers. We also perform computational tests on larger problems; these tests give us insight into what policies one should employ.",what approach ?,Submodular approximation,polynomial time,False,False
"We describe a dynamic and stochastic vehicle dispatching problem called the delivery dispatching problem. This problem is modeled as a Markov decision process. Because exact solution of this model is impractical, we adopt a heuristic approach for handling the problem. The heuristic is based in part on a decomposition of the problem by customer, where customer subproblems generate penalty functions that are applied in a master dispatching problem. We describe how to compute bounds on the algorithm's performance, and apply it to several examples with good results.",what approach ?,Markov decision process,heuristic,False,False
"Vendor managed inventory replenishment is a business practice in which vendors monitor their customers' inventories, and decide when and how much inventory should be replenished. The inventory routing problem addresses the coordination of inventory management and transportation. The ability to solve the inventory routing problem contributes to the realization of the potential savings in inventory and transportation costs brought about by vendor managed inventory replenishment. The inventory routing problem is hard, especially if a large number of customers is involved. We formulate the inventory routing problem as a Markov decision process, and we propose approximation methods to find good solutions with reasonable computational effort. Computational results are presented for the inventory routing problem with direct deliveries.",what approach ?,Markov decision process,markov,False,False
"We describe a system which is capable of learning the presentation of document logical structure, exemplary as shown for business letters. Presenting a set of instances to the system, it clusters them into structural concepts and induces a concept hierarchy. This concept hierarchy is taken as a reference for classifying future input. The article introduces the sequence of learning steps and describes how the resulting concept hierarchy is applied to logical labeling, and reports the results.",what Application Domain ?,letters,system,False,False
"In order to deal with heterogeneous knowledge in the medical field, this paper proposes a method which can learn a heavy-weighted medical ontology based on medical glossaries and Web resources. Firstly, terms and taxonomic relations are extracted based on disease and drug glossaries and a light-weighted ontology is constructed, Secondly, non-taxonomic relations are automatically learned from Web resources with linguistic patterns, and the two ontologies (disease and drug) are expanded from light-weighted level towards heavy-weighted level, At last, the disease ontology and drug ontology are integrated to create a practical medical ontology. Experiment shows that this method can integrate and expand medical terms with taxonomic and different kinds of non-taxonomic relations. Our experiments show that the performance is promising.",what Application Domain ?,Medical,"medical field,",False,True
"Explainability can help cyber-physical systems alleviating risk in automating decisions that are affecting our life. Building an explainable cyber-physical system requires deriving explanations from system events and causality between the system elements. Cyber-physical energy systems such as smart grids involve cyber and physical aspects of energy systems and other elements, namely social and economic. Moreover, a smart-grid scale can range from a small village to a large region across countries. Therefore, integrating these varieties of data and knowledge is a fundamental challenge to build an explainable cyber-physical energy system. This paper aims to use knowledge graph based framework to solve this challenge. The framework consists of an ontology to model and link data from various sources and graph-based algorithm to derive explanations from the events. A simulated demand response scenario covering the above aspects further demonstrates the applicability of this framework.",what Application Domain ?,Smart Grids,,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what method ?,the priori ty program 1103,mathematical,False,False
"Today the Web represents a rich source of labour market data for both public and private operators, as a growing number of job offers are advertised through Web portals and services. In this paper we apply and compare several techniques, namely explicit-rules, machine learning, and LDA-based algorithms to classify a real dataset of Web job offers collected from 12 heterogeneous sources against a standard classification system of occupations.",what method ?,machine learning,,False,False
"While Wikipedia exists in 287 languages, its content is unevenly distributed among them. In this work, we investigate the generation of open domain Wikipedia summaries in underserved languages using structured data from Wikidata. To this end, we propose a neural network architecture equipped with copy actions that learns to generate single-sentence and comprehensible textual summaries from Wikidata triples. We demonstrate the effectiveness of the proposed approach by evaluating it against a set of baselines on two languages of different natures: Arabic, a morphological rich language with a larger vocabulary than English, and Esperanto, a constructed language known for its easy acquisition.",what method ?,a neural network architecture equipped with copy actions,neural network,False,False
"Today the Web represents a rich source of labour market data for both public and private operators, as a growing number of job offers are advertised through Web portals and services. In this paper we apply and compare several techniques, namely explicit-rules, machine learning, and LDA-based algorithms to classify a real dataset of Web job offers collected from 12 heterogeneous sources against a standard classification system of occupations.",what method ?,LDA-based algorithms,,False,False
"The purpose of this paper is to develop an understanding of the knowledge, skills and competencies demanded of early career information systems (IS) graduates in Australia. Online job advertisements from 2006 were collected and investigated using content analysis software to determine the frequencies and patterns of occurrence of specific requirements. This analysis reveals a dominant cluster of core IS knowledge and competency skills that revolves around IS Development as the most frequently required category of knowledge (78% of ads) and is strongly associated with: Business Analysis, Systems Analysis; Management; Operations, Maintenance & Support; Communication Skills; Personal Characteristics; Computer Languages; Data & Information Management; Internet, Intranet, Web Applications; and Software Packages. Identification of the core cluster of IS knowledge and skills - in demand across a wide variety of jobs - is important to better understand employers' needs for and expectations from IS graduates and the implications for education programs. Much less prevalent is the second cluster that includes knowledge and skills at a more technical side of IS (Architecture and Infrastructure, Operating Systems, Networks, and Security). Issues raised include the nature of entry level positions and their role in the preparation of their incumbents for future more senior positions. The findings add an Australian perspective to the literature on information systems job ads and should be of value to educators, employers, as well as current and future IS professionals.",what method ?,content analysis,content analysis,True,True
"Today the Web represents a rich source of labour market data for both public and private operators, as a growing number of job offers are advertised through Web portals and services. In this paper we apply and compare several techniques, namely explicit-rules, machine learning, and LDA-based algorithms to classify a real dataset of Web job offers collected from 12 heterogeneous sources against a standard classification system of occupations.",what method ?,explicit-rules,,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what method ?,the research project “Relation Based Process Modelling,mathematical,False,False
"Hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. The need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the Open Biodiversity Knowledge Management System (OBKMS). This paper presents OpenBiodiv: An OBKMS that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. It is presented as a Linked Open Dataset generated from scientific literature. OpenBiodiv encompasses data extracted from more than 5000 scholarly articles published by Pensoft and many more taxonomic treatments extracted by Plazi from journals of other publishers. The data from both sources are converted to Resource Description Framework (RDF) and integrated in a graph database using the OpenBiodiv-O ontology and an RDF version of the Global Biodiversity Information Facility (GBIF) taxonomic backbone. Through the application of semantic technologies, the project showcases the value of open publishing of Findable, Accessible, Interoperable, Reusable (FAIR) data towards the establishment of open science practices in the biodiversity domain.",what method ?,a graph database,,False,False
"A two-dimensional model of microwave-induced plasma (field frequency 2.45 GHz) in argon at atmospheric pressure is presented. The model describes in a self-consistent manner the gas flow and heat transfer, the in-coupling of the microwave energy into the plasma, and the reaction kinetics relevant to high-pressure argon plasma including the contribution of molecular ion species. The model provides the gas and electron temperature distributions, the electron, ion, and excited state number densities, and the power deposited into the plasma for given gas flow rate and temperature at the inlet, and input power of the incoming TEM microwave. For flow rate and absorbed microwave power typical for analytical applications (200-400 ml/min and 20 W), the plasma is far from thermodynamic equilibrium. The gas temperature reaches values above 2000 K in the plasma region, while the electron temperature is about 1 eV. The electron density reaches a maximum value of about 4 × 10(21) m(-3). The balance of the charged particles is essentially controlled by the kinetics of the molecular ions. For temperatures above 1200 K, quasineutrality of the plasma is provided by the atomic ions, and below 1200 K the molecular ion density exceeds the atomic ion density and a contraction of the discharge is observed. Comparison with experimental data is presented which demonstrates good quantitative and qualitative agreement.",what Excitation_frequency ?,2.45 GHz,2. 45 ghz ),False,False
"The atmospheric-pressure helium plasma jet is of emerging interest as a cutting-edge biomedical device for cancer treatment, wound healing and sterilization. Reactive oxygen species such as OH and O radicals are considered to be major factors in the application of biological plasma. In this study, density distribution, temporal behaviour and flux of OH and O radicals on a surface are measured using laser-induced fluorescence. A helium plasma jet is generated by applying pulsed high voltage of 8 kV with 10 kHz using a quartz tube with an inner diameter of 4 mm. To evaluate the relation between the surface condition and active species production, three surfaces are used: dry, wet and rat skin. When the helium flow rate is 1.5 l min−1, radial distribution of OH density on the rat skin surface shows a maximum density of 1.2 × 1013 cm−3 at the centre of the plasma-mediated area, while O atom density shows a maximum of 1.0 × 1015 cm−3 at 2.0 mm radius from the centre of the plasma-mediated area. Their densities in the effluent of the plasma jet are almost constant during the intervals of the discharge pulses because their lifetimes are longer than the pulse interval. Their density distribution depends on the helium flow rate and the surface humidity. With these results, OH and O production mechanisms in the plasma jet and their flux onto the surface are discussed.",what Excitation_frequency ?,10 kHz,10 khz,True,True
"Two-dimensional spatially resolved absolute atomic oxygen densities are measured within an atmospheric pressure micro plasma jet and in its effluent. The plasma is operated in helium with an admixture of 0.5% of oxygen at 13.56 MHz and with a power of 1 W. Absolute atomic oxygen densities are obtained using two photon absorption laser induced fluorescence spectroscopy. The results are interpreted based on measurements of the electron dynamics by phase resolved optical emission spectroscopy in combination with a simple model that balances the production of atomic oxygen with its losses due to chemical reactions and diffusion. Within the discharge, the atomic oxygen density builds up with a rise time of 600 µs along the gas flow and reaches a plateau of 8 × 1015 cm−3. In the effluent, the density decays exponentially with a decay time of 180 µs (corresponding to a decay length of 3 mm at a gas flow of 1.0 slm). It is found that both, the species formation behavior and the maximum distance between the jet nozzle and substrates for possible oxygen treatments of surfaces can be controlled by adjusting the gas flow.",what Excitation_frequency ?,13.56,13. 56,False,False
"We describe the annotation of chemical named entities in scientific text. A set of annotation guidelines defines 5 types of named entities, and provides instructions for the resolution of special cases. A corpus of fulltext chemistry papers was annotated, with an inter-annotator agreement F score of 93%. An investigation of named entity recognition using LingPipe suggests that F scores of 63% are possible without customisation, and scores of 74% are possible with the addition of custom tokenisation and the use of dictionaries.",what description ?,"annotation of chemical named entities in scientific text. A set of annotation guidelines defines 5 types of named entities, and provides instructions for the resolution of special cases.",chemical named entities,False,False
"MOTIVATION The MEDLINE database of biomedical abstracts contains scientific knowledge about thousands of interacting genes and proteins. Automated text processing can aid in the comprehension and synthesis of this valuable information. The fundamental task of identifying gene and protein names is a necessary first step towards making full use of the information encoded in biomedical text. This remains a challenging task due to the irregularities and ambiguities in gene and protein nomenclature. We propose to approach the detection of gene and protein names in scientific abstracts as part-of-speech tagging, the most basic form of linguistic corpus annotation. RESULTS We present a method for tagging gene and protein names in biomedical text using a combination of statistical and knowledge-based strategies. This method incorporates automatically generated rules from a transformation-based part-of-speech tagger, and manually generated rules from morphological clues, low frequency trigrams, indicator terms, suffixes and part-of-speech information. Results of an experiment on a test corpus of 56K MEDLINE documents demonstrate that our method to extract gene and protein names can be applied to large sets of MEDLINE abstracts, without the need for special conditions or human experts to predetermine relevant subsets. AVAILABILITY The programs are available on request from the authors.",what description ?,We present a method for tagging gene and protein names in biomedical text using a combination of statistical and knowledge-based strategies,,False,False
"Leukemia is a fatal cancer and has two main types: Acute and chronic. Each type has two more subtypes: Lymphoid and myeloid. Hence, in total, there are four subtypes of leukemia. This study proposes a new approach for diagnosis of all subtypes of leukemia from microscopic blood cell images using convolutional neural networks (CNN), which requires a large training data set. Therefore, we also investigated the effects of data augmentation for an increasing number of training samples synthetically. We used two publicly available leukemia data sources: ALL-IDB and ASH Image Bank. Next, we applied seven different image transformation techniques as data augmentation. We designed a CNN architecture capable of recognizing all subtypes of leukemia. Besides, we also explored other well-known machine learning algorithms such as naive Bayes, support vector machine, k-nearest neighbor, and decision tree. To evaluate our approach, we set up a set of experiments and used 5-fold cross-validation. The results we obtained from experiments showed that our CNN model performance has 88.25% and 81.74% accuracy, in leukemia versus healthy and multi-class classification of all subtypes, respectively. Finally, we also showed that the CNN model has a better performance than other well-known machine learning algorithms.",what description ?,"Leukemia is a fatal cancer and has two main types: Acute and chronic. Each type has two more subtypes: Lymphoid and myeloid. Hence, in total, there are four subtypes of leukemia. This study proposes a new approach for diagnosis of all subtypes of leukemia from microscopic blood cell images using convolutional neural networks (CNN), which requires a large training data set","leukemia is a fatal cancer and has two main types : acute and chronic. each type has two more subtypes : lymphoid and myeloid. hence, in total, there are four subtypes of leukemia. this study proposes a new approach for diagnosis of all subtypes of leukemia from microscopic blood cell images using convolutional neural networks ( cnn )",False,False
"This paper presents the results of the 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas. The shared task featured two independent tracks, and participants submitted machine translation systems for up to 10 indigenous languages. Overall, 8 teams participated with a total of 214 submissions. We provided training sets consisting of data collected from various sources, as well as manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages.",what description ?,Open Machine Translation for Indigenous Languages of the Americas,official baseline,False,False
"The BioCreative NLM-Chem track calls for a community effort to fine-tune automated recognition of chemical names in biomedical literature. Chemical names are one of the most searched biomedical entities in PubMed and – as highlighted during the COVID-19 pandemic – their identification may significantly advance research in multiple biomedical subfields. While previous community challenges focused on identifying chemical names mentioned in titles and abstracts, the full text contains valuable additional detail. We organized the BioCreative NLM-Chem track to call for a community effort to address automated chemical entity recognition in full-text articles. The track consisted of two tasks: 1) Chemical Identification task, and 2) Chemical Indexing prediction task. For the Chemical Identification task, participants were expected to predict with high accuracy all chemicals mentioned in recently published full-text articles, both span (i.e., named entity recognition) and normalization (i.e., entity linking) using MeSH. For the Chemical Indexing task, participants identified which chemicals should be indexed as topics for the article's topic terms in the NLM article and indexing, i.e., appear in the listing of MeSH terms for the document. This manuscript summarizes the BioCreative NLM-Chem track. We received a total of 88 submissions in total from 17 teams worldwide. The highest performance achieved for the Chemical Identification task was 0.8672 f-score (0.8759 precision, 0.8587 recall) for strict NER performance and 0.8136 f-score (0.8621 precision, 0.7702 recall) for strict normalization performance. The highest performance achieved for the Chemical Indexing task was 0.4825 f-score (0.4397 precision, 0.5344 recall). The NLM-Chem track dataset and other challenge materials are publicly available at https://ftp.ncbi.nlm.nih.gov/pub/lu/BC7-NLM-Chem-track/. This community challenge demonstrated 1) the current substantial achievements in deep learning technologies can be utilized to further improve automated prediction accuracy, and 2) the Chemical Indexing task is substantially more challenging. We look forward to further development of biomedical text mining methods to respond to the rapid growth of biomedical literature. Keywords— biomedical text mining; natural language processing; artificial intelligence; machine learning; deep learning; text mining; chemical entity recognition; chemical indexing",what description ?,"the BioCreative NLM-Chem track to call for a community effort to address automated chemical entity recognition in full-text articles. The track consisted of two tasks: 1) Chemical Identification task, and 2) Chemical Indexing prediction task. For the Chemical Identification task, participants were expected to predict with high accuracy all chemicals mentioned in recently published full-text articles, both span (i.e., named entity recognition) and normalization (i.e., entity linking) using MeSH. For the Chemical Indexing task, participants identified which chemicals should be indexed as topics for the article's topic terms in the NLM article and indexing, i.e., appear in the listing of MeSH terms for the document.",chemical names in biomedical literature. chemical names,False,False
"Abstract Background Our goal in BioCreAtIve has been to assess the state of the art in text mining, with emphasis on applications that reflect real biological applications, e.g., the curation process for model organism databases. This paper summarizes the BioCreAtIvE task 1B, the ""Normalized Gene List"" task, which was inspired by the gene list supplied for each curated paper in a model organism database. The task was to produce the correct list of unique gene identifiers for the genes and gene products mentioned in sets of abstracts from three model organisms (Yeast, Fly, and Mouse). Results Eight groups fielded systems for three data sets (Yeast, Fly, and Mouse). For Yeast, the top scoring system (out of 15) achieved 0.92 F-measure (harmonic mean of precision and recall); for Mouse and Fly, the task was more difficult, due to larger numbers of genes, more ambiguity in the gene naming conventions (particularly for Fly), and complex gene names (for Mouse). For Fly, the top F-measure was 0.82 out of 11 systems and for Mouse, it was 0.79 out of 16 systems. Conclusion This assessment demonstrates that multiple groups were able to perform a real biological task across a range of organisms. The performance was dependent on the organism, and specifically on the naming conventions associated with each organism. These results hold out promise that the technology can provide partial automation of the curation process in the near future.",what description ?,"The task was to produce the correct list of unique gene identifiers for the genes and gene products mentioned in sets of abstracts from three model organisms (Yeast, Fly, and Mouse).","normalized gene list """,False,False
"MOTIVATION The MEDLINE database of biomedical abstracts contains scientific knowledge about thousands of interacting genes and proteins. Automated text processing can aid in the comprehension and synthesis of this valuable information. The fundamental task of identifying gene and protein names is a necessary first step towards making full use of the information encoded in biomedical text. This remains a challenging task due to the irregularities and ambiguities in gene and protein nomenclature. We propose to approach the detection of gene and protein names in scientific abstracts as part-of-speech tagging, the most basic form of linguistic corpus annotation. RESULTS We present a method for tagging gene and protein names in biomedical text using a combination of statistical and knowledge-based strategies. This method incorporates automatically generated rules from a transformation-based part-of-speech tagger, and manually generated rules from morphological clues, low frequency trigrams, indicator terms, suffixes and part-of-speech information. Results of an experiment on a test corpus of 56K MEDLINE documents demonstrate that our method to extract gene and protein names can be applied to large sets of MEDLINE abstracts, without the need for special conditions or human experts to predetermine relevant subsets. AVAILABILITY The programs are available on request from the authors.",what description ?,"We propose to approach the detection of gene and protein names in scientific abstracts as part-of-speech tagging, the most basic form of linguistic corpus annotation",,False,False
"NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.",what description ?,"NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora.","natural language toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready - to - use computational linguistics courseware. nltk covers symbolic and statistical natural language processing,",False,False
"ABSTRACT Question: Do specific environmental conditions affect the performance and growth dynamics of one of the most invasive taxa (Carpobrotus aff. acinaciformis) on Mediterranean islands? Location: Four populations located on Mallorca, Spain. Methods: We monitored growth rates of main and lateral shoots of this stoloniferous plant for over two years (2002–2003), comparing two habitats (rocky coast vs. coastal dune) and two different light conditions (sun vs. shade). In one population of each habitat type, we estimated electron transport rate and the level of plant stress (maximal photochemical efficiency Fv/Fm) by means of chlorophyll fluorescence. Results: Main shoots of Carpobrotus grew at similar rates at all sites, regardless habitat type. However, growth rate of lateral shoots was greater in shaded plants than in those exposed to sunlight. Its high phenotypic plasticity, expressed in different allocation patterns in sun and shade individuals, and its clonal growth which promotes the continuous sea...",what Specific traits ?,Growth rates of main and lateral shoots,performance,False,False
"Invasiveness may result from genetic variation and adaptation or phenotypic plasticity, and genetic variation in fitness traits may be especially critical. Pennisetum setaceum (fountain grass, Poaceae) is highly invasive in Hawaii (HI), moderately invasive in Arizona (AZ), and less invasive in southern California (CA). In common garden experiments, we examined the relative importance of quantitative trait variation, precipitation, and phenotypic plasticity in invasiveness. In two very different environments, plants showed no differences by state of origin (HI, CA, AZ) in aboveground biomass, seeds/flower, and total seed number. Plants from different states were also similar within watering treatment. Plants with supplemental watering, relative to unwatered plants, had greater biomass, specific leaf area (SLA), and total seed number, but did not differ in seeds/flower. Progeny grown from seeds produced under different watering treatments showed no maternal effects in seed mass, germination, biomass or SLA. High phenotypic plasticity, rather than local adaptation is likely responsible for variation in invasiveness. Global change models indicate that temperature and precipitation patterns over the next several decades will change, although the direction of change is uncertain. Drier summers in southern California may retard further invasion, while wetter summers may favor the spread of fountain grass.",what Specific traits ?,Biomass,,False,False
"Background: In temperate mountains, most non-native plant species reach their distributional limit somewhere along the elevational gradient. However, it is unclear if growth limitations can explain upper range limits and whether phenotypic plasticity or genetic changes allow species to occupy a broad elevational gradient. Aims: We investigated how non-native plant individuals from different elevations responded to growing season temperatures, which represented conditions at the core and margin of the elevational distributions of the species. Methods: We recorded the occurrence of nine non-native species in the Swiss Alps and subsequently conducted a climate chamber experiment to assess growth rates of plants from different elevations under different temperature treatments. Results: The elevational limit observed in the field was not related to the species' temperature response in the climate chamber experiment. Almost all species showed a similar level of reduction in growth rates under lower temperatures independent of the upper elevational limit of the species' distribution. For two species we found indications for genetic differentiation among plants from different elevations. Conclusions: We conclude that factors other than growing season temperatures, such as extreme events or winter mortality, might shape the elevational limit of non-native species, and that ecological filtering might select for genotypes that are phenotypically plastic.",what Specific traits ?,Growth rates of plants from different elevations under different temperature treatments,"extreme events or winter mortality,",False,False
"Due to altered ecological and evolutionary contexts, we might expect the responses of alien plants to environmental gradients, as revealed through patterns of trait variation, to differ from those of the same species in their native range. In particular, the spread of alien plant species along such gradients might be limited by their ability to establish clinal patterns of trait variation. We investigated trends in growth and reproductive traits in natural populations of eight invasive Asteraceae forbs along altitudinal gradients in their native and introduced ranges (Valais, Switzerland, and Wallowa Mountains, Oregon, USA). Plants showed similar responses to altitude in both ranges, being generally smaller and having fewer inflorescences but larger seeds at higher altitudes. However, these trends were modified by region-specific effects that were independent of species status (native or introduced), suggesting that any differential performance of alien species in the introduced range cannot be interpreted without a fully reciprocal approach to test the basis of these differences. Furthermore, we found differences in patterns of resource allocation to capitula among species in the native and the introduced areas. These suggest that the mechanisms underlying trait variation, for example, increasing seed size with altitude, might differ between ranges. The rapid establishment of clinal patterns of trait variation in the new range indicates that the need to respond to altitudinal gradients, possibly by local adaptation, has not limited the ability of these species to invade mountain regions. Studies are now needed to test the underlying mechanisms of altitudinal clines in traits of alien species.",what Specific traits ?,Trends in growth and reproductive traits,growth and reproductive traits,False,False
"Background COVID-19, caused by the novel SARS-CoV-2, is considered the most threatening respiratory infection in the world, with over 40 million people infected and over 0.934 million related deaths reported worldwide. It is speculated that epidemiological and clinical features of COVID-19 may differ across countries or continents. Genomic comparison of 48,635 SARS-CoV-2 genomes has shown that the average number of mutations per sample was 7.23, and most SARS-CoV-2 strains belong to one of 3 clades characterized by geographic and genomic specificity: Europe, Asia, and North America. Objective The aim of this study was to compare the genomes of SARS-CoV-2 strains isolated from Italy, Sweden, and Congo, that is, 3 different countries in the same meridian (longitude) but with different climate conditions, and from Brazil (as an outgroup country), to analyze similarities or differences in patterns of possible evolutionary pressure signatures in their genomes. Methods We obtained data from the Global Initiative on Sharing All Influenza Data repository by sampling all genomes available on that date. Using HyPhy, we achieved the recombination analysis by genetic algorithm recombination detection method, trimming, removal of the stop codons, and phylogenetic tree and mixed effects model of evolution analyses. We also performed secondary structure prediction analysis for both sequences (mutated and wild-type) and “disorder” and “transmembrane” analyses of the protein. We analyzed both protein structures with an ab initio approach to predict their ontologies and 3D structures. Results Evolutionary analysis revealed that codon 9628 is under episodic selective pressure for all SARS-CoV-2 strains isolated from the 4 countries, suggesting it is a key site for virus evolution. Codon 9628 encodes the P0DTD3 (Y14_SARS2) uncharacterized protein 14. Further investigation showed that the codon mutation was responsible for helical modification in the secondary structure. The codon was positioned in the more ordered region of the gene (41-59) and near to the area acting as the transmembrane (54-67), suggesting its involvement in the attachment phase of the virus. The predicted protein structures of both wild-type and mutated P0DTD3 confirmed the importance of the codon to define the protein structure. Moreover, ontological analysis of the protein emphasized that the mutation enhances the binding probability. Conclusions Our results suggest that RNA secondary structure may be affected and, consequently, the protein product changes T (threonine) to G (glycine) in position 50 of the protein. This position is located close to the predicted transmembrane region. Mutation analysis revealed that the change from G (glycine) to D (aspartic acid) may confer a new function to the protein—binding activity, which in turn may be responsible for attaching the virus to human eukaryotic cells. These findings can help design in vitro experiments and possibly facilitate a vaccine design and successful antiviral strategies.",what Has result ?,attachment phase of the virus,"7. 23,",False,False
"ABSTRACT The technology behind big data, although still in its nascent stages, is inspiring many companies to hire data scientists and explore the potential of big data to support strategic initiatives, including developing new products and services. To better understand the skills and knowledge that are highly valued by industry for jobs within big data, this study reports on an analysis of 1216 job advertisements that contained “big data” in the job title. Our results are presented within a conceptual framework of big data skills categories and confirm the multi-faceted nature of big data job skills. Our research also found that many big data job advertisements emphasize developing analytical information systems and that soft skills remain highly valued, in addition to the value placed on emerging hard technological skills.",what Has result ?,conceptual framework of big data skills categories,confirm the multi - faceted nature of big data job skills.,False,False
"Petroleum hydrocarbons contamination of soil, sediments and marine environment associated with the inadvertent discharges of petroleum–derived chemical wastes and petroleum hydrocarbons associated with spillage and other sources into the environment often pose harmful effects on human health and the natural environment, and have negative socio–economic impacts in the oil–producing host communities. In practice, plants and microbes have played a major role in microbial transformation and growth–linked mineralization of petroleum hydrocarbons in contaminated soils and/or sediments over the past years. Bioremediation strategies has been recognized as an environmental friendly and cost–effective alternative in comparison with the traditional physico-chemical approaches for the restoration and reclamation of contaminated sites. The success of any plant–based remediation strategy depends on the interaction of plants with rhizospheric microbial populations in the surrounding soil medium and the organic contaminant. Effective understanding of the fate and behaviour of organic contaminants in the soil can help determine the persistence of the contaminant in the terrestrial environment, promote the success of any bioremediation approach and help develop a high–level of risks mitigation strategies. In this review paper, we provide a clear insight into the role of plants and microbes in the microbial degradation of petroleum hydrocarbons in contaminated soil that have emerged from the growing body of bioremediation research and its applications in practice. In addition, plant–microbe interactions have been discussed with respect to biodegradation of petroleum hydrocarbons and these could provide a better understanding of some important factors necessary for development of in situ bioremediation strategies for risks mitigation in petroleum hydrocarbon–contaminated soil.",what Has result ?,Bioremediation strategies has been recognized as an environmental friendly and cost–effective alternative in comparison with the traditional physico-chemical approaches for the restoration and reclamation of contaminated sites. The success of any plant–based remediation strategy depends on the interaction of plants with rhizospheric microbial populations in the surrounding soil medium and the organic contaminant. ,,False,False
"<jats:title>ABSTRACT</jats:title>
<jats:p>The clinical performances of six molecular diagnostic tests and a rapid antigen test for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) were clinically evaluated for the diagnosis of coronavirus disease 2019 (COVID-19) in self-collected saliva. Saliva samples from 103 patients with laboratory-confirmed COVID-19 (15 asymptomatic and 88 symptomatic) were collected on the day of hospital admission. SARS-CoV-2 RNA in saliva was detected using a quantitative reverse transcription-PCR (RT-qPCR) laboratory-developed test (LDT), a cobas SARS-CoV-2 high-throughput system, three direct RT-qPCR kits, and reverse transcription–loop-mediated isothermal amplification (RT-LAMP). The viral antigen was detected by a rapid antigen immunochromatographic assay. Of the 103 samples, viral RNA was detected in 50.5 to 81.6% of the specimens by molecular diagnostic tests, and an antigen was detected in 11.7% of the specimens by the rapid antigen test. Viral RNA was detected at significantly higher percentages (65.6 to 93.4%) in specimens collected within 9 days of symptom onset than in specimens collected after at least 10 days of symptoms (22.2 to 66.7%) and in specimens collected from asymptomatic patients (40.0 to 66.7%). Self-collected saliva is an alternative specimen option for diagnosing COVID-19. The RT-qPCR LDT, a cobas SARS-CoV-2 high-throughput system, direct RT-qPCR kits (except for one commercial kit), and RT-LAMP showed sufficient sensitivities in clinical use to be selectively used in clinical settings and facilities. The rapid antigen test alone is not recommended for an initial COVID-19 diagnosis because of its low sensitivity.</jats:p>",what Has result ?,11.7,clinical performances,False,False
"Abstract Background Data papers have emerged as a powerful instrument for open data publishing, obtaining credit, and establishing priority for datasets generated in scientific experiments. Academic publishing improves data and metadata quality through peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. Objective We aimed to establish a new type of article structure and template for omics studies: the omics data paper. To improve data interoperability and further incentivize researchers to publish well-described datasets, we created a prototype workflow for streamlined import of genomics metadata from the European Nucleotide Archive directly into a data paper manuscript. Methods An omics data paper template was designed by defining key article sections that encourage the description of omics datasets and methodologies. A metadata import workflow, based on REpresentational State Transfer services and Xpath, was prototyped to extract information from the European Nucleotide Archive, ArrayExpress, and BioSamples databases. Findings The template and workflow for automatic import of standard-compliant metadata into an omics data paper manuscript provide a mechanism for enhancing existing metadata through publishing. Conclusion The omics data paper structure and workflow for import of genomics metadata will help to bring genomic and other omics datasets into the spotlight. Promoting enhanced metadata descriptions and enforcing manuscript peer review and data auditing of the underlying datasets brings additional quality to datasets. We hope that streamlined metadata reuse for scholarly publishing encourages authors to create enhanced metadata descriptions in the form of data papers to improve both the quality of their metadata and its findability and accessibility.",what Has result ?,data import workflow,findability and accessibility.,False,False
"Knowledge graphs in manufacturing and production aim to make production lines more efficient and flexible with higher quality output. This makes knowledge graphs attractive for companies to reach Industry 4.0 goals. However, existing research in the field is quite preliminary, and more research effort on analyzing how knowledge graphs can be applied in the field of manufacturing and production is needed. Therefore, we have conducted a systematic literature review as an attempt to characterize the state-of-the-art in this field, i.e., by identifying existing research and by identifying gaps and opportunities for further research. We have focused on finding the primary studies in the existing literature, which were classified and analyzed according to four criteria: bibliometric key facts, research type facets, knowledge graph characteristics, and application scenarios. Besides, an evaluation of the primary studies has also been carried out to gain deeper insights in terms of methodology, empirical evidence, and relevance. As a result, we can offer a complete picture of the domain, which includes such interesting aspects as the fact that knowledge fusion is currently the main use case for knowledge graphs, that empirical research and industrial application are still missing to a large extent, that graph embeddings are not fully exploited, and that technical literature is fast-growing but still seems to be far from its peak.",what Has result ?," knowledge fusion is currently the main use case for knowledge graphs,","evaluation of the primary studies has also been carried out to gain deeper insights in terms of methodology, empirical evidence, and relevance. as a result, we can offer a complete picture of the domain,",False,False
"The development of electronic health records, wearable devices, health applications and Internet of Things (IoT)-empowered smart homes is promoting various applications. It also makes health self-management much more feasible, which can partially mitigate one of the challenges that the current healthcare system is facing. Effective and convenient self-management of health requires the collaborative use of health data and home environment data from different services, devices, and even open data on the Web. Although health data interoperability standards including HL7 Fast Healthcare Interoperability Resources (FHIR) and IoT ontology including Semantic Sensor Network (SSN) have been developed and promoted, it is impossible for all the different categories of services to adopt the same standard in the near future. This study presents a method that applies Semantic Web technologies to integrate the health data and home environment data from heterogeneously built services and devices. We propose a Web Ontology Language (OWL)-based integration ontology that models health data from HL7 FHIR standard implemented services, normal Web services and Web of Things (WoT) services and Linked Data together with home environment data from formal ontology-described WoT services. It works on the resource integration layer of the layered integration architecture. An example use case with a prototype implementation shows that the proposed method successfully integrates the health data and home environment data into a resource graph. The integrated data are annotated with semantics and ontological links, which make them machine-understandable and cross-system reusable.",what Has result ?,successfully integrates the health data and home environment data into a resource graph,,False,False
"Coastal safety may be influenced by climate change, as changes in extreme surge levels and wave extremes may increase the vulnerability of dunes and other coastal defenses. In the North Sea, an area already prone to severe flooding, these high surge levels and waves are generated by low atmospheric pressure and severe wind speeds during storm events. As a result of the geometry of the North Sea, not only the maximum wind speed is relevant, but also wind direction. Climate change could change maximum wind conditions, with potentially negative effects for coastal safety. Here, we use an ensemble of 12 Coupled Model Intercomparison Project Phase 5 (CMIP5) General Circulation Models (GCMs) and diagnose the effect of two climate scenarios (rcp4.5 and rcp8.5) on annual maximum wind speed, wind speeds with lower return frequencies, and the direction of these annual maximum wind speeds. The 12 selected CMIP5 models do not project changes in annual maximum wind speed and in wind speeds with lower return frequencies; however, we do find an indication that the annual extreme wind events are coming more often from western directions. Our results are in line with the studies based on CMIP3 models and do not confirm the statement based on some reanalysis studies that there is a climate‐change‐related upward trend in storminess in the North Sea area.",what Has result ?,n indication that the annual extreme wind events are coming more often from western directions,climate ‐ change ‐ related upward trend in storminess,False,False
"Background COVID-19, caused by the novel SARS-CoV-2, is considered the most threatening respiratory infection in the world, with over 40 million people infected and over 0.934 million related deaths reported worldwide. It is speculated that epidemiological and clinical features of COVID-19 may differ across countries or continents. Genomic comparison of 48,635 SARS-CoV-2 genomes has shown that the average number of mutations per sample was 7.23, and most SARS-CoV-2 strains belong to one of 3 clades characterized by geographic and genomic specificity: Europe, Asia, and North America. Objective The aim of this study was to compare the genomes of SARS-CoV-2 strains isolated from Italy, Sweden, and Congo, that is, 3 different countries in the same meridian (longitude) but with different climate conditions, and from Brazil (as an outgroup country), to analyze similarities or differences in patterns of possible evolutionary pressure signatures in their genomes. Methods We obtained data from the Global Initiative on Sharing All Influenza Data repository by sampling all genomes available on that date. Using HyPhy, we achieved the recombination analysis by genetic algorithm recombination detection method, trimming, removal of the stop codons, and phylogenetic tree and mixed effects model of evolution analyses. We also performed secondary structure prediction analysis for both sequences (mutated and wild-type) and “disorder” and “transmembrane” analyses of the protein. We analyzed both protein structures with an ab initio approach to predict their ontologies and 3D structures. Results Evolutionary analysis revealed that codon 9628 is under episodic selective pressure for all SARS-CoV-2 strains isolated from the 4 countries, suggesting it is a key site for virus evolution. Codon 9628 encodes the P0DTD3 (Y14_SARS2) uncharacterized protein 14. Further investigation showed that the codon mutation was responsible for helical modification in the secondary structure. The codon was positioned in the more ordered region of the gene (41-59) and near to the area acting as the transmembrane (54-67), suggesting its involvement in the attachment phase of the virus. The predicted protein structures of both wild-type and mutated P0DTD3 confirmed the importance of the codon to define the protein structure. Moreover, ontological analysis of the protein emphasized that the mutation enhances the binding probability. Conclusions Our results suggest that RNA secondary structure may be affected and, consequently, the protein product changes T (threonine) to G (glycine) in position 50 of the protein. This position is located close to the predicted transmembrane region. Mutation analysis revealed that the change from G (glycine) to D (aspartic acid) may confer a new function to the protein—binding activity, which in turn may be responsible for attaching the virus to human eukaryotic cells. These findings can help design in vitro experiments and possibly facilitate a vaccine design and successful antiviral strategies.",what Has result ?,codon mutation,"7. 23,",False,False
"Research and development activities are one of the main drivers for progress, economic growth and wellbeing in many societies. This article proposes a text mining approach applied to a large amount of data extracted from job vacancies advertisements, aiming to shed light on the main skills and demands that characterize first stage research positions in Europe. Results show that data handling and processing skills are essential for early career researchers, irrespective of their research field. Also, as many analyzed first stage research positions are connected to universities, they include teaching activities to a great extent. Management of time, risks, projects, and resources plays an important part in the job requirements included in the analyzed advertisements. Such information is relevant not only for early career researchers who perform job selection taking into account the match of possessed skills with the required ones, but also for educational institutions that are responsible for skills development of the future R&D professionals.",what Has result ?,"data handling and processing skills are essential for early career researchers, irrespective of their research field","data handling and processing skills are essential for early career researchers, irrespective of their research field.",True,True
"This paper presents the results of the two shared tasks associated with W-NUT 2015: (1) a text normalization task with 10 participants; and (2) a named entity tagging task with 8 participants. We outline the task, annotation process and dataset statistics, and provide a high-level overview of the participating systems for each shared task.",what Total teams ?,8 participants,10 participants ;,False,False
"This paper presents the results of the Twitter Named Entity Recognition shared task associated with W-NUT 2016: a named entity tagging task with 10 teams participating. We outline the shared task, annotation process and dataset statistics, and provide a high-level overview of the participating systems for each shared task.",what Total teams ?,10 teams,10 teams,True,True
"Considering recent progress in NLP, deep learning techniques and biomedical language models there is a pressing need to generate annotated resources and comparable evaluation scenarios that enable the development of advanced biomedical relation extraction systems that extract interactions between drugs/chemical entities and genes, proteins or miRNAs. Building on the results and experience of the CHEMDNER, CHEMDNER patents and ChemProt tracks, we have posed the DrugProt track at BioCreative VII. The DrugProt track focused on the evaluation of automatic systems able to extract 13 different types of drug-genes/protein relations of importance to understand gene regulatory and pharmacological mechanisms. The DrugProt track addressed regulatory associations (direct/indirect, activator/inhibitor relations), certain types of binding associations (antagonist and agonist relations) as well as metabolic associations (substrate or product relations). To promote development of novel tools and offer a comparative evaluation scenario we have released 61,775 manually annotated gene mentions, 65,561 chemical and drug mentions and a total of 24,526 relationships manually labeled by domain experts. A total of 30 teams submitted results for the DrugProt main track, while 9 teams submitted results for the large-scale text mining subtrack that required processing of over 2,3 million records. Teams obtained very competitive results, with predictions reaching fmeasures of over 0.92 for some relation types (antagonist) and fmeasures across all relation types close to 0.8. INTRODUCTION Among the most relevant biological and pharmacological relation types are those that involve (a) chemical compounds and drugs as well as (b) gene products including genes, proteins, miRNAs. A variety of associations between chemicals and genes/proteins are described in the biomedical literature, and there is a growing interest in facilitating a more systematic extraction of these relations from the literature, either for manual database curation initiatives or to generate large knowledge graphs of importance for drug discovery, drug repurposing, building regulatory or interaction networks or to characterize off-target interactions of drugs that might be of importance to understand better adverse drug reactions. At BioCreative VI, the ChemProt track tried to promote the development of novel systems between chemicals and genes for groups of biologically related association types (ChemProt track relation groups or CPRs). Although the obtained results did have a considerable impact in the development and evaluation of new biomedical relation extraction systems, a limitation of grouping more specific relation types into broader groups was the difficulty to directly exploit the results for database curation efforts and biomedical knowledge graph mining application scenarios. The considerable interest in the integration of chemical and biomedical data for drug-discovery purposes, together with the ongoing curation of relationships between biological and chemical entities from scientific publications and patents due to the recent COVID-19 pandemic, motivated the DrugProt track of BioCreative VII, which proposed using more granular relation types. In order to facilitate the development of more granular relation extraction systems large manually annotated corpora are needed. Those corpora should include high-quality manually labled entity mentions together with exhaustive relation annotations generated by domain experts. TRACK AND CORPUS DESCRIPTION Corpus description To carry out the DrugProt track at BioCreative VII, we have released a large manually labelled corpus including annotations of mentions of chemical compounds and drugs as well as genes, proteins and miRNAs. Domain experts with experience in biomedical literature annotation and database curation annotated by hand all abstracts using the BRAT annotation interface. The manual labeling of chemicals and genes was done in separate steps and by different experts to avoid introducing biases during the text annotation process. The manual tagging of entity mentions of chemicals and drugs as well as genes, proteins and miRNAs was done following a carefully designed annotation process and in line with publicly released annotation guidelines. Gene/protein entity mentions were manually mapped to their corresponding biologic al database identifiers whenever possible and classified as either normalizable to databases (tag: GENE-Y) or non normalizable mentions (GENE-N). Teams that participated at the DrugProt track were only provided with this classification of gene mentions and not the actual database identifier to avoid usage of external knowledge bases for producing their predictions. The corpus construction process required first annotating exhaustively all chemical and gene mentions (phase 1). Afterwards the relation annotation phase followed (phase 2), were relationships between these two types of entities had to be labeled according to public available annotation guidelines. Thus, to facilitate the annotation of chemical-protein interactions, the DrugProt track organizers constructed very granular relation annotation rules described in a 33 pages annotation guidelines document. These guidelines were refined during an iterative process based on the annotation of sample documents. The guidelines provided the basic details of the chemicalprotein interaction annotation task and the conventions that had to be followed during the corpus construction process. They incorporated suggestions made by curators as well as observations of annotation inconsistencies encountered when comparing results from different human curators. In brief, DrugProt interactions covered direct interactions (when a physical contact existed between a chemical/drug and a gene/protein) as well as indirect regulatory interactions that alter either the function or the quantity of the gene/gene product. The aim of the iterative manual annotation cycle was to improve the quality and consistency of the guidelines. During the planning of the guidelines some rules had to be reformulated to make them more explicit and clear and additional rules were added wherever necessary to better cover the practical annotation scenario and for being more complete. The manual annotation task basically consisted of labeling or marking manually through a customized BRAT webinterface the interactions given the article abstracts as content. Figure 1 summarizes the DrugProt relation types included in the annotation guidelines. Fig. 1. Overview of the DrugProt relation type hierarchy. The corpus annotation carried out for the DrugProt track was exhaustive for all the types of interactions previously specified. This implied that mentions of other kind of relationships between chemicals and genes (e.g. phenotypic and biological responses) were not manually labelled. Moreover, the DrugProt relations are directed in the sense that only relations of “what a chemical does to a gene/protein"" (chemical → gene/protein direction) were annotated, and not vice versa. To establish a easy to understand relation nomenclature and avoid redundant class definitions, we reviewed several chemical repositories that included chemical – biology information. We revised DrugBank, the Therapeutic Targets Database (TTD) and ChEMBL, assay normalization ontologies (BAO) and previously existing formalizations for the annotation of relationships: the Biological Expression Language (BEL), curation guidelines for transcription regulation interactions (DNA-binding transcription factor – target gene interaction) and SIGNOR, a database of causal relationships between biological entities. Each of these resources inspired the definition of the subclasses DIRECT REGULATOR (e.g. DrugBank, ChEMBL, BAO and SIGNOR) and the INDIRECT REGULATOR (e.g. BEL, curation guidelines for transcription regulation interactions and SIGNOR). For example, DrugBank relationships for drugs included a total of 22 definitions, some of them overlapping with CHEMPROT subclasses (e.g. “Inhibitor”, “Antagonist”, “Agonist”,...), some of them being regarded as highly specific for the purpose of this task (e.g. “intercalation”, “cross-linking/alkylation”) or referring to biological roles (e.g. “Antibody”, “Incorporation into and Destabilization”) and others, partially overlapping between them (e.g. “Binder” and “Ligand”), that were merged into a single class. Concerning indirect regulatory aspects, the five classes of casual relationships between a subject and an object term defined by BEL (“decreases”, “directlyDecreases”, “increases”, “directlyIncreases” and “causesNoChange”) were highly inspiring. Subclasses definitions of pharmacological modes of action were defined according to the UPHAR/BPS Guide to Pharmacology in 2016. For the DrugProt track a very granular chemical-protein relation annotation was carried out, with the aim to cover most of the relations that are of importance from the point of view of biochemical and pharmacological/biomedical perspective. Nevertheless, for the DrugProt track only a total of 13 relation types were used, keeping those that had enough training instances/examples and sufficient manual annotation consistency. The final list of relation types used for this shared task was: INDIRECT-DOWNREGULATOR, INDIRECTUPREGULATOR, DIRECT-REGULATOR, ACTIVATOR, INHIBITOR, AGONIST, ANTAGONIST, AGONISTACTIVATOR, AGONIST-INHIBITOR, PRODUCT-OF, SUBSTRATE, SUBSTRATE_PRODUCT-OF or PART-OF. The DrugProt corpus was split randomly into training, development and test set. We also included a background and large scale background collection of records that were automatically annotated with drugs/chemicals and genes/proteins/miRNAs using an entity tagger trained on the manual DrugProt entity mentions. The background collections were merged with the test set to be able to get team predictions also for these records. Table 1 shows a su",what Total teams ?,9 teams submitted results for the large-scale text mining subtrack,30 teams,False,False
"Amid the intensive competition among global industries, the relationship between manufacturers and suppliers has turned from antagonist to cooperative. Through partnerships, both parties can be mutually benefited, and the key factor that maintains such relationship lies in how manufacturers select proper suppliers. The purpose of this study is to explore the key factors considered by manufacturers in supplier selection and the relationships between these factors. Through a literature review, eight supplier selection factors, comprising price response capability, quality management capability, technological capability, delivery capability, flexible capability, management capability, commercial image, and financial capability are derived. Based on the theoretic foundation proposed by previous researchers, a causal model of supplier selection factors is further constructed. The results of a survey on high-tech industries are used to verify the relationships between the eight factors using structural equation modelling (SEM). Based on the empirical results, conclusions and suggestions are finally proposed as a reference for manufacturers and suppliers.",what Critical success factors ?,quality management capability,"price response capability, quality management capability, technological capability, delivery capability, flexible capability,",False,True
"Amid the intensive competition among global industries, the relationship between manufacturers and suppliers has turned from antagonist to cooperative. Through partnerships, both parties can be mutually benefited, and the key factor that maintains such relationship lies in how manufacturers select proper suppliers. The purpose of this study is to explore the key factors considered by manufacturers in supplier selection and the relationships between these factors. Through a literature review, eight supplier selection factors, comprising price response capability, quality management capability, technological capability, delivery capability, flexible capability, management capability, commercial image, and financial capability are derived. Based on the theoretic foundation proposed by previous researchers, a causal model of supplier selection factors is further constructed. The results of a survey on high-tech industries are used to verify the relationships between the eight factors using structural equation modelling (SEM). Based on the empirical results, conclusions and suggestions are finally proposed as a reference for manufacturers and suppliers.",what Critical success factors ?,Price response capability,"price response capability, quality management capability, technological capability, delivery capability, flexible capability,",False,True
"Problem statement: Based on a literature survey, an attempt has been made in this study to develop a framework for identifying the success factors. In addition, a list of key success factors is presented. The emphasis is on success factors dealing with breadth of services, internationalization of operations, industry focus, customer focus, 3PL experience, relationship with 3PLs, investment in quality assets, investment in information systems, availability of skilled professionals and supply chain integration. In developing the factors an effort has been made to align and relate them to financial performance. Conclusion/Recommendations: We found success factors “relationship with 3PLs and skilled logistics professionals” would substantially improves financial performance metric profit growth. Our findings also contribute to managerial practice by offering a benchmarking tool that can be used by managers in the 3PL service provider industry in India.",what Critical success factors ?,supply chain integration,"customer focus, 3pl experience,",False,False
"Purpose – The purpose of this paper is to identify and evaluate the critical success factors (CSFs) responsible for supplier development (SD) in a manufacturing supply chain environment.Design/methodology/approach – In total, 13 CSFs for SD are identified (i.e. long‐term strategic goal; top management commitment; incentives; supplier's supplier condition; proximity to manufacturing base; supplier certification; innovation capability; information sharing; environmental readiness; external environment; project completion experience; supplier status and direct involvement) through extensive literature review and discussion held with managers/engineers in different Indian manufacturing companies. A fuzzy analytic hierarchy process (FAHP) is proposed and developed to evaluate the degree of impact of each CSF on SD.Findings – The degree of impact for each CSF on SD is established for an Indian company. The results are discussed in detail with managerial implications. The long‐term strategic goal is found to be ...",what Critical success factors ?,Top management commitment,supplier status and direct involvement ),False,False
"Extranet is an enabler/system that enriches the information service quality in e-supply chain. This paper uses factor analysis to determine four extranet success factors: system quality, information quality, service quality, and work performance quality. A critical analysis of areas that require improvement is also conducted.",what Critical success factors ?,work performance quality,"system quality, information quality, service quality, and work performance quality.",False,True
"The purpose of this paper is to shed the light on the critical success factors that lead to high supply chain performance outcomes in a Malaysian manufacturing company. The critical success factors consist of relationship with customer and supplier, information communication and technology (ICT), material flow management, corporate culture and performance measurement. Questionnaire was the main instrument for the study and it was distributed to 84 staff from departments of purchasing, planning, logistics and operation. Data analysis was conducted by employing descriptive analysis (mean and standard deviation), reliability analysis, Pearson correlation analysis and multiple regression. The findings show that there are relationships exist between relationship with customer and supplier, ICT, material flow management, performance measurement and supply chain management (SCM) performance, but not for corporate culture. Forming a good customer and supplier relationship is the main predictor of SCM performance, followed by performance measurement, material flow management and ICT. It is recommended that future study to determine additional success factors that are pertinent to firms’ current SCM strategies and directions, competitive advantages and missions. Logic suggests that further study to include more geographical data coverage, other nature of businesses and research instruments. Key words: Supply chain management, critical success factor.",what Critical success factors ?,material flow management,,False,False
"Purpose – The aim of this paper is threefold: first, to examine the content of supply chain quality management (SCQM); second, to identify the structure of SCQM; and third, to show ways for finding improvement opportunities and organizing individual institution's resources/actions into collective performance outcomes.Design/methodology/approach – To meet the goals of this work, the paper uses abductive reasoning and two qualitative methods: content analysis and formal concept analysis (FCA). Primary data were collected from both original design manufacturers (ODMs) and original equipment manufacturers (OEMs) in Taiwan.Findings – According to the qualitative empirical study, modern enterprises need to pay immediate attention to the following two pathways: a compliance approach and a voluntary approach. For the former, three strategic content variables are identified: training programs, ISO, and supplier quality audit programs. As for initiating a voluntary effort, modern lead firms need to instill “motivat...",what Critical success factors ?,quality management,"training programs, iso, and supplier quality audit programs.",False,False
"Amid the intensive competition among global industries, the relationship between manufacturers and suppliers has turned from antagonist to cooperative. Through partnerships, both parties can be mutually benefited, and the key factor that maintains such relationship lies in how manufacturers select proper suppliers. The purpose of this study is to explore the key factors considered by manufacturers in supplier selection and the relationships between these factors. Through a literature review, eight supplier selection factors, comprising price response capability, quality management capability, technological capability, delivery capability, flexible capability, management capability, commercial image, and financial capability are derived. Based on the theoretic foundation proposed by previous researchers, a causal model of supplier selection factors is further constructed. The results of a survey on high-tech industries are used to verify the relationships between the eight factors using structural equation modelling (SEM). Based on the empirical results, conclusions and suggestions are finally proposed as a reference for manufacturers and suppliers.",what Critical success factors ?,management capability,"price response capability, quality management capability, technological capability, delivery capability, flexible capability,",False,True
"Purpose – The purpose of this paper is to identify and evaluate the critical success factors (CSFs) responsible for supplier development (SD) in a manufacturing supply chain environment.Design/methodology/approach – In total, 13 CSFs for SD are identified (i.e. long‐term strategic goal; top management commitment; incentives; supplier's supplier condition; proximity to manufacturing base; supplier certification; innovation capability; information sharing; environmental readiness; external environment; project completion experience; supplier status and direct involvement) through extensive literature review and discussion held with managers/engineers in different Indian manufacturing companies. A fuzzy analytic hierarchy process (FAHP) is proposed and developed to evaluate the degree of impact of each CSF on SD.Findings – The degree of impact for each CSF on SD is established for an Indian company. The results are discussed in detail with managerial implications. The long‐term strategic goal is found to be ...",what Critical success factors ?,environmental readiness,supplier status and direct involvement ),False,False
"This paper reports the results of a survey on the critical success factors (CSFs) of web-based supply-chain management systems (WSCMS). An empirical study was conducted and an exploratory factor analysis of the survey data revealed five major dimensions of the CSFs for WSCMS implementation, namely (1) communication, (2) top management commitment, (3) data security, (4) training and education, and (5) hardware and software reliability. The findings of the results provide insights for companies using or planning to use WSCMS.",what Critical success factors ?,training and education,hardware and software reliability.,False,False
"Purpose – The purpose of this paper is to determine those factors perceived by users to influence the successful on‐going use of e‐commerce systems in business‐to‐business (B2B) buying and selling transactions through examination of the views of individuals acting in both purchasing and selling roles within the UK National Health Service (NHS) pharmaceutical supply chain.Design/methodology/approach – Literature from the fields of operations and supply chain management (SCM) and information systems (IS) is used to determine candidate factors that might influence the success of the use of e‐commerce. A questionnaire based on these is used for primary data collection in the UK NHS pharmaceutical supply chain. Factor analysis is used to analyse the data.Findings – The paper yields five composite factors that are perceived by users to influence successful e‐commerce use. “System quality,” “information quality,” “management and use,” “world wide web – assurance and empathy,” and “trust” are proposed as potentia...",what Critical success factors ?,trust,"system quality, ” “ information quality, ” “ management and use, ” “ world wide web – assurance and empathy, ” and “ trust ”",False,True
"This study is the first attempt that assembled published academic work on critical success factors (CSFs) in supply chain management (SCM) fields. The purpose of this study are to review the CSFs in SCM and to uncover the major CSFs that are apparent in SCM literatures. This study apply literature survey techniques from published CSFs studies in SCM. A collection of 42 CSFs studies in various SCM fields are obtained from major databases. The search uses keywords such as as supply chain management, critical success factors, logistics management and supply chain drivers and barriers. From the literature survey, four major CSFs are proposed. The factors are collaborative partnership, information technology, top management support and human resource. It is hoped that this review will serve as a platform for future research in SCM and CSFs studies. Plus, this study contribute to existing SCM knowledge and further appraise the concept of CSFs.",what Critical success factors ?,collaborative partnership,"information technology, top management support and human resource.",False,False
"This paper reports the results of a survey on the critical success factors (CSFs) of web-based supply-chain management systems (WSCMS). An empirical study was conducted and an exploratory factor analysis of the survey data revealed five major dimensions of the CSFs for WSCMS implementation, namely (1) communication, (2) top management commitment, (3) data security, (4) training and education, and (5) hardware and software reliability. The findings of the results provide insights for companies using or planning to use WSCMS.",what Critical success factors ?,Top management commitment,hardware and software reliability.,False,False
"Extranet is an enabler/system that enriches the information service quality in e-supply chain. This paper uses factor analysis to determine four extranet success factors: system quality, information quality, service quality, and work performance quality. A critical analysis of areas that require improvement is also conducted.",what Critical success factors ?,service quality,"system quality, information quality, service quality, and work performance quality.",False,True
"Purpose – The purpose of this paper is to explore critical factors for implementing green supply chain management (GSCM) practice in the Taiwanese electrical and electronics industries relative to European Union directives.Design/methodology/approach – A tentative list of critical factors of GSCM was developed based on a thorough and detailed analysis of the pertinent literature. The survey questionnaire contained 25 items, developed based on the literature and interviews with three industry experts, specifically quality and product assurance representatives. A total of 300 questionnaires were mailed out, and 87 were returned, of which 84 were valid, representing a response rate of 28 percent. Using the data collected, the identified critical factors were performed via factor analysis to establish reliability and validity.Findings – The results show that 20 critical factors were extracted into four dimensions, which denominated supplier management, product recycling, organization involvement and life cycl...",what Critical success factors ?,Supplier management,"product recycling,",False,False
"Objectives: To estimate the prevalence and incidence of epilepsy in Italy using a national database of general practitioners (GPs). Methods: The Health Search CSD Longitudinal Patient Database (HSD) has been established in 1998 by the Italian College of GPs. Participants were 700 GPs, representing a population of 912,458. For each patient, information on age and sex, EEG, CT scan, and MRI was included. Prevalent cases with a diagnosis of ‘epilepsy' (ICD9CM: 345*) were selected in the 2011 population. Incident cases of epilepsy were identified in 2011 by excluding patients diagnosed for epilepsy and convulsions and those with EEG, CT scan, MRI prescribed for epilepsy and/or convulsions in the previous years. Crude and standardized (Italian population) prevalence and incidence were calculated. Results: Crude prevalence of epilepsy was 7.9 per 1,000 (men 8.1; women 7.7). The highest prevalence was in patients <25 years and ≥75 years. The incidence of epilepsy was 33.5 per 100,000 (women 35.3; men 31.5). The highest incidence was in women <25 years and in men 75 years or older. Conclusions: Prevalence and incidence of epilepsy in this study were similar to those of other industrialized countries. HSD appears as a reliable data source for the surveillance of epilepsy in Italy. i 2014 S. Karger AG, Basel",what Country of study ?,Italy,italy,True,True
"Abstract Purpose: The paper analyzes factors that affect the likelihood of adoption of different agriculture-related information sources by farmers. Design/Methodology/Approach: The paper links the theoretical understanding of the existing multiple sources of information that farmer use, with the empirical model to analyze the factors that affect the farmer's adoption of different agriculture-related information sources. The analysis is done using a multivariate probit model and primary survey data of 1,200 farmer households of five Indo-Gangetic states of India, covering 120 villages. Findings: The results of the study highlight that farmer's age, education level and farm size influence farmer's behaviour in selecting different sources of information. The results show that farmers use multiple information sources, that may be complementary or substitutes to each other and this also implies that any single source does not satisfy all information needs of the farmer. Practical implication: If we understand the likelihood of farmer's choice of source of information then direction can be provided and policies can be developed to provide information through those sources in targeted regions with the most effective impact. Originality/Value: Information plays a key role in a farmer's life by enhancing their knowledge and strengthening their decision-making ability. Farmers use multiple sources of information as no one source is sufficient in itself.",what Country of study ?,India,"india,",True,True
"Science communication only reaches certain segments of society. Various underserved audiences are detached from it and feel left out, which is a challenge for democratic societies that build on informed participation in deliberative processes. While only recently researchers and practitioners have addressed the question on the detailed composition of the not reached groups, even less is known about the emotional impact on underserved audiences: feelings and emotions can play an important role in how science communication is received, and “feeling left out” can be an important aspect of exclusion. In this exploratory study, we provide insights from interviews and focus groups with three different underserved audiences in Germany. We found that on the one hand, material exclusion factors such as available infrastructure or financial means as well as specifically attributable factors such as language skills, are influencing the audience composition of science communication. On the other hand, emotional exclusion factors such as fear, habitual distance, and self- as well as outside-perception also play an important role. Therefore, simply addressing material aspects can only be part of establishing more inclusive science communication practices. Rather, being aware of emotions and feelings can serve as a point of leverage for science communication in reaching out to underserved audiences.",what Country of study ?,Germany,germany.,True,True
"ABSTRACT This study has investigated farm households' simultaneous use of social networks, field extension, traditional media, and modern information and communication technologies (ICTs) to access information on cotton crop production. The study was based on a field survey, conducted in Punjab, Pakistan. Data were collected from 399 cotton farm households using the multistage sampling technique. Important combinations of information sources were found in terms of their simultaneous use to access information. The study also examined the factors influencing the use of various available information sources. A multivariate probit model was used considering the correlation among the use of social networks, field extension, traditional media, and modern ICTs. The findings indicated the importance of different socioeconomic and institutional factors affecting farm households' use of available information sources on cotton production. Important policy conclusions are drawn based on findings.",what Country of study ?,Pakistan,pakistan.,True,True
"Abstract Background As the COVID-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. We determine the incubation period and serial interval distribution for transmission clusters in Singapore and in Tianjin. We infer the basic reproduction number and identify the extent of pre-symptomatic transmission. Methods We collected outbreak information from Singapore and Tianjin, China, reported from Jan.19-Feb.26 and Jan.21-Feb.27, respectively. We estimated incubation periods and serial intervals in both populations. Results The mean incubation period was 7.1 (6.13, 8.25) days for Singapore and 9 (7.92, 10.2) days for Tianjin. Both datasets had shorter incubation periods for earlier-occurring cases. The mean serial interval was 4.56 (2.69, 6.42) days for Singapore and 4.22 (3.43, 5.01) for Tianjin. We inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (Singapore, Tianjin). The estimated basic reproduction number for Singapore was 1.97 (1.45, 2.48) secondary cases per infective; for Tianjin it was 1.87 (1.65, 2.09) secondary cases per infective. Conclusions Estimated serial intervals are shorter than incubation periods in both Singapore and Tianjin, suggesting that pre-symptomatic transmission is occurring. Shorter serial intervals lead to lower estimates of R0, which suggest that half of all secondary infections should be prevented to control spread.",what Study location ?,Singapore,"singapore and in tianjin. we infer the basic reproduction number and identify the extent of pre - symptomatic transmission. methods we collected outbreak information from singapore and tianjin, china,",False,True
"With ongoing research, increased information sharing and knowledge exchange, humanitarian organizations have an increasing amount of evidence at their disposal to support their decisions. Nevertheless, effectively building decisions on the increasing amount of insights and information remains challenging. At the individual, organizational, and environmental levels, various factors influence the use of evidence in the decision-making process. This research examined these factors and specifically their influence in a case-study on humanitarian organizations and their WASH interventions in Uganda. Interviewees reported several factors that impede the implementation of evidence-based decision making. Revealing that, despite advancements in the past years, evidence-based information itself is relatively small, contradictory, and non-repeatable. Moreover, the information is often not connected or in a format that can be acted upon. Most importantly, however, are the human aspects and organizational settings that limit access to and use of supporting data, information, and evidence. This research shows the importance of considering these factors, in addition to invest in creating knowledge and technologies to support evidence-based decision-making.",what Study location ?,Uganda,uganda.,True,True
"Humanitarian disasters are highly dynamic and uncertain. The shifting situation, volatility of information, and the emergence of decision processes and coordination structures require humanitarian organizations to continuously adapt their operations. In this study, we aim to make headway in understanding adaptive decision-making in a dynamic interplay between changing situation, volatile information, and emerging coordination structures. Starting from theories of sensemaking, coordination, and decision-making, we present two case studies that represent the response to two different humanitarian disasters: Typhoon Haiyan in the Philippines, and the Syria Crisis, one of the most prominent ongoing conflicts. For both, we highlight how volatile information and the urge to respond via sensemaking lead to fragmentation and misalignment of emergent coordination structures and decisions, which, in turn, slow down adaptation. Based on the case studies, we derive propositions and the need to continuously align laterally between different regions and hierarchically between operational and strategic levels to avoid persistence of coordination-information bubbles. We discuss the implications of our findings for the development of methods and theory to ensure that humanitarian operations management captures the critical role of information as a driver of emergent coordination and adaptive decisions.",what Study location ?,The Philippines,"philippines,",False,False
"ABSTRACT This paper identifies the characteristics of smart cities as they emerge from the recent literature. It then examines whether and in what way these characteristics are present in the smart city plans of 15 cities: Amsterdam, Barcelona, London, PlanIT Valley, Stockholm, Cyberjaya, Singapore, King Abdullah Economic City, Masdar, Skolkovo, Songdo, Chicago, New York, Rio de Janeiro, and Konza. The results are presented with respect to each smart city characteristic. As expected, most strategies emphasize the role of information and communication technologies in improving the functionality of urban systems and advancing knowledge transfer and innovation networks. However, this research yields other interesting findings that may not yet have been documented across multiple case studies; for example, most smart city strategies fail to incorporate bottom-up approaches, are poorly adapted to accommodate the local needs of their area, and consider issues of privacy and security inadequately.",what has smart city instance ?,Chicago,"amsterdam,",False,False
"Abstract. In 2017 we published a seminal research study in the International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences about how smart city tools, solutions and applications underpinned historical and cultural heritage of cities at that time (Angelidou et al. 2017). We now return to investigate the progress that has been made during the past three years, and specifically whether the weak substantiation of cultural heritage in smart city strategies that we observed in 2017 has been improved. The newest literature suggests that smart cities should capitalize on local strengths and give prominence to local culture and traditions and provides a handful of solutions to this end. However, a more thorough examination of what has been actually implemented reveals a (still) rather immature approach. The smart city cases that were selected for the purposes of this research include Tarragona (Spain), Budapest (Hungary) and Karlsruhe (Germany). For each one we collected information regarding the overarching structure of the initiative, the positioning of cultural heritage and the inclusion of heritage-related smart city applications. We then performed a comparative analysis based on a simplified version of the Digital Strategy Canvas. Our findings suggest that a rich cultural heritage and a broader strategic focus on touristic branding and promotion are key ingredients of smart city development in this domain; this is a commonality of all the investigated cities. Moreover, three different strategy architectures emerge, representing the different interplays among the smart city, cultural heritage and sustainable urban development. We conclude that a new generation of smart city initiatives is emerging, in which cultural heritage is of increasing importance. This generation tends to associate cultural heritage with social and cultural values, liveability and sustainable urban development.",what has smart city instance ?,Budapest (Hungary),,False,False
"Abstract. In 2017 we published a seminal research study in the International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences about how smart city tools, solutions and applications underpinned historical and cultural heritage of cities at that time (Angelidou et al. 2017). We now return to investigate the progress that has been made during the past three years, and specifically whether the weak substantiation of cultural heritage in smart city strategies that we observed in 2017 has been improved. The newest literature suggests that smart cities should capitalize on local strengths and give prominence to local culture and traditions and provides a handful of solutions to this end. However, a more thorough examination of what has been actually implemented reveals a (still) rather immature approach. The smart city cases that were selected for the purposes of this research include Tarragona (Spain), Budapest (Hungary) and Karlsruhe (Germany). For each one we collected information regarding the overarching structure of the initiative, the positioning of cultural heritage and the inclusion of heritage-related smart city applications. We then performed a comparative analysis based on a simplified version of the Digital Strategy Canvas. Our findings suggest that a rich cultural heritage and a broader strategic focus on touristic branding and promotion are key ingredients of smart city development in this domain; this is a commonality of all the investigated cities. Moreover, three different strategy architectures emerge, representing the different interplays among the smart city, cultural heritage and sustainable urban development. We conclude that a new generation of smart city initiatives is emerging, in which cultural heritage is of increasing importance. This generation tends to associate cultural heritage with social and cultural values, liveability and sustainable urban development.",what has smart city instance ?,Tarragona (Spain),,False,False
"ABSTRACT This paper identifies the characteristics of smart cities as they emerge from the recent literature. It then examines whether and in what way these characteristics are present in the smart city plans of 15 cities: Amsterdam, Barcelona, London, PlanIT Valley, Stockholm, Cyberjaya, Singapore, King Abdullah Economic City, Masdar, Skolkovo, Songdo, Chicago, New York, Rio de Janeiro, and Konza. The results are presented with respect to each smart city characteristic. As expected, most strategies emphasize the role of information and communication technologies in improving the functionality of urban systems and advancing knowledge transfer and innovation networks. However, this research yields other interesting findings that may not yet have been documented across multiple case studies; for example, most smart city strategies fail to incorporate bottom-up approaches, are poorly adapted to accommodate the local needs of their area, and consider issues of privacy and security inadequately.",what has smart city instance ?,PlanIT Valley,"amsterdam,",False,False
"ABSTRACT This paper identifies the characteristics of smart cities as they emerge from the recent literature. It then examines whether and in what way these characteristics are present in the smart city plans of 15 cities: Amsterdam, Barcelona, London, PlanIT Valley, Stockholm, Cyberjaya, Singapore, King Abdullah Economic City, Masdar, Skolkovo, Songdo, Chicago, New York, Rio de Janeiro, and Konza. The results are presented with respect to each smart city characteristic. As expected, most strategies emphasize the role of information and communication technologies in improving the functionality of urban systems and advancing knowledge transfer and innovation networks. However, this research yields other interesting findings that may not yet have been documented across multiple case studies; for example, most smart city strategies fail to incorporate bottom-up approaches, are poorly adapted to accommodate the local needs of their area, and consider issues of privacy and security inadequately.",what has smart city instance ?,Masdar,"amsterdam,",False,False
"Abstract A full description of the ModelE version of the Goddard Institute for Space Studies (GISS) atmospheric general circulation model (GCM) and results are presented for present-day climate simulations (ca. 1979). This version is a complete rewrite of previous models incorporating numerous improvements in basic physics, the stratospheric circulation, and forcing fields. Notable changes include the following: the model top is now above the stratopause, the number of vertical layers has increased, a new cloud microphysical scheme is used, vegetation biophysics now incorporates a sensitivity to humidity, atmospheric turbulence is calculated over the whole column, and new land snow and lake schemes are introduced. The performance of the model using three configurations with different horizontal and vertical resolutions is compared to quality-controlled in situ data, remotely sensed and reanalysis products. Overall, significant improvements over previous models are seen, particularly in upper-atmosphere te...",what Earth System Model ?,Atmosphere,atmosphere,True,True
"The NCEP Climate Forecast System Reanalysis (CFSR) was completed for the 31-yr period from 1979 to 2009, in January 2010. The CFSR was designed and executed as a global, high-resolution coupled atmosphere–ocean–land surface–sea ice system to provide the best estimate of the state of these coupled domains over this period. The current CFSR will be extended as an operational, real-time product into the future. New features of the CFSR include 1) coupling of the atmosphere and ocean during the generation of the 6-h guess field, 2) an interactive sea ice model, and 3) assimilation of satellite radiances by the Gridpoint Statistical Interpolation (GSI) scheme over the entire period. The CFSR global atmosphere resolution is ~38 km (T382) with 64 levels extending from the surface to 0.26 hPa. The global ocean's latitudinal spacing is 0.25° at the equator, extending to a global 0.5° beyond the tropics, with 40 levels to a depth of 4737 m. The global land surface model has four soil levels and the global sea ice m...",what Earth System Model ?,Atmosphere,ocean –,False,False
"The NCEP Climate Forecast System Reanalysis (CFSR) was completed for the 31-yr period from 1979 to 2009, in January 2010. The CFSR was designed and executed as a global, high-resolution coupled atmosphere–ocean–land surface–sea ice system to provide the best estimate of the state of these coupled domains over this period. The current CFSR will be extended as an operational, real-time product into the future. New features of the CFSR include 1) coupling of the atmosphere and ocean during the generation of the 6-h guess field, 2) an interactive sea ice model, and 3) assimilation of satellite radiances by the Gridpoint Statistical Interpolation (GSI) scheme over the entire period. The CFSR global atmosphere resolution is ~38 km (T382) with 64 levels extending from the surface to 0.26 hPa. The global ocean's latitudinal spacing is 0.25° at the equator, extending to a global 0.5° beyond the tropics, with 40 levels to a depth of 4737 m. The global land surface model has four soil levels and the global sea ice m...",what Earth System Model ?,Sea Ice,ocean –,False,False
"4OASIS3.2–5 coupling framework. The primary goal of the ACCESS-CM development is to provide the Australian climate community with a new generation fully coupled climate model for climate research, and to participate in phase five of the Coupled Model Inter-comparison Project (CMIP5). This paper describes the ACCESS-CM framework and components, and presents the control climates from two versions of the ACCESS-CM, ACCESS1.0 and ACCESS1.3, together with some fields from the 20 th century historical experiments, as part of model evaluation. While sharing the same ocean sea-ice model (except different setups for a few parameters), ACCESS1.0 and ACCESS1.3 differ from each other in their atmospheric and land surface components: the former is configured with the UK Met Office HadGEM2 (r1.1) atmospheric physics and the Met Office Surface Exchange Scheme land surface model version 2, and the latter with atmospheric physics similar to the UK Met Office Global Atmosphere 1.0 includ ing modifications performed at CAWCR and the CSIRO Community Atmosphere Biosphere Land Exchange land surface model version 1.8. The global average annual mean surface air temperature across the 500-year preindustrial control integrations show a warming drift of 0.35 °C in ACCESS1.0 and 0.04 °C in ACCESS1.3. The overall skills of ACCESS-CM in simulating a set of key climatic fields both globally and over Australia significantly surpass those from the preceding CSIRO Mk3.5 model delivered to the previous coupled model inter-comparison. However, ACCESS-CM, like other CMIP5 models, has deficiencies in various as pects, and these are also discussed.",what Earth System Model ?,Land Surface,atmosphere,False,False
"Abstract. The recently developed Norwegian Earth System Model (NorESM) is employed for simulations contributing to the CMIP5 (Coupled Model Intercomparison Project phase 5) experiments and the fifth assessment report of the Intergovernmental Panel on Climate Change (IPCC-AR5). In this manuscript, we focus on evaluating the ocean and land carbon cycle components of the NorESM, based on the preindustrial control and historical simulations. Many of the observed large scale ocean biogeochemical features are reproduced satisfactorily by the NorESM. When compared to the climatological estimates from the World Ocean Atlas (WOA), the model simulated temperature, salinity, oxygen, and phosphate distributions agree reasonably well in both the surface layer and deep water structure. However, the model simulates a relatively strong overturning circulation strength that leads to noticeable model-data bias, especially within the North Atlantic Deep Water (NADW). This strong overturning circulation slightly distorts the structure of the biogeochemical tracers at depth. Advancements in simulating the oceanic mixed layer depth with respect to the previous generation model particularly improve the surface tracer distribution as well as the upper ocean biogeochemical processes, particularly in the Southern Ocean. Consequently, near-surface ocean processes such as biological production and air–sea gas exchange, are in good agreement with climatological observations. The NorESM adopts the same terrestrial model as the Community Earth System Model (CESM1). It reproduces the general pattern of land-vegetation gross primary productivity (GPP) when compared to the observationally based values derived from the FLUXNET network of eddy covariance towers. While the model simulates well the vegetation carbon pool, the soil carbon pool is smaller by a factor of three relative to the observational based estimates. The simulated annual mean terrestrial GPP and total respiration are slightly larger than observed, but the difference between the global GPP and respiration is comparable. Model-data bias in GPP is mainly simulated in the tropics (overestimation) and in high latitudes (underestimation). Within the NorESM framework, both the ocean and terrestrial carbon cycle models simulate a steady increase in carbon uptake from the preindustrial period to the present-day. The land carbon uptake is noticeably smaller than the observations, which is attributed to the strong nitrogen limitation formulated by the land model.",what Earth System Model ?,Ocean,noresm ),False,False
"The NCEP Climate Forecast System Reanalysis (CFSR) was completed for the 31-yr period from 1979 to 2009, in January 2010. The CFSR was designed and executed as a global, high-resolution coupled atmosphere–ocean–land surface–sea ice system to provide the best estimate of the state of these coupled domains over this period. The current CFSR will be extended as an operational, real-time product into the future. New features of the CFSR include 1) coupling of the atmosphere and ocean during the generation of the 6-h guess field, 2) an interactive sea ice model, and 3) assimilation of satellite radiances by the Gridpoint Statistical Interpolation (GSI) scheme over the entire period. The CFSR global atmosphere resolution is ~38 km (T382) with 64 levels extending from the surface to 0.26 hPa. The global ocean's latitudinal spacing is 0.25° at the equator, extending to a global 0.5° beyond the tropics, with 40 levels to a depth of 4737 m. The global land surface model has four soil levels and the global sea ice m...",what Earth System Model ?,Land Surface,ocean –,False,False
"4OASIS3.2–5 coupling framework. The primary goal of the ACCESS-CM development is to provide the Australian climate community with a new generation fully coupled climate model for climate research, and to participate in phase five of the Coupled Model Inter-comparison Project (CMIP5). This paper describes the ACCESS-CM framework and components, and presents the control climates from two versions of the ACCESS-CM, ACCESS1.0 and ACCESS1.3, together with some fields from the 20 th century historical experiments, as part of model evaluation. While sharing the same ocean sea-ice model (except different setups for a few parameters), ACCESS1.0 and ACCESS1.3 differ from each other in their atmospheric and land surface components: the former is configured with the UK Met Office HadGEM2 (r1.1) atmospheric physics and the Met Office Surface Exchange Scheme land surface model version 2, and the latter with atmospheric physics similar to the UK Met Office Global Atmosphere 1.0 includ ing modifications performed at CAWCR and the CSIRO Community Atmosphere Biosphere Land Exchange land surface model version 1.8. The global average annual mean surface air temperature across the 500-year preindustrial control integrations show a warming drift of 0.35 °C in ACCESS1.0 and 0.04 °C in ACCESS1.3. The overall skills of ACCESS-CM in simulating a set of key climatic fields both globally and over Australia significantly surpass those from the preceding CSIRO Mk3.5 model delivered to the previous coupled model inter-comparison. However, ACCESS-CM, like other CMIP5 models, has deficiencies in various as pects, and these are also discussed.",what Earth System Model ?,Ocean,atmosphere,False,False
"The coordination of humanitarian relief, e.g. in a natural disaster or a conflict situation, is often complicated by a scarcity of data to inform planning. Remote sensing imagery, from satellites or drones, can give important insights into conditions on the ground, including in areas which are difficult to access. Applications include situation awareness after natural disasters, structural damage assessment in conflict, monitoring human rights violations or population estimation in settlements. We review machine learning approaches for automating these problems, and discuss their potential and limitations. We also provide a case study of experiments using deep learning methods to count the numbers of structures in multiple refugee settlements in Africa and the Middle East. We find that while high levels of accuracy are possible, there is considerable variation in the characteristics of imagery collected from different sensors and regions. In this, as in the other applications discussed in the paper, critical inferences must be made from a relatively small amount of pixel data. We, therefore, consider that using machine learning systems as an augmentation of human analysts is a reasonable strategy to transition from current fully manual operational pipelines to ones which are both more efficient and have the necessary levels of quality control. This article is part of a discussion meeting issue ‘The growing ubiquity of algorithms in society: implications, impacts and innovations’.",what Study Area ?,Africa,middle east.,False,False
"Gale Crater on Mars has the layered structure of deposit covered by the Noachian/Hesperian boundary. Mineral identification and classification at this region can provide important constrains on environment and geological evolution for Mars. Although Curiosity rove has provided the in-situ mineralogical analysis in Gale, but it restricted in small areas. Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) aboard the Mars Reconnaissance Orbiter (MRO) with enhanced spectral resolution can provide more information in spatial and time scale. In this paper, CRISM near-infrared spectral data are used to identify mineral classes and groups at Martian Gale region. By using diagnostic absorptions features analysis in conjunction with spectral angle mapper (SAM), detailed mineral species are identified at Gale region, e.g., kaolinite, chlorites, smectite, jarosite, and northupite. The clay minerals' diversity in Gale Crater suggests the variation of aqueous alteration. The detection of northupite suggests that the Gale region has experienced the climate change from moist condition with mineral dissolution to dryer climate with water evaporation. The presence of ferric sulfate mineral jarosite formed through the oxidation of iron sulfides in acidic environments shows the experience of acidic sulfur-rich condition in Gale history.",what Study Area ?,Gale crater,gale crater,True,True
"Identification of Martian surface minerals can contribute to understand the Martian environmental change and geological evolution as well as explore the habitability of the Mars. The Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) aboard the Mars Reconnaissance Orbiter (MRO) is covers the visible to near infrared wavelengths along with enhanced spectral resolution, which provides ability to map the mineralogy on Mars. In this paper, based on the spectrum matching, mineral composition and geological evolution of Martian Jezero and Holden crater are analyzed using the MRO CRISM. The hydrated minerals are detected in the studied areas, including the carbonate, hydrated silicate and hydrated sulfate. These minerals suggested that the Holden and Jezero craters have experienced long time water-rock interactions. Also, the diverse alteration minerals found in these regions indicate the aqueous activities in multiple distinct environments.",what Study Area ?,Jezero crater,holden crater,False,False
"CRISM is a hyperspectral imager onboard the Mars Reconnaissance Orbiter (MRO; NASA, 2005) which has been acquiring data since November 2006 and has targeted hydrated minerals previously detected by OMEGA (Mars Express; ESA, 2003). The present study focuses on hydrated minerals detected with CRISM at high spatial resolution in the vicinity of Capri Chasma, a canyon of the Valles Marineris system. CRISM data were processed and coupled with MRO and other spacecraft data, in particular HiRiSE (High Resolution Science Experiment, MRO) images. Detections revealed sulfates in abundance in Capri, especially linked to the interior layered deposits (ILD) that lie in the central part of the chasma. Both monohydrated and polyhydrated sulfates are found at different elevations and are associated with different layers. Monohydrated sulfates are widely detected over the massive light-toned cliffs of the ILD, whereas polyhydrated sulfates seem to form a basal and a top layer associated with lower-albedo deposits in flatter areas. Hydrated silicates (phyllosilicates or opaline silica) have also been detected very locally on two mounds about a few hundred meters in diameter at the bottom of the ILD cliffs. We suggest some formation models of these minerals that are consistent with our observations.",what Study Area ?, Capri Chasma,valles marineris,False,False
"Butterfly monitoring and Red List programs in Switzerland rely on a combination of observations and collection records to document changes in species distributions through time. While most butterflies can be identified using morphology, some taxa remain challenging, making it difficult to accurately map their distributions and develop appropriate conservation measures. In this paper, we explore the use of the DNA barcode (a fragment of the mitochondrial gene COI) as a tool for the identification of Swiss butterflies and forester moths (Rhopalocera and Zygaenidae). We present a national DNA barcode reference library including 868 sequences representing 217 out of 224 resident species, or 96.9% of Swiss fauna. DNA barcodes were diagnostic for nearly 90% of Swiss species. The remaining 10% represent cases of para- and polyphyly likely involving introgression or incomplete lineage sorting among closely related taxa. We demonstrate that integrative taxonomic methods incorporating a combination of morphological and genetic techniques result in a rate of species identification of over 96% in females and over 98% in males, higher than either morphology or DNA barcodes alone. We explore the use of the DNA barcode for exploring boundaries among taxa, understanding the geographical distribution of cryptic diversity and evaluating the status of purportedly endemic taxa. Finally, we discuss how DNA barcodes may be used to improve field practices and ultimately enhance conservation strategies.",what Study Location  ?,Switzerland,switzerland,True,True
"This study summarizes results of a DNA barcoding campaign on German Diptera, involving analysis of 45,040 specimens. The resultant DNA barcode library includes records for 2,453 named species comprising a total of 5,200 barcode index numbers (BINs), including 2,700 COI haplotype clusters without species‐level assignment, so called “dark taxa.” Overall, 88 out of 117 families (75%) recorded from Germany were covered, representing more than 50% of the 9,544 known species of German Diptera. Until now, most of these families, especially the most diverse, have been taxonomically inaccessible. By contrast, within a few years this study provided an intermediate taxonomic system for half of the German Dipteran fauna, which will provide a useful foundation for subsequent detailed, integrative taxonomic studies. Using DNA extracts derived from bulk collections made by Malaise traps, we further demonstrate that species delineation using BINs and operational taxonomic units (OTUs) constitutes an effective method for biodiversity studies using DNA metabarcoding. As the reference libraries continue to grow, and gaps in the species catalogue are filled, BIN lists assembled by metabarcoding will provide greater taxonomic resolution. The present study has three main goals: (a) to provide a DNA barcode library for 5,200 BINs of Diptera; (b) to demonstrate, based on the example of bulk extractions from a Malaise trap experiment, that DNA barcode clusters, labelled with globally unique identifiers (such as OTUs and/or BINs), provide a pragmatic, accurate solution to the “taxonomic impediment”; and (c) to demonstrate that interim names based on BINs and OTUs obtained through metabarcoding provide an effective method for studies on species‐rich groups that are usually neglected in biodiversity research projects because of their unresolved taxonomy.",what Study Location  ?,Germany,germany,True,True
"The identification of Afrotropical hoverflies is very difficult because of limited recent taxonomic revisions and the lack of comprehensive identification keys. In order to assist in their identification, and to improve the taxonomy of this group, we constructed a reference dataset of 513 COI barcodes of 90 of the more common nominal species from Ghana, Togo, Benin and Nigeria (W Africa) and added ten publically available COI barcodes from nine nominal Afrotropical species to this (total: 523 COI barcodes; 98 nominal species; 26 genera). The identification accuracy of this dataset was evaluated with three methods (K2P distance-based, Neighbor-Joining (NJ) / Maximum Likelihood (ML) analysis, and using SpeciesIdentifier). Results of the three methods were highly congruent and showed a high identification success. Nine species pairs showed a low (< 0.03) mean interspecific K2P distance that resulted in several incorrect identifications. A high (> 0.03) maximum intraspecific K2P distance was observed in eight species and barcodes of these species not always formed single clusters in the NJ / ML analayses which may indicate the occurrence of cryptic species. Optimal K2P thresholds to differentiate intra- from interspecific K2P divergence were highly different among the three subfamilies (Eristalinae: 0.037, Syrphinae: 0.06, Microdontinae: 0.007–0.02), and among the different general suggesting that optimal thresholds are better defined at the genus level. In addition to providing an alternative identification tool, our study indicates that DNA barcoding improves the taxonomy of Afrotropical hoverflies by selecting (groups of) taxa that deserve further taxonomic study, and by attributing the unknown sex to species for which only one of the sexes is known.",what Study Location  ?, Nigeria,benin and nigeria ( w africa ),False,True
"Mosquitoes are insects of the Diptera, Nematocera, and Culicidae families, some species of which are important disease vectors. Identifying mosquito species based on morphological characteristics is difficult, particularly the identification of specimens collected in the field as part of disease surveillance programs. Because of this difficulty, we constructed DNA barcodes of the cytochrome c oxidase subunit 1, the COI gene, for the more common mosquito species in China, including the major disease vectors. A total of 404 mosquito specimens were collected and assigned to 15 genera and 122 species and subspecies on the basis of morphological characteristics. Individuals of the same species grouped closely together in a Neighborhood-Joining tree based on COI sequence similarity, regardless of collection site. COI gene sequence divergence was approximately 30 times higher for species in the same genus than for members of the same species. Divergence in over 98% of congeneric species ranged from 2.3% to 21.8%, whereas divergence in conspecific individuals ranged from 0% to 1.67%. Cryptic species may be common and a few pseudogenes were detected.",what Study Location  ?,China,"china,",True,True
"Although central to much biological research, the identification of species is often difficult. The use of DNA barcodes, short DNA sequences from a standardized region of the genome, has recently been proposed as a tool to facilitate species identification and discovery. However, the effectiveness of DNA barcoding for identifying specimens in species-rich tropical biotas is unknown. Here we show that cytochrome c oxidase I DNA barcodes effectively discriminate among species in three Lepidoptera families from Area de Conservación Guanacaste in northwestern Costa Rica. We found that 97.9% of the 521 species recognized by prior taxonomic work possess distinctive cytochrome c oxidase I barcodes and that the few instances of interspecific sequence overlap involve very similar species. We also found two or more barcode clusters within each of 13 supposedly single species. Covariation between these clusters and morphological and/or ecological traits indicates overlooked species complexes. If these results are general, DNA barcoding will significantly aid species identification and discovery in tropical settings.",what Study Location  ?,Costa Rica,costa rica.,True,True
"Abstract A fundamental aspect of well performing cities is successful public spaces. For centuries, understanding these places has been limited to sporadic observations and laborious data collection. This study proposes a novel methodology to analyze citywide, discrete urban spaces using highly accurate anonymized telecom data and machine learning algorithms. Through superposition of human dynamics and urban features, this work aims to expose clear correlations between the design of the city and the behavioral patterns of its users. Geolocated telecom data, obtained for the state of Andorra, were initially analyzed to identify “stay-points”—events in which cellular devices remain within a certain roaming distance for a given length of time. These stay-points were then further analyzed to find clusters of activity characterized in terms of their size, persistence, and diversity. Multivariate linear regression models were used to identify associations between the formation of these clusters and various urban features such as urban morphology or land-use within a 25–50 meters resolution. Some of the urban features that were found to be highly related to the creation of large, diverse and long-lasting clusters were the presence of service and entertainment amenities, natural water features, and the betweenness centrality of the road network; others, such as educational and park amenities were shown to have a negative impact. Ultimately, this study suggests a “reversed urbanism” methodology: an evidence-based approach to urban design, planning, and decision making, in which human behavioral patterns are instilled as a foundational design tool for inferring the success rates of highly performative urban places.",what Study Location  ?,Andorra,"public spaces. for centuries, understanding these places has been limited to sporadic observations and laborious data collection. this study proposes a novel methodology to analyze citywide, discrete urban spaces using highly accurate anonymized telecom data and machine learning algorithms. through superposition of human dynamics and urban features, this work aims to expose clear correlations between the design of the city and the behavioral patterns of its users. geolocated telecom data, obtained for the state of andorra,",False,True
"DNA barcodes were obtained for 81 butterfly species belonging to 52 genera from sites in north‐central Pakistan to test the utility of barcoding for their identification and to gain a better understanding of regional barcode variation. These species represent 25% of the butterfly fauna of Pakistan and belong to five families, although the Nymphalidae were dominant, comprising 38% of the total specimens. Barcode analysis showed that maximum conspecific divergence was 1.6%, while there was 1.7–14.3% divergence from the nearest neighbour species. Barcode records for 55 species showed <2% sequence divergence to records in the Barcode of Life Data Systems (BOLD), but only 26 of these cases involved specimens from neighbouring India and Central Asia. Analysis revealed that most species showed little incremental sequence variation when specimens from other regions were considered, but a threefold increase was noted in a few cases. There was a clear gap between maximum intraspecific and minimum nearest neighbour distance for all 81 species. Neighbour‐joining cluster analysis showed that members of each species formed a monophyletic cluster with strong bootstrap support. The barcode results revealed two provisional species that could not be clearly linked to known taxa, while 24 other species gained their first coverage. Future work should extend the barcode reference library to include all butterfly species from Pakistan as well as neighbouring countries to gain a better understanding of regional variation in barcode sequences in this topographically and climatically complex region.",what Study Location  ?,Pakistan,pakistan,True,True
"Abstract. Carrion-breeding Sarcophagidae (Diptera) can be used to estimate the post-mortem interval in forensic cases. Difficulties with accurate morphological identifications at any life stage and a lack of documented thermobiological profiles have limited their current usefulness. The molecular-based approach of DNA barcoding, which utilises a 648-bp fragment of the mitochondrial cytochrome oxidase subunit I gene, was evaluated in a pilot study for discrimination between 16 Australian sarcophagids. The current study comprehensively evaluated barcoding for a larger taxon set of 588 Australian sarcophagids. In total, 39 of the 84 known Australian species were represented by 580 specimens, which includes 92% of potentially forensically important species. A further eight specimens could not be identified, but were included nonetheless as six unidentifiable taxa. A neighbour-joining tree was generated and nucleotide sequence divergences were calculated. All species except Sarcophaga (Fergusonimyia) bancroftorum, known for high morphological variability, were resolved as monophyletic (99.2% of cases), with bootstrap support of 100. Excluding S. bancroftorum, the mean intraspecific and interspecific variation ranged from 1.12% and 2.81–11.23%, respectively, allowing for species discrimination. DNA barcoding was therefore validated as a suitable method for molecular identification of Australian Sarcophagidae, which will aid in the implementation of this fauna in forensic entomology.",what Study Location  ?, Australia,australian,False,True
"Abstract How introduced plants, which may be locally adapted to specific climatic conditions in their native range, cope with the new abiotic conditions that they encounter as exotics is not well understood. In particular, it is unclear what role plasticity versus adaptive evolution plays in enabling exotics to persist under new environmental circumstances in the introduced range. We determined the extent to which native and introduced populations of St. John's Wort (Hypericum perforatum) are genetically differentiated with respect to leaf-level morphological and physiological traits that allow plants to tolerate different climatic conditions. In common gardens in Washington and Spain, and in a greenhouse, we examined clinal variation in percent leaf nitrogen and carbon, leaf δ13C values (as an integrative measure of water use efficiency), specific leaf area (SLA), root and shoot biomass, root/shoot ratio, total leaf area, and leaf area ratio (LAR). As well, we determined whether native European H. perforatum experienced directional selection on leaf-level traits in the introduced range and we compared, across gardens, levels of plasticity in these traits. In field gardens in both Washington and Spain, native populations formed latitudinal clines in percent leaf N. In the greenhouse, native populations formed latitudinal clines in root and shoot biomass and total leaf area, and in the Washington garden only, native populations also exhibited latitudinal clines in percent leaf C and leaf δ13C. Traits that failed to show consistent latitudinal clines instead exhibited significant phenotypic plasticity. Introduced St. John's Wort populations also formed significant or marginally significant latitudinal clines in percent leaf N in Washington and Spain, percent leaf C in Washington, and in root biomass and total leaf area in the greenhouse. In the Washington common garden, there was strong directional selection among European populations for higher percent leaf N and leaf δ13C, but no selection on any other measured trait. The presence of convergent, genetically based latitudinal clines between native and introduced H. perforatum, together with previously published molecular data, suggest that native and exotic genotypes have independently adapted to a broad-scale variation in climate that varies with latitude.",what Continent ?,Europe,phenotypic plasticity.,False,False
"Both human-related and natural factors can affect the establishment and distribution of exotic species. Understanding the relative role of the different factors has important scientific and applied implications. Here, we examined the relative effect of human-related and natural factors in determining the richness of exotic bird species established across Europe. Using hierarchical partitioning, which controls for covariation among factors, we show that the most important factor is the human-related community-level propagule pressure (the number of exotic species introduced), which is often not included in invasion studies due to the lack of information for this early stage in the invasion process. Another, though less important, factor was the human footprint (an index that includes human population size, land use and infrastructure). Biotic and abiotic factors of the environment were of minor importance in shaping the number of established birds when tested at a European extent using 50×50 km2 grid squares. We provide, to our knowledge, the first map of the distribution of exotic bird richness in Europe. The richest hotspot of established exotic birds is located in southeastern England, followed by areas in Belgium and The Netherlands. Community-level propagule pressure remains the major factor shaping the distribution of exotic birds also when tested for the UK separately. Thus, studies examining the patterns of establishment should aim at collecting the crucial and hard-to-find information on community-level propagule pressure or develop reliable surrogates for estimating this factor. Allowing future introductions of exotic birds into Europe should be reconsidered carefully, as the number of introduced species is basically the main factor that determines the number established.",what Continent ?,Europe,establishment,False,False
"Blue spruce (Picea pungens Engelm.) is native to the central and southern Rocky Mountains of the USA (DAUBENMIRE, 1972), from where it has been introduced to other parts of North America, Europe, etc. In Central Europe, blue spruce was mostly planted in ornamental settings in urban areas, Christmas tree plantations and forests too. In the Slovak Republic, blue spruce has patchy distribution. Its scattered stands cover the area of 2,618 ha and 0.14% of the forest area (data from the National Forest Centre, Zvolen). Compared to the Slovak Republic, the area afforested with blue spruce in the Czech Republic is much larger –8,741 ha and 0.4% of the forest area (KRIVANEK et al., 2006, UHUL, 2006). Plantations of blue spruce in the Czech Republic were largely established in the western and north-western parts of the country (BERAN and SINDELAR, 1996; BALCAR et al., 2008b).",what Continent ?,Europe,"europe,",True,True
"To quantify the relative importance of propagule pressure, climate‐matching and host availability for the invasion of agricultural pest arthropods in Europe and to forecast newly emerging pest species and European areas with the highest risk of arthropod invasion under current climate and a future climate scenario (A1F1).",what Continent ?,Europe,"propagule pressure,",False,False
"Species introduced to novel regions often leave behind many parasite species. Signatures of parasite release could thus be used to resolve cryptogenic (uncertain) origins such as that of Littorina littorea, a European marine snail whose history in North America has been debated for over 100 years. Through extensive field and literature surveys, we examined species richness of parasitic trematodes infecting this snail and two co-occurring congeners, L. saxatilis and L. obtusata, both considered native throughout the North Atlantic. Of the three snails, only L. littorea possessed significantly fewer trematode species in North America, and all North American trematodes infecting the three Littorina spp. were a nested subset of Europe. Surprisingly, several of L. littorea's missing trematodes in North America infected the other Littorina congeners. Most likely, long separation of these trematodes from their former host resulted in divergence of the parasites' recognition of L. littorea. Overall, these patterns of parasitism suggest a recent invasion from Europe to North America for L. littorea and an older, natural expansion from Europe to North America for L. saxatilis and L. obtusata.",what Continent ?,North America,release,False,False
"Norway maple (Acer platanoides L), which is among the most invasive tree species in forests of eastern North America, is associated with reduced regeneration of the related native species, sugar maple (Acer saccharum Marsh) and other native flora. To identify traits conferring an advantage to Norway maple, we grew both species through an entire growing season under simulated light regimes mimicking a closed forest understorey vs. a canopy disturbance (gap). Dynamic shade-houses providing a succession of high-intensity direct-light events between longer periods of low, diffuse light were used to simulate the light regimes. We assessed seedling height growth three times in the season, as well as stem diameter, maximum photosynthetic capacity, biomass allocation above- and below-ground, seasonal phenology and phenotypic plasticity. Given the north European provenance of Norway maple, we also investigated the possibility that its growth in North America might be increased by delayed fall senescence. We found that Norway maple had significantly greater photosynthetic capacity in both light regimes and grew larger in stem diameter than sugar maple. The differences in below- and above-ground biomass, stem diameter, height and maximum photosynthesis were especially important in the simulated gap where Norway maple continued extension growth during the late fall. In the gap regime sugar maple had a significantly higher root : shoot ratio that could confer an advantage in the deepest shade of closed understorey and under water stress or browsing pressure. Norway maple is especially invasive following canopy disturbance where the opposite (low root : shoot ratio) could confer a competitive advantage. Considering the effects of global change in extending the potential growing season, we anticipate that the invasiveness of Norway maple will increase in the future.",what Continent ?,North America,disturbance,False,False
"<jats:title>Abstract</jats:title><jats:p>Russian olive (<jats:italic>Elaeagnus angustifolia</jats:italic> Linnaeus; Elaeagnaceae) is an exotic shrub/tree that has become invasive in many riparian ecosystems throughout semi-arid, western North America, including southern British Columbia, Canada. Despite its prevalence and the potentially dramatic impacts it can have on riparian and aquatic ecosystems, little is known about the insect communities associated with Russian olive within its invaded range. At six sites throughout the Okanagan valley of southern British Columbia, Canada, we compared the diversity of insects associated with Russian olive plants to that of insects associated with two commonly co-occurring native plant species: Woods’ rose (<jats:italic>Rosa woodsii</jats:italic> Lindley; Rosaceae) and Saskatoon (<jats:italic>Amelanchier alnifolia</jats:italic> (Nuttall) Nuttall ex Roemer; Rosaceae). Total abundance did not differ significantly among plant types. Family richness and Shannon diversity differed significantly between Woods’ rose and Saskatoon, but not between either of these plant types and Russian olive. An abundance of Thripidae (Thysanoptera) on Russian olive and Tingidae (Hemiptera) on Saskatoon contributed to significant compositional differences among plant types. The families Chloropidae (Diptera), Heleomyzidae (Diptera), and Gryllidae (Orthoptera) were uniquely associated with Russian olive, albeit in low abundances. Our study provides valuable and novel information about the diversity of insects associated with an emerging plant invader of western Canada.</jats:p>",what Continent ?,North America,plants,False,False
"Introduced hosts populations may benefit of an ""enemy release"" through impoverishment of parasite communities made of both few imported species and few acquired local ones. Moreover, closely related competing native hosts can be affected by acquiring introduced taxa (spillover) and by increased transmission risk of native parasites (spillback). We determined the macroparasite fauna of invasive grey squirrels (Sciurus carolinensis) in Italy to detect any diversity loss, introduction of novel parasites or acquisition of local ones, and analysed variation in parasite burdens to identify factors that may increase transmission risk for native red squirrels (S. vulgaris). Based on 277 grey squirrels sampled from 7 populations characterised by different time scales in introduction events, we identified 7 gastro-intestinal helminths and 4 parasite arthropods. Parasite richness is lower than in grey squirrel's native range and independent from introduction time lags. The most common parasites are Nearctic nematodes Strongyloides robustus (prevalence: 56.6%) and Trichostrongylus calcaratus (6.5%), red squirrel flea Ceratophyllus sciurorum (26.0%) and Holarctic sucking louse Neohaematopinus sciuri (17.7%). All other parasites are European or cosmopolitan species with prevalence below 5%. S. robustus abundance is positively affected by host density and body mass, C. sciurorum abundance increases with host density and varies with seasons. Overall, we show that grey squirrels in Italy may benefit of an enemy release, and both spillback and spillover processes towards native red squirrels may occur.",what Continent ?,Europe,"enemy release """,False,False
"ABSTRACT: Despite mounting evidence of invasive species’ impacts on the environment and society,our ability to predict invasion establishment, spread, and impact are inadequate. Efforts to explainand predict invasion outcomes have been limited primarily to terrestrial and freshwater ecosystems.Invasions are also common in coastal marine ecosystems, yet to date predictive marine invasion mod-els are absent. Here we present a model based on biological attributes associated with invasion suc-cess (establishment) of marine molluscs that compares successful and failed invasions from a groupof 93 species introduced to San Francisco Bay (SFB) in association with commercial oyster transfersfrom eastern North America (ca. 1869 to 1940). A multiple logistic regression model correctly classi-fied 83% of successful and 80% of failed invaders according to their source region abundance at thetime of oyster transfers, tolerance of low salinity, and developmental mode. We tested the generalityof the SFB invasion model by applying it to 3 coastal locations (2 in North America and 1 in Europe)that received oyster transfers from the same source and during the same time as SFB. The model cor-rectly predicted 100, 75, and 86% of successful invaders in these locations, indicating that abun-dance, environmental tolerance (ability to withstand low salinity), and developmental mode not onlyexplain patterns of invasion success in SFB, but more importantly, predict invasion success in geo-graphically disparate marine ecosystems. Finally, we demonstrate that the proportion of marine mol-luscs that succeeded in the latter stages of invasion (i.e. that establish self-sustaining populations,spread and become pests) is much greater than has been previously predicted or shown for otheranimals and plants.KEY WORDS: Invasion · Bivalve · Gastropod · Mollusc · Marine · Oyster · Vector · Risk assessment",what Continent ?,North America,"environment and society,",False,False
"Herbivorous arthropod fauna of the horse nettle Solanum carolinense L., an alien solanaceous herb of North American origin, was characterized by surveying arthropod communities in the fields and comparing them with the original community compiled from published data to infer the impact of herbivores on the weed in the introduced region. Field surveys were carried out in the central part of mainland Japan for five years including an intensive regular survey in 1992. Thirty-nine arthropod species were found feeding on the weed. The leaf, stem, flower and fruit of the weed were infested by the herbivores. The comparison of characteristics of the arthropod community with those of the community in the USA indicated that more sapsuckers and less chewers were on the weed in Japan than in the USA. The community in Japan was composed of high proportions of polyphages and exophages compared to that in the USA. Eighty-seven percent of the species are known to be pests of agricultural crops. Low species diversity of the community was also suggested. The depauperated herbivore community, in terms of feeding habit and niche on S. carolinense, suggested that the weed partly escaped from herbivory in its reproductive parts. The regular population census, however, indicated that a dominant coccinellid beetle, Epilachna vigintioctopunctata, caused a noticeable damage on the leaves of the weed.",what Continent ?,North America,,False,False
"Schinus molle (Peruvian pepper tree) was introduced to South Africa more than 150 years ago and was widely planted, mainly along roads. Only in the last two decades has the species become naturalized and invasive in some parts of its new range, notably in semi-arid savannas. Research is being undertaken to predict its potential for further invasion in South Africa. We studied production, dispersal and predation of seeds, seed banks, and seedling establishment in relation to land uses at three sites, namely ungrazed savanna once used as a military training ground; a savanna grazed by native game; and an ungrazed mine dump. We found that seed production and seed rain density of S. molle varied greatly between study sites, but was high at all sites (384 864–1 233 690 seeds per tree per year; 3877–9477 seeds per square metre per year). We found seeds dispersed to distances of up to 320 m from female trees, and most seeds were deposited within 50 m of putative source trees. Annual seed rain density below canopies of Acacia tortillis, the dominant native tree at all sites, was significantly lower in grazed savanna. The quality of seed rain was much reduced by endophagous predators. Seed survival in the soil was low, with no survival recorded beyond 1 year. Propagule pressure to drive the rate of recruitment: densities of seedlings and sapling densities were higher in ungrazed savanna and the ungrazed mine dump than in grazed savanna, as reflected by large numbers of young individuals, but adult : seedling ratios did not differ between savanna sites. Frequent and abundant seed production, together with effective dispersal of viable S. molle seed by birds to suitable establishment sites below trees of other species to overcome predation effects, facilitates invasion. Disturbance enhances invasion, probably by reducing competition from native plants.",what Continent ?,Africa,,False,False
"The introduction of non-indigenous species (NIS) across the major European seas is a dynamic non-stop process. Up to September 2004, 851 NIS (the majority being zoobenthic organ- isms) have been reported in European marine and brackish waters, the majority during the 1960s and 1970s. The Mediterranean is by far the major recipient of exotic species with an average of one introduction every 4 wk over the past 5 yr. Of the 25 species recorded in 2004, 23 were reported in the Mediterranean and only two in the Baltic. The most updated patterns and trends in the rate, mode of introduction and establishment success of introductions were examined, revealing a process similar to introductions in other parts of the world, but with the uniqueness of migrants through the Suez Canal into the Mediterranean (Lessepsian or Erythrean migration). Shipping appears to be the major vector of introduction (excluding the Lessepsian migration). Aquaculture is also an important vector with target species outnumbered by those introduced unintentionally. More than half of immigrants have been estab- lished in at least one regional sea. However, for a significant part of the introductions both the establishment success and mode of introduction remain unknown. Finally, comparing trends across taxa and seas is not as accurate as could have been wished because there are differences in the spatial and taxonomic effort in the study of NIS. These differences lead to the conclusion that the number of NIS remains an underestimate, calling for continuous updating and systematic research.",what Continent ?,Europe,establishment,False,False
"Biogeographic experiments that test how multiple interacting factors influence exotic plant abundance in their home and recipient communities are remarkably rare. We examined the effects of soil fungi, disturbance and propagule pressure on seed germination, seedling recruitment and adult plant establishment of the invasive Centaurea stoebe in its native European and non‐native North American ranges. Centaurea stoebe can establish virtual monocultures in parts of its non‐native range, but occurs at far lower abundances where it is native. We conducted parallel experiments at four European and four Montana (USA) grassland sites with all factorial combinations of ± suppression of soil fungi, ±disturbance and low versus high knapweed propagule pressure [100 or 300 knapweed seeds per 0.3 m × 0.3 m plot (1000 or 3000 per m2)]. We also measured germination in buried bags containing locally collected knapweed seeds that were either treated or not with fungicide. Disturbance and propagule pressure increased knapweed recruitment and establishment, but did so similarly in both ranges. Treating plots with fungicides had no effect on recruitment or establishment in either range. However, we found: (i) greater seedling recruitment and plant establishment in undisturbed plots in Montana compared to undisturbed plots in Europe and (ii) substantially greater germination of seeds in bags buried in Montana compared to Europe. Also, across all treatments, total plant establishment was greater in Montana than in Europe. Synthesis. Our results highlight the importance of simultaneously examining processes that could influence invasion in both ranges. They indicate that under ‘background’ undisturbed conditions, knapweed recruits and establishes at greater abundance in Montana than in Europe. However, our results do not support the importance of soil fungi or local disturbances as mechanisms for knapweed's differential success in North America versus Europe.",what Continent ?,North America,establishment,False,False
"1. With continued globalization, species are being transported and introduced into novel habitats at an accelerating rate. Interactions between invasive species may provide important mechanisms that moderate their impacts on native species. 2. The European green crab Carcinus maenas is an aggressive predator that was introduced to the east coast of North America in the mid-1800 s and is capable of rapid consumption of bivalve prey. A newer invasive predator, the Asian shore crab Hemigrapsus sanguineus, was first discovered on the Atlantic coast in the 1980s, and now inhabits many of the same regions as C. maenas within the Gulf of Maine. Using a series of field and laboratory investigations, we examined the consequences of interactions between these predators. 3. Density patterns of these two species at different spatial scales are consistent with negative interactions. As a result of these interactions, C. maenas alters its diet to consume fewer mussels, its preferred prey, in the presence of H. sanguineus. Decreased mussel consumption in turn leads to lower growth rates for C. maenas, with potential detrimental effects on C. maenas populations. 4. Rather than an invasional meltdown, this study demonstrates that, within the Gulf of Maine, this new invasive predator can moderate the impacts of the older invasive predator.",what Continent ?,North America,"invasional meltdown,",False,False
"Species become invasive if they (i) are introduced to a new range, (ii) establish themselves, and (iii) spread. To address the global problems caused by invasive species, several studies investigated steps ii and iii of this invasion process. However, only one previous study looked at step i and examined the proportion of species that have been introduced beyond their native range. We extend this research by investigating all three steps for all freshwater fish, mammals, and birds native to Europe or North America. A higher proportion of European species entered North America than vice versa. However, the introduction rate from Europe to North America peaked in the late 19th century, whereas it is still rising in the other direction. There is no clear difference in invasion success between the two directions, so neither the imperialism dogma (that Eurasian species are exceptionally successful invaders) is supported, nor is the contradictory hypothesis that North America offers more biotic resistance to invaders than Europe because of its less disturbed and richer biota. Our results do not support the tens rule either: that approximately 10% of all introduced species establish themselves and that approximately 10% of established species spread. We find a success of approximately 50% at each step. In comparison, only approximately 5% of native vertebrates were introduced in either direction. These figures show that, once a vertebrate is introduced, it has a high potential to become invasive. Thus, it is crucial to minimize the number of species introductions to effectively control invasive vertebrates.",what Continent ?,North America,"global problems caused by invasive species,",False,False
"In a microcosm experiment, I tested how species composition, species rich- ness, and community age affect the susceptibility of grassland communities to invasion by a noxious weed (Centaurea solstitialis L.). I also examined how these factors influenced Centaurea's impact on the rest of the plant community. When grown in monoculture, eight species found in California's grasslands differed widely in their ability to suppress Centaurea growth. The most effective competitor in monoculture was Hemizonia congesta ssp. Iuzulifolia, which, like Centaurea, is a summer- active annual forb. On average, Centaurea growth decreased as the species richness of communities increased. However, no polyculture suppressed Centaurea growth more than the monoculture of Hemizonia. Centaurea generally made up a smaller proportion of com- munity biomass in newly created (""new"") microcosms than in older (""established"") mi- crocosms, largely because Centaurea's competitors were more productive in the new treat- ment. Measures of complementarity suggest that Centaurea partitioned resources with an- nual grasses in the new microcosms. This resource partitioning may help to explain Cen- taurea's great success in western North American grasslands. Centaurea strongly suppressed growth of some species but hardly affected others. An- nual grasses were the least affected species in the new monocultures, and perennial grasses were among the least affected species in the established monocultures. In the new micro- cosms, Centaurea's suppression of competing species marginally abated with increasing species richness. This trend was a consequence of the declining success of Centaurea in species-rich communities, rather than a change in the vulnerability of these communities to suppression by a given amount of the invader. The impact of the invader was not related to species richness in the-established microcosms. The results of this study suggest that, at the neighborhood level, diversity can limit invasibility and may reduce the impact of an invader.",what Continent ?,North America,community,False,False
"At large spatial scales, exotic and native plant diversity exhibit a strong positive relationship. This may occur because exotic and native species respond similarly to processes that influence diversity over large geographical areas. To test this hypothesis, we compared exotic and native species-area relationships within six North American ecoregions. We predicted and found that within ecoregions the ratio of exotic to native species richness remains constant with increasing area. Furthermore, we predicted that areas with more native species than predicted by the species-area relationship would have proportionally more exotics as well. We did find that these exotic and native deviations were highly correlated, but areas that were good (or bad) for native plants were even better (or worse) for exotics. Similar processes appear to influence exotic and native plant diversity but the degree of this influence may differ with site quality.",what Continent ?,North America,plant diversity,False,False
"The Natural Enemies Hypothesis (i.e., introduced species experience release from their natural enemies) is a common explanation for why invasive species are so successful. We tested this hypothesis for Ammophila arenaria (Poaceae: European beachgrass), an aggressive plant invading the coastal dunes of California, USA, by comparing the demographic effects of belowground pathogens on A. arenaria in its introduced range to those reported in its native range. European research on A. arenaria in its native range has established that soil-borne pathogens, primarily nematodes and fungi, reduce A. arenaria's growth. In a greenhouse experiment designed to parallel European studies, seeds and 2-wk-old seedlings were planted in sterilized and nonsterilized soil collected from the A. arenaria root zone in its introduced range of California. We assessed the effects of pathogens via soil sterilization on three early performance traits: seed germination, seedling survival, and plant growth. We found that seed germinatio...",what Continent ?,Europe,,False,False
"Abstract: The tallgrass prairie is one of the most severely affected ecosystems in North America. As a result of extensive conversion to agriculture during the last century, as little as 1% of the original tallgrass prairie remains. The remaining fragments of tallgrass prairie communities have conservation significance, but questions remain about their viability and importance to conservation. We investigated the effects of fragment size, native plant species diversity, and location on invasion by exotic plant species at 25 tallgrass prairie sites in central North America at various geographic scales. We used exotic species richness and relative cover as measures of invasion. Exotic species richness and cover were not related to area for all sites considered together. There were no significant relationships between native species richness and exotic species richness at the cluster and regional scale or for all sites considered together. At the local scale, exotic species richness was positively related to native species richness at four sites and negatively related at one. The 10 most frequently occurring and abundant exotic plant species in the prairie fragments were cool‐season, or C3, species, in contrast to the native plant community, which was dominated by warm‐season, or C4, species. This suggests that timing is important to the success of exotic species in the tallgrass prairie. Our study indicates that some small fragments of tallgrass prairie are relatively intact and should not be overlooked as long‐term refuges for prairie species, sources of genetic variability, and material for restoration.",what Continent ?,North America,,False,False
"Since 1995, Dikerogammarus villosus Sowinski, a Ponto-Caspian amphi- pod species, has been invading most of Western Europe' s hydrosystems. D. villosus geographic extension and quickly increasing population density has enabled it to become a major component of macrobenthic assemblages in recipient ecosystems. The ecological characteristics of D. villosus on a mesohabitat scale were investigated at a station in the Moselle River. This amphipod is able to colonize a wide range of sub- stratum types, thus posing a threat to all freshwater ecosystems. Rivers whose domi- nant substratum is cobbles and which have tree roots along the banks could harbour particularly high densities of D. villosus. A relationship exists between substratum par- ticle size and the length of the individuals, and spatial segregation according to length was shown. This allows the species to limit intra-specific competition between genera- tions while facilitating reproduction. A strong association exists between D. villosus and other Ponto-Caspian species, such as Dreissena polymorpha and Corophium cur- vispinum, in keeping with Invasional Meltdown Theory. Four taxa (Coenagrionidae, Calopteryx splendens, Corophium curvispinum and Gammarus pulex ) exhibited spa- tial niches that overlap significantly that of D. villosus. According to the predatory be- haviour of the newcomer, their populations may be severely impacted.",what Continent ?,Europe,freshwater,False,False
"Invasive plant species are a considerable threat to ecosystems globally and on islands in particular where species diversity can be relatively low. In this study, we examined the phylogenetic basis of invasion success on Robben Island in South Africa. The flora of the island was sampled extensively and the phylogeny of the local community was reconstructed using the two core DNA barcode regions, rbcLa and matK. By analysing the phylogenetic patterns of native and invasive floras at two different scales, we found that invasive alien species are more distantly related to native species, a confirmation of Darwin's naturalization hypothesis. However, this pattern also holds even for randomly generated communities, therefore discounting the explanatory power of Darwin's naturalization hypothesis as the unique driver of invasion success on the island. These findings suggest that the drivers of invasion success on the island may be linked to species traits rather than their evolutionary history alone, or to the combination thereof. This result also has implications for the invasion management programmes currently being implemented to rehabilitate the native diversity on Robben Island. © 2013 The Linnean Society of London, Botanical Journal of the Linnean Society, 2013, 172, 142–152.",what Continent ?,Africa,invasion success,False,False
"Few field experiments have examined the effects of both resource availability and propagule pressure on plant community invasibility. Two non-native forest species, a herb and a shrub (Hesperis matronalis and Rhamnus cathartica, respectively), were sown into 60 1-m 2 sub-plots distributed across three plots. These contained reconstructed native plant communities in a replaced surface soil layer in a North American forest interior. Resource availability and propagule pressure were manipulated as follows: understorey light level (shaded/unshaded), nutrient availability (control/fertilized), and seed pressures of the two non-native species (control/low/high). Hesperis and Rhamnus cover and the above-ground biomass of Hesperis were significantly higher in shaded sub-plots and at greater propagule pressures. Similarly, the above-ground biomass of Rhamnus was significantly increased with propagule pressure, although this was a function of density. In contrast, of species that seeded into plots from the surrounding forest during the growing season, the non-native species had significantly greater cover in unshaded sub-plots. Plants in these unshaded sub-plots were significantly taller than plants in shaded sub-plots, suggesting a greater fitness. Total and non-native species richness varied significantly among plots indicating the importance of fine-scale dispersal patterns. None of the experimental treatments influenced native species. Since the forest seed bank in our study was colonized primarily by non-native ruderal species that dominated understorey vegetation, the management of invasions by non-native species in forest understoreys will have to address factors that influence light levels and dispersal pathways.",what Continent ?,North America,plant community,False,False
"Hussner A (2012). Alien aquatic plant species in European countries. Weed Research52, 297–306. Summary Alien aquatic plant species cause serious ecological and economic impacts to European freshwater ecosystems. This study presents a comprehensive overview of all alien aquatic plants in Europe, their places of origin and their distribution within the 46 European countries. In total, 96 aquatic species from 30 families have been reported as aliens from at least one European country. Most alien aquatic plants are native to Northern America, followed by Asia and Southern America. Elodea canadensis is the most widespread alien aquatic plant in Europe, reported from 41 European countries. Azolla filiculoides ranks second (25), followed by Vallisneria spiralis (22) and Elodea nuttallii (20). The highest number of alien aquatic plant species has been found in Italy and France (34 species), followed by Germany (27), Belgium and Hungary (both 26) and the Netherlands (24). Even though the number of alien aquatic plants seems relatively small, the European and Mediterranean Plant Protection Organization (EPPO, http://www.eppo.org) has listed 18 of these species as invasive or potentially invasive within the EPPO region. As ornamental trade has been regarded as the major pathway for the introduction of alien aquatic plants, trading bans seem to be the most effective option to reduce the risk of further unintended entry of alien aquatic plants into Europe.",what Continent ?,Europe,freshwater ecosystems.,False,False
"Invasions by alien species are one of the major threats to the native environment. There are multifold attempts to counter alien species, but limited resources for mitigation or eradication programmes makes prioritisation indispensable. We used the generic impact scoring system to assess the impact of alien fish species in Europe. It prioritises species, but also offers the possibility to compare the impact of alien invasive species between different taxonomic groups. For alien fish in Europe, we compiled a list of 40 established species. By literature research, we assessed the environmental impact (through herbivory, predation, competition, disease transmission, hybridisation and ecosystem alteration) and economic impact (on agriculture, animal production, forestry, human infrastructure, human health and human social life) of each species. The goldfish/gibel complex Carassius auratus/C. gibelio scored the highest impact points, followed by the grass carp Ctenopharyngodon idella and the topmouth gudgeon Pseudorasbora parva. According to our analyses, alien fish species have the strongest impact on the environment through predation, followed by competition with native species. Besides negatively affecting animal production (mainly in aquaculture), alien fish have no pronounced economic impact. At the species level, C. auratus/C. gibelio show similar impact scores to the worst alien mammals in Europe. This study indicates that the generic impact scoring system is useful to investigate the impact of alien fish, also allowing cross-taxa comparisons. Our results are therefore of major relevance for stakeholders and decision-makers involved in management and eradication of alien fish species.",what Continent ?,Europe,native environment.,False,False
"1 The emerald ash borer Agrilus planipennis (Coleoptera: Buprestidae) (EAB), an invasive wood‐boring beetle, has recently caused significant losses of native ash (Fraxinus spp.) trees in North America. Movement of wood products has facilitated EAB spread, and heat sanitation of wooden materials according to International Standards for Phytosanitary Measures No. 15 (ISPM 15) is used to prevent this. 2 In the present study, we assessed the thermal conditions experienced during a typical heat‐treatment at a facility using protocols for pallet wood treatment under policy PI‐07, as implemented in Canada. The basal high temperature tolerance of EAB larvae and pupae was determined, and the observed heating rates were used to investigate whether the heat shock response and expression of heat shock proteins occurred in fourth‐instar larvae. 3 The temperature regime during heat treatment greatly exceeded the ISPM 15 requirements of 56 °C for 30 min. Emerald ash borer larvae were highly tolerant of elevated temperatures, with some instars surviving exposure to 53 °C without any heat pre‐treatments. High temperature survival was increased by either slow warming or pre‐exposure to elevated temperatures and a recovery regime that was accompanied by up‐regulated hsp70 expression under some of these conditions. 4 Because EAB is highly heat tolerant and exhibits a fully functional heat shock response, we conclude that greater survival than measured in vitro is possible under industry treatment conditions (with the larvae still embedded in the wood). We propose that the phenotypic plasticity of EAB may lead to high temperature tolerance very close to conditions experienced in an ISPM 15 standard treatment.",what Continent ?,North America,emerald ash,False,False
"Abstract Objectives: To examine associations of household crop diversity with school-aged child dietary diversity in Vietnam and Ethiopia and mechanisms underlying these associations. Design: We created a child diet diversity score (DDS) using data on seven food groups consumed in the last 24 h. Generalised estimating equations were used to model associations of household-level crop diversity, measured as a count of crop species richness (CSR) and of plant crop nutritional functional richness (CNFR), with DDS. We examined effect modification by household wealth and subsistence orientation, and mediation by the farm’s market orientation. Setting: Two survey years of longitudinal data from the Young Lives cohort. Participants: Children (aged 5 years in 2006 and 8 years in 2009) from rural farming households in Ethiopia (n 1012) and Vietnam (n 1083). Results: There was a small, positive association between household CNFR and DDS in Ethiopia (CNFR–DDS, β = 0·13; (95 % CI 0·07, 0·19)), but not in Vietnam. Associations of crop diversity and child diet diversity were strongest among poor households in Ethiopia and among subsistence-oriented households in Vietnam. Agricultural earnings positively mediated the crop diversity–diet diversity association in Ethiopia. Discussion: Children from households that are poorer and those that rely more on their own agricultural production for food may benefit most from increased crop diversity.",what Location ?,Ethiopia ,vietnam,False,False
"ABSTRACT Accurate and up-to-date built-up area mapping is of great importance to the science community, decision-makers, and society. Therefore, satellite-based, built-up area (BUA) extraction at medium resolution with supervised classification has been widely carried out. However, the spectral confusion between BUA and bare land (BL) is the primary hindering factor for accurate BUA mapping over large regions. Here we propose a new methodology for the efficient BUA extraction using multi-sensor data under Google Earth Engine cloud computing platform. The proposed method mainly employs intra-annual satellite imagery for water and vegetation masks, and a random-forest machine learning classifier combined with auxiliary data to discriminate between BUA and BL. First, a vegetation mask and water mask are generated using NDVI (normalized differenced vegetation index) max in vegetation growth periods and the annual water-occurrence frequency. Second, to accurately extract BUA from unmasked pixels, consisting of BUA and BL, random-forest-based classification is conducted using multi-sensor features, including temperature, night-time light, backscattering, topography, optical spectra, and NDVI time-series metrics. This approach is applied in Zhejiang Province, China, and an overall accuracy of 92.5% is obtained, which is 3.4% higher than classification with spectral data only. For large-scale BUA mapping, it is feasible to enhance the performance of BUA mapping with multi-temporal and multi-sensor data, which takes full advantage of datasets available in Google Earth Engine.",what Location ?,China,"china,",True,True
"ABSTRACT In this study, we investigated the relationship between agricultural biodiversity and dietary diversity of children and whether factors such as economic access may affect this relationship. This paper is based on data collected in a baseline cross-sectional survey in November 2013.The study population comprising 1200 mother-child pairs was selected using a two-stage cluster sampling. Dietary diversity was defined as the number of food groups consumed 24 h prior to the assessment. The number of crop and livestock species produced on a farm was used as the measure of production diversity. Hierarchical regression analysis was used to identify predictors and test for interactions. Whereas the average production diversity score was 4.7 ± 1.6, only 42.4% of households consumed at least four food groups out of seven over the preceding 24-h recall period. Agricultural biodiversity (i.e. variety of animals kept and food groups produced) associated positively with dietary diversity of children aged 6–36 months but the relationship was moderated by household socioeconomic status. The interaction term was also statistically significant [β = −0.08 (95% CI: −0.05, −0.01, p = 0.001)]. Spearman correlation (rho) analysis showed that agricultural biodiversity was positively associated with individual dietary diversity of the child more among children of low socioeconomic status in rural households compared to children of high socioeconomic status (r = 0.93, p < 0.001 versus r = 0.08, p = 0.007). Socioeconomic status of the household also partially mediated the link between agricultural biodiversity and dietary diversity of a child’s diet. The effect of increased agricultural biodiversity on dietary diversity was significantly higher in households of lower socioeconomic status. Therefore, improvement of agricultural biodiversity could be one of the best approaches for ensuring diverse diets especially for households of lower socioeconomic status in rural areas of Northern Ghana.",what Location ?,Ghana,ghana,True,True
"Abstract Objective To evaluate viral loads at different stages of disease progression in patients infected with the 2019 severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) during the first four months of the epidemic in Zhejiang province, China. Design Retrospective cohort study. Setting A designated hospital for patients with covid-19 in Zhejiang province, China. Participants 96 consecutively admitted patients with laboratory confirmed SARS-CoV-2 infection: 22 with mild disease and 74 with severe disease. Data were collected from 19 January 2020 to 20 March 2020. Main outcome measures Ribonucleic acid (RNA) viral load measured in respiratory, stool, serum, and urine samples. Cycle threshold values, a measure of nucleic acid concentration, were plotted onto the standard curve constructed on the basis of the standard product. Epidemiological, clinical, and laboratory characteristics and treatment and outcomes data were obtained through data collection forms from electronic medical records, and the relation between clinical data and disease severity was analysed. Results 3497 respiratory, stool, serum, and urine samples were collected from patients after admission and evaluated for SARS-CoV-2 RNA viral load. Infection was confirmed in all patients by testing sputum and saliva samples. RNA was detected in the stool of 55 (59%) patients and in the serum of 39 (41%) patients. The urine sample from one patient was positive for SARS-CoV-2. The median duration of virus in stool (22 days, interquartile range 17-31 days) was significantly longer than in respiratory (18 days, 13-29 days; P=0.02) and serum samples (16 days, 11-21 days; P<0.001). The median duration of virus in the respiratory samples of patients with severe disease (21 days, 14-30 days) was significantly longer than in patients with mild disease (14 days, 10-21 days; P=0.04). In the mild group, the viral loads peaked in respiratory samples in the second week from disease onset, whereas viral load continued to be high during the third week in the severe group. Virus duration was longer in patients older than 60 years and in male patients. Conclusion The duration of SARS-CoV-2 is significantly longer in stool samples than in respiratory and serum samples, highlighting the need to strengthen the management of stool samples in the prevention and control of the epidemic, and the virus persists longer with higher load and peaks later in the respiratory tissue of patients with severe disease.",what Location ?,China,china.,True,True
"Abstract This paper attempts to provide methods to estimate the real scenario of the novel coronavirus pandemic crisis on Brazil and the states of Sao Paulo, Pernambuco, Espirito Santo, Amazonas and Distrito Federal. By the use of a SEIRD mathematical model with age division, we predict the infection and death curve, stating the peak date for Brazil and these states. We also carry out a prediction for the ICU demand on these states for a visualization of the size of a possible collapse on the local health system. By the end, we establish some future scenarios including the stopping of social isolation and the introduction of vaccines and efficient medicine against the virus.",what has location ?,Brazil,"brazil and the states of sao paulo,",False,True
"ABSTRACT This paper provides a timely evaluation of whether the main COVID-19 lockdown policies – remote work, short-time work and closure of schools and childcare – have an immediate effect on the German population in terms of changes in satisfaction with work and family life. Relying on individual level panel data collected before and during the lockdown, we examine (1) how family satisfaction and work satisfaction of individuals have changed over the lockdown period, and (2) how lockdown-driven changes in the labour market situation (i.e. working remotely and being sent on short-time work) have affected satisfactions. We apply first-difference regressions for mothers, fathers, and persons without children. Our results show a general decrease in family satisfaction. We also find an overall decline in work satisfaction which is most pronounced for mothers and those without children who have to switch to short-time work. In contrast, fathers' well-being is less affected negatively and their family satisfaction even increased after changing to short-time work. We conclude that while the lockdown circumstances generally have a negative effect on the satisfaction with work and family of individuals in Germany, effects differ between childless persons, mothers, and fathers with the latter being least negatively affected.",what has location ?,Germany,german,False,False
"Abstract We examine the effects of Covid-19 and related restrictions on individuals with dependent children in Germany. We specifically focus on the role of day care center and school closures, which may be regarded as a “disruptive exogenous shock” to family life. We make use of a novel representative survey of parental well-being collected in May and June 2020 in Germany, when schools and day care centers were closed but while other measures had been relaxed and new infections were low. In our descriptive analysis, we compare well-being during this period with a pre-crisis period for different groups. In a difference-in-differences design, we compare the change for individuals with children to the change for individuals without children, accounting for unrelated trends as well as potential survey mode and context effects. We find that the crisis lowered the relative well-being of individuals with children, especially for individuals with young children, for women, and for persons with lower secondary schooling qualifications. Our results suggest that public policy measures taken to contain Covid-19 can have large effects on family well-being, with implications for child development and parental labor market outcomes.",what has location ?,Germany,germany.,True,True
"Wesselsbron is a neglected, mosquito-borne zoonotic disease endemic to Africa. The virus is mainly transmitted by the mosquitoes of the Aedes genus and primarily affects domestic livestock species with teratogenic effects but can jump to humans. Although no major outbreak or fatal case in humans has been reported as yet worldwide, a total of 31 acute human cases of Wesselsbron infection have been previously described since its first isolation in 1955. However, most of these cases were reported from Sub-Saharan Africa where resources are limited and a lack of diagnostic means exists. We describe here two molecular diagnostic tools suitable for Wesselsbron virus detection. The newly established reverse transcription-quantitative polymerase chain reaction and reverse-transcription-recombinase polymerase amplification assays are highly specific and repeatable, and exhibit good agreement with the reference assay on the samples tested. The validation on clinical and veterinary samples shows that they can be accurately used for Wesselsbron virus detection in public health activities and the veterinary field. Considering the increasing extension of Aedes species worldwide, these new assays could be useful not only in laboratory studies for Wesselsbron virus, but also in routine surveillance activities for zoonotic arboviruses and could be applied in well-equipped central laboratories or in remote areas in Africa, regarding the reverse-transcription-recombinase polymerase amplification assay.",what has location ?,Africa,africa.,True,True
"Abstract Hackathons, time-bounded events where participants write computer code and build apps, have become a popular means of socializing tech students and workers to produce “innovation” despite little promise of material reward. Although they offer participants opportunities for learning new skills and face-to-face networking and set up interaction rituals that create an emotional “high,” potential advantage is even greater for the events’ corporate sponsors, who use them to outsource work, crowdsource innovation, and enhance their reputation. Ethnographic observations and informal interviews at seven hackathons held in New York during the course of a single school year show how the format of the event and sponsors’ discursive tropes, within a dominant cultural frame reflecting the appeal of Silicon Valley, reshape unpaid and precarious work as an extraordinary opportunity, a ritual of ecstatic labor, and a collective imaginary for fictional expectations of innovation that benefits all, a powerful strategy for manufacturing workers’ consent in the “new” economy.",what has location ?,New York,new york,True,True
"Abstract Introduction The first wave of COVID-19 pandemic period has drastically changed people’s lives all over the world. To cope with the disruption, digital solutions have become more popular. However, the ability to adopt digitalised alternatives is different across socio-economic and socio-demographic groups. Objective This study investigates how individuals have changed their activity-travel patterns and internet usage during the first wave of the COVID-19 pandemic period, and which of these changes may be kept. Methods An empirical data collection was deployed through online forms. 781 responses from different countries (Italy, Sweden, India and others) have been collected, and a series of multivariate analyses was carried out. Two linear regression models are presented, related to the change of travel activities and internet usage, before and during the pandemic period. Furthermore, a binary regression model is used to examine the likelihood of the respondents to adopt and keep their behaviours beyond the pandemic period. Results The results show that the possibility to change the behaviour matter. External restrictions and personal characteristics are the driving factors of the reduction in ones' daily trips. However, the estimation results do not show a strong correlation between the countries' restriction policy and the respondents' likelihood to adopt the new and online-based behaviours for any of the activities after the restriction period. Conclusion The acceptance and long-term adoption of the online alternatives for activities are correlated with the respondents' personality and socio-demographic group, highlighting the importance of promoting alternatives as a part of longer-term behavioural and lifestyle changes.",what Country ?,India,"italy, sweden,",False,False
"Advocates of software risk management claim that by identifying and analyzing threats to success (i.e., risks) action can be taken to reduce the chance of failure of a project. The first step in the risk management process is to identify the risk itself, so that appropriate countermeasures can be taken. One problem in this task, however, is that no validated lists are available to help the project manager understand the nature and types of risks typically faced in a software project. This paper represents a first step toward alleviating this problem by developing an authoritative list of common risk factors. We deploy a rigorous data collection method called a ""ranking-type"" Delphi survey to produce a rank-order list of risk factors. This data collection method is designed to elicit and organize opinions of a panel of experts through iterative, controlled feedback. Three simultaneous surveys were conducted in three different settings: Hong Kong, Finland, and the United States. This was done to broaden our view of the types of risks, rather than relying on the view of a single culture-an aspect that has been ignored in past risk management research. In forming the three panels, we recruited experienced project managers in each country. The paper presents the obtained risk factor list, compares it with other published risk factor lists for completeness and variation, and analyzes common features and differences in risk factor rankings in the three countries. We conclude by discussing implications of our findings for both research and improving risk management practice.",what Country ?,Hong Kong,"finland,",False,False
"The restrictive measures implemented in response to the COVID-19 pandemic have triggered sudden massive changes to travel behaviors of people all around the world. This study examines the individual mobility patterns for all transport modes (walk, bicycle, motorcycle, car driven alone, car driven in company, bus, subway, tram, train, airplane) before and during the restrictions adopted in ten countries on six continents: Australia, Brazil, China, Ghana, India, Iran, Italy, Norway, South Africa and the United States. This cross-country study also aims at understanding the predictors of protective behaviors related to the transport sector and COVID-19. Findings hinge upon an online survey conducted in May 2020 (N = 9,394). The empirical results quantify tremendous disruptions for both commuting and non-commuting travels, highlighting substantial reductions in the frequency of all types of trips and use of all modes. In terms of potential virus spread, airplanes and buses are perceived to be the riskiest transport modes, while avoidance of public transport is consistently found across the countries. According to the Protection Motivation Theory, the study sheds new light on the fact that two indicators, namely income inequality, expressed as Gini index, and the reported number of deaths due to COVID-19 per 100,000 inhabitants, aggravate respondents’ perceptions. This research indicates that socio-economic inequality and morbidity are not only related to actual health risks, as well documented in the relevant literature, but also to the perceived risks. These findings document the global impact of the COVID-19 crisis as well as provide guidance for transportation practitioners in developing future strategies.",what Country ?,India,,False,False
"Advocates of software risk management claim that by identifying and analyzing threats to success (i.e., risks) action can be taken to reduce the chance of failure of a project. The first step in the risk management process is to identify the risk itself, so that appropriate countermeasures can be taken. One problem in this task, however, is that no validated lists are available to help the project manager understand the nature and types of risks typically faced in a software project. This paper represents a first step toward alleviating this problem by developing an authoritative list of common risk factors. We deploy a rigorous data collection method called a ""ranking-type"" Delphi survey to produce a rank-order list of risk factors. This data collection method is designed to elicit and organize opinions of a panel of experts through iterative, controlled feedback. Three simultaneous surveys were conducted in three different settings: Hong Kong, Finland, and the United States. This was done to broaden our view of the types of risks, rather than relying on the view of a single culture-an aspect that has been ignored in past risk management research. In forming the three panels, we recruited experienced project managers in each country. The paper presents the obtained risk factor list, compares it with other published risk factor lists for completeness and variation, and analyzes common features and differences in risk factor rankings in the three countries. We conclude by discussing implications of our findings for both research and improving risk management practice.",what Country ?,Finland,"finland,",True,True
"The article analyses the most intense phase of a process of constitutional review in Kenya that has been ongoing since about 1990: that stage began in 2000 and is, perhaps, not yet completed, there being as yet no new constitution. The article describes the reasons for the review and the process. It offers an account of the role of the media and various sectors of society including women and previously marginalized ethnic groups, in shaping the agenda, the process and the outcome. It argues that although civil society, with much popular support, was prominent in pushing for change, when an official process of review began, the vested interests of government and even of those trusted with the review frustrated a quick outcome, and especially any outcome that meant curtailing the powers of government. Even high levels of popular involvement were unable to guarantee a new constitution against manipulation by government and other vested interests involved in review, including the law and the courts. However, a new constitution may yet emerge, and in any case the process may prove to have made an ineradicable impact on the shape of the nation's politics and the consciousness of the ordinary citizen.",what Country ?,Kenya,kenya,True,True
"The restrictive measures implemented in response to the COVID-19 pandemic have triggered sudden massive changes to travel behaviors of people all around the world. This study examines the individual mobility patterns for all transport modes (walk, bicycle, motorcycle, car driven alone, car driven in company, bus, subway, tram, train, airplane) before and during the restrictions adopted in ten countries on six continents: Australia, Brazil, China, Ghana, India, Iran, Italy, Norway, South Africa and the United States. This cross-country study also aims at understanding the predictors of protective behaviors related to the transport sector and COVID-19. Findings hinge upon an online survey conducted in May 2020 (N = 9,394). The empirical results quantify tremendous disruptions for both commuting and non-commuting travels, highlighting substantial reductions in the frequency of all types of trips and use of all modes. In terms of potential virus spread, airplanes and buses are perceived to be the riskiest transport modes, while avoidance of public transport is consistently found across the countries. According to the Protection Motivation Theory, the study sheds new light on the fact that two indicators, namely income inequality, expressed as Gini index, and the reported number of deaths due to COVID-19 per 100,000 inhabitants, aggravate respondents’ perceptions. This research indicates that socio-economic inequality and morbidity are not only related to actual health risks, as well documented in the relevant literature, but also to the perceived risks. These findings document the global impact of the COVID-19 crisis as well as provide guidance for transportation practitioners in developing future strategies.",what Country ?,Australia,,False,False
"The article deals with an oral speech phenomenon widespread in the Republic of Belarus, where it is known as trasjanka. This code originated through constant contact between Russian and Belarusian, two closely related East Slavonic languages. Discussed are the main features of this code (as used in the city of Minsk), the sources of its origin, different linguistic definitions and the attitude towards this code from those who dwell in the city of Minsk. Special attention is paid to the problem of distinction between trasjanka and different forms of codeswitching, also widely used in the Minsk language community.",what Population under analysis ?,Minsk,"belarus,",False,False
"This paper addresses the issue of overeducation and undereducation using for the first time a British dataset which contains explicit information on the level of required education to enter a job across the generality of occupations. Three key issues within the overeducation literature are addressed. First, what determines the existence of over and undereducation and to what extent are over and undereducation substitutes for experience, tenure and training? Second, to what extent are over and undereducation temporary or permanent phenomena? Third, what are the returns to over and undereducation and do certain stylized facts discovered for the US and a number of European countries hold for Britain?",what Population under analysis ?,General,undereducation,False,False
"There is little question that substantial labormarket differences exist between men and women. Among the most researched difference is the male-female wage gap. Many different theories are used to explain why men earn more than women. One possible reason is based on the limited geographic mobility of married women (Robert Frank, 1978). Family mobility is a joint decision in which the needs of the husband and wife are balanced to maximize family welfare. Job-motivated relocations are generally made to benefit the primary earner in the family. This leads to a constrained job search for the secondary earner, as he or she must search for a job in a limited geographic area. Since the husband is still the primary wage earner in many families, the job search of the wife may suffer. Individuals who are tied to a certain area are labeled ""tied-stayers,"" while secondary earners who move for the benefit of the family are labeled ""tied-movers"" (Jacob Mincer, 1978). The wages of a tied-stayer or tied-mover may not be substantially lower if the family lives in or moves to a large city. If a large labor market has more vacancies, the wife may locate a wage offer near the maximum she would find with a nationwide job search. However, being a tied-stayer or tied-mover can lower the wife's wage if the family lives in or moves to a small community. A small labor market will reduce the likelihood of her finding a job that utilizes her skills. As a result she may accept a job for which she is overqualified and thus earn a lower wage.' This hypothesized relationship between the likelihood of being overqualified and SMSA size is termed ""differential overqualification."" Frank ( 1978) and Haim Ofek and Yesook Merrill (1994) provide support for the theory of differential overqualification by finding that the malefemale wage gap is greater in smaller SMSA's. While the results are consistent with the existence of differential overqualification, they may also result from other situations as well. Firms in small labor markets may use their monopsony power to keep wages down.2 Local demand shocks are found to be a major source of wage variation both across and within local labor markets (Robert Topel, 1986). Since large labor markets are generally more diversified, a demand shock can have a substantial impact on immobile workers in small labor markets. Another reason for examining differential overqualification involves the assumption that there are more vacancies in large labor markets. While there is little doubt that more vacancies exist in large labor markets, there are also likely to be more people searching for jobs in large labor markets. If the greater number of vacancies is offset by the larger number of searchers, it is unclear whether women will be more likely to be overqualified in small labor markets. Instead of relying on wages to determine if differential overqualification exists, we consider an explicit form of overqualification based on education.",what Population under analysis ?,General,,False,False
"A SEIR simulation model for the COVID-19 pandemic was developed (http://covidsim.eu) and applied to a hypothetical European country of 10 million population. Our results show which interventions potentially push the epidemic peak into the subsequent year (when vaccinations may be available) or which fail. Different levels of control (via contact reduction) resulted in 22% to 63% of the population sick, 0.2% to 0.6% hospitalised, and 0.07% to 0.28% dead (n=6,450 to 28,228).",what location ?,Hypothetical European Country,european,False,False
"<jats:sec id=""sec001"">
<jats:title>Background</jats:title>
<jats:p>To control the COVID-19 outbreak in Japan, sports and entertainment events were canceled and schools were closed throughout Japan from February 26 through March 19. That policy has been designated as voluntary event cancellation and school closure (VECSC).</jats:p>
</jats:sec>
<jats:sec id=""sec002"">
<jats:title>Object</jats:title>
<jats:p>This study assesses VECSC effectiveness based on predicted outcomes.</jats:p>
</jats:sec>
<jats:sec id=""sec003"">
<jats:title>Methods</jats:title>
<jats:p>A simple susceptible–infected–recovered model was applied to data of patients with symptoms in Japan during January 14 through March 26. The respective reproduction numbers for periods before VECSC (R<jats:sub>0</jats:sub>), during VECSC (R<jats:sub>e</jats:sub>), and after VECSC (R<jats:sub>a</jats:sub>) were estimated.</jats:p>
</jats:sec>
<jats:sec id=""sec004"">
<jats:title>Results</jats:title>
<jats:p>Results suggest R<jats:sub>0</jats:sub> before VECSC as 2.534 [2.449, 2.598], R<jats:sub>e</jats:sub> during VECSC as 1.077 [0.948, 1.228], and R<jats:sub>a</jats:sub> after VECSC as 4.455 [3.615, 5.255].</jats:p>
</jats:sec>
<jats:sec id=""sec005"">
<jats:title>Discussion and conclusion</jats:title>
<jats:p>Results demonstrated that VECSC can reduce COVID-19 infectiousness considerably, but after VECSC, the value of the reproduction number rose to exceed 4.0.</jats:p>
</jats:sec>",what location ?,Japan,"japan,",True,True
"Since December 2019, COVID-19 has raged in Wuhan and subsequently all over China and the world. We propose a Cybernetics-based Dynamic Infection Model (CDIM) to the dynamic infection process with a probability distributed incubation delay and feedback principle. Reproductive trends and the stability of the SARS-COV-2 infection in a city can then be analyzed, and the uncontrollable risks can be forecasted before they really happen. The infection mechanism of a city is depicted using the philosophy of cybernetics and approaches of the control engineering. Distinguished with other epidemiological models, such as SIR, SEIR, etc., that compute the theoretical number of infected people in a closed population, CDIM considers the immigration and emigration population as system inputs, and administrative and medical resources as dynamic control variables. The epidemic regulation can be simulated in the model to support the decision-making for containing the outbreak. City case studies are demonstrated for verification and validation.",what location ?,China,china,True,True
"(1) Background: Although bullying victimization is a phenomenon that is increasingly being recognized as a public health and mental health concern in many countries, research attention on this aspect of youth violence in low- and middle-income countries, especially sub-Saharan Africa, is minimal. The current study examined the national prevalence of bullying victimization and its correlates among in-school adolescents in Ghana. (2) Methods: A sample of 1342 in-school adolescents in Ghana (55.2% males; 44.8% females) aged 12–18 was drawn from the 2012 Global School-based Health Survey (GSHS) for the analysis. Self-reported bullying victimization “during the last 30 days, on how many days were you bullied?” was used as the central criterion variable. Three-level analyses using descriptive, Pearson chi-square, and binary logistic regression were performed. Results of the regression analysis were presented as adjusted odds ratios (aOR) at 95% confidence intervals (CIs), with a statistical significance pegged at p < 0.05. (3) Results: Bullying victimization was prevalent among 41.3% of the in-school adolescents. Pattern of results indicates that adolescents in SHS 3 [aOR = 0.34, 95% CI = 0.25, 0.47] and SHS 4 [aOR = 0.30, 95% CI = 0.21, 0.44] were less likely to be victims of bullying. Adolescents who had sustained injury [aOR = 2.11, 95% CI = 1.63, 2.73] were more likely to be bullied compared to those who had not sustained any injury. The odds of bullying victimization were higher among adolescents who had engaged in physical fight [aOR = 1.90, 95% CI = 1.42, 2.25] and those who had been physically attacked [aOR = 1.73, 95% CI = 1.32, 2.27]. Similarly, adolescents who felt lonely were more likely to report being bullied [aOR = 1.50, 95% CI = 1.08, 2.08] as against those who did not feel lonely. Additionally, adolescents with a history of suicide attempts were more likely to be bullied [aOR = 1.63, 95% CI = 1.11, 2.38] and those who used marijuana had higher odds of bullying victimization [aOR = 3.36, 95% CI = 1.10, 10.24]. (4) Conclusions: Current findings require the need for policy makers and school authorities in Ghana to design and implement policies and anti-bullying interventions (e.g., Social Emotional Learning (SEL), Emotive Behavioral Education (REBE), Marijuana Cessation Therapy (MCT)) focused on addressing behavioral issues, mental health and substance abuse among in-school adolescents.",what location ?,Ghana,ghana.,True,True
"Abstract Background As the COVID-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. We determine the incubation period and serial interval distribution for transmission clusters in Singapore and in Tianjin. We infer the basic reproduction number and identify the extent of pre-symptomatic transmission. Methods We collected outbreak information from Singapore and Tianjin, China, reported from Jan.19-Feb.26 and Jan.21-Feb.27, respectively. We estimated incubation periods and serial intervals in both populations. Results The mean incubation period was 7.1 (6.13, 8.25) days for Singapore and 9 (7.92, 10.2) days for Tianjin. Both datasets had shorter incubation periods for earlier-occurring cases. The mean serial interval was 4.56 (2.69, 6.42) days for Singapore and 4.22 (3.43, 5.01) for Tianjin. We inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (Singapore, Tianjin). The estimated basic reproduction number for Singapore was 1.97 (1.45, 2.48) secondary cases per infective; for Tianjin it was 1.87 (1.65, 2.09) secondary cases per infective. Conclusions Estimated serial intervals are shorter than incubation periods in both Singapore and Tianjin, suggesting that pre-symptomatic transmission is occurring. Shorter serial intervals lead to lower estimates of R0, which suggest that half of all secondary infections should be prevented to control spread.",what location ?,Singapore,"singapore and in tianjin. we infer the basic reproduction number and identify the extent of pre - symptomatic transmission. methods we collected outbreak information from singapore and tianjin, china,",False,True
"<jats:p>Based on the study of Darjeeling Municipality, the paper engages with issues pertaining to understanding the matrixes of power relations involved in the supply of water in Darjeeling town in India. The discussions in the paper focuses on urbanization, the shrinking water resources, and increased demand for water on the one hand; and the role of local administration, the emergence of the water mafia, and the ‘Samaj’ (society) all contributing to a skewed and inequitable distribution of water and the assumption of proprietorship or the appropriation of water commons, culminating in the accentuation of water-rights deprivation in Darjeeling Municipal Area. HYDRO Nepal JournalJournal of Water Energy and EnvironmentIssue No: 22Page: 16-24Uploaded date: January 14, 2018</jats:p>",what location ?,Darjeeling,india.,False,False
"English Abstract: Background: Since the emergence of the first pneumonia cases in Wuhan, China, the novel coronavirus (2019-nCov) infection has been quickly spreading out to other provinces and neighbouring countries. Estimation of the basic reproduction number by means of mathematical modelling can be helpful for determining the potential and severity of an outbreak, and providing critical information for identifying the type of disease interventions and intensity. Methods: A deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and the intervention measures. Findings: The estimation results based on likelihood and model analysis reveal that the control reproduction number may be as high as 6.47 (95% CI 5.71-7.23). Sensitivity analyses reveal that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction of Wuhan on 2019-nCov infection in Beijing being almost equivalent to increasing quarantine by 100-thousand baseline value. Interpretation: It is essential to assess how the expensive, resource-intensive measures implemented by the Chinese authorities can contribute to the prevention and control of the 2019-nCov infection, and how long should be maintained. Under the most restrictive measures, the outbreak is expected to peak within two weeks (since January 23rd 2020) with significant low peak value. With travel restriction (no imported exposed individuals to Beijing), the number of infected individuals in 7 days will decrease by 91.14% in Beijing, compared with the scenario of no travel restriction. Mandarin Abstract: 背景：自从中国武汉出现第一例肺炎病例以来，新型冠状病毒（2019-nCov）感染已迅速传播到其他省份和周边国家。通过数学模型估计基本再生数，有助于确定疫情爆发的可能性和严重性，并为确定疾病干预类型和强度提供关键信息。 方法：根据疾病的临床进展，个体的流行病学状况和干预措施，设计确定性的仓室模型。 结果：基于似然函数和模型分析的估计结果表明，控制再生数可能高达6.47（95％CI 5.71-7.23）。敏感性分析显示，密集接触追踪和隔离等干预措施可以有效减少控制再生数和传播风险，武汉封城措施对北京2019-nCov感染的影响几乎等同于增加隔离措施10万的基线值。 解释：必须评估中国当局实施的昂贵，资源密集型措施如何有助于预防和控制2019-nCov感染，以及应维持多长时间。在最严格的措施下，预计疫情将在两周内（自2020年1月23日起）达到峰值，峰值较低。与没有出行限制的情况相比，有了出行限制（即没有输入的潜伏类个体进入北京），北京的7天感染者数量将减少91.14％。",what location ?,China,"china,",True,True
"By 27 February 2020, the outbreak of coronavirus disease 2019 (COVID‐19) caused 82 623 confirmed cases and 2858 deaths globally, more than severe acute respiratory syndrome (SARS) (8273 cases, 775 deaths) and Middle East respiratory syndrome (MERS) (1139 cases, 431 deaths) caused in 2003 and 2013, respectively. COVID‐19 has spread to 46 countries internationally. Total fatality rate of COVID‐19 is estimated at 3.46% by far based on published data from the Chinese Center for Disease Control and Prevention (China CDC). Average incubation period of COVID‐19 is around 6.4 days, ranges from 0 to 24 days. The basic reproductive number (R0) of COVID‐19 ranges from 2 to 3.5 at the early phase regardless of different prediction models, which is higher than SARS and MERS. A study from China CDC showed majority of patients (80.9%) were considered asymptomatic or mild pneumonia but released large amounts of viruses at the early phase of infection, which posed enormous challenges for containing the spread of COVID‐19. Nosocomial transmission was another severe problem. A total of 3019 health workers were infected by 12 February 2020, which accounted for 3.83% of total number of infections, and extremely burdened the health system, especially in Wuhan. Limited epidemiological and clinical data suggest that the disease spectrum of COVID‐19 may differ from SARS or MERS. We summarize latest literatures on genetic, epidemiological, and clinical features of COVID‐19 in comparison to SARS and MERS and emphasize special measures on diagnosis and potential interventions. This review will improve our understanding of the unique features of COVID‐19 and enhance our control measures in the future.",what location ?,China,china,True,True
We estimate the effective reproduction number for 2019-nCoV based on the daily reported cases from China CDC. The results indicate that 2019-nCoV has a higher effective reproduction number than SARS with a comparable fatality rate.,what location ?,China,china,True,True
"Self-sustaining human-to-human transmission of the novel coronavirus (2019-nCov) is the only plausible explanation of the scale of the outbreak in Wuhan. We estimate that, on average, each case infected 2.6 (uncertainty range: 1.5-3.5) other people up to 18 January 2020, based on an analysis combining our past estimates of the size of the outbreak in Wuhan with computational modelling of potential epidemic trajectories. This implies that control measures need to block well over 60% of transmission to be effective in controlling the outbreak. It is likely, based on the experience of SARS and MERS-CoV, that the number of secondary cases caused by a case of 2019-nCoV is highly variable – with many cases causing no secondary infections, and a few causing many. Whether transmission is continuing at the same rate currently depends on the effectiveness of current control measures implemented in China and the extent to which the populations of affected areas have adopted risk-reducing behaviours. In the absence of antiviral drugs or vaccines, control relies upon the prompt detection and isolation of symptomatic cases. It is unclear at the current time whether this outbreak can be contained within China; uncertainties include the severity spectrum of the disease caused by this virus and whether cases with relatively mild symptoms are able to transmit the virus efficiently. Identification and testing of potential cases need to be as extensive as is permitted by healthcare and diagnostic testing capacity – including the identification, testing and isolation of suspected cases with only mild to moderate disease (e.g. influenza-like illness), when logistically feasible.",what location ?,Wuhan,wuhan.,True,True
"Background: Estimating key infectious disease parameters from the COVID-19 outbreak is quintessential for modelling studies and guiding intervention strategies. Whereas different estimates for the incubation period distribution and the serial interval distribution have been reported, estimates of the generation interval for COVID-19 have not been provided. Methods: We used outbreak data from clusters in Singapore and Tianjin, China to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. From those estimates we obtained the proportions pre-symptomatic transmission and reproduction numbers. Results: The mean generation interval was 5.20 (95%CI 3.78-6.78) days for Singapore and 3.95 (95%CI 3.01-4.91) days for Tianjin, China when relying on a previously reported incubation period with mean 5.2 and SD 2.8 days. The proportion of pre-symptomatic transmission was 48% (95%CI 32-67%) for Singapore and 62% (95%CI 50-76%) for Tianjin, China. Estimates of the reproduction number based on the generation interval distribution were slightly higher than those based on the serial interval distribution. Conclusions: Estimating generation and serial interval distributions from outbreak data requires careful investigation of the underlying transmission network. Detailed contact tracing information is essential for correctly estimating these quantities.",what location ?,"Tianjin, China",singapore,False,False
"Bullying is relatively common and is considered to be a public health problem among adolescents worldwide. The present study examined the risk factors associated with bullying behavior among adolescents in a lower-middle-income country setting. Data on 6235 adolescents aged 11–16 years, derived from the Republic of Ghana’s contribution to the Global School-based Health Survey, were analyzed using bivariate and multinomial logistic regression analysis. A high prevalence of bullying was found among Ghanaian adolescents. Alcohol-related health compromising behaviors (alcohol use, alcohol misuse and getting into trouble as a result of alcohol) increased the risk of being bullied. In addition, substance use, being physically attacked, being seriously injured, hunger and truancy were also found to increase the risk of being bullied. However, having understanding parents and having classmates who were kind and helpful reduced the likelihood of being bullied. These findings suggest that school-based intervention programs aimed at reducing rates of peer victimization should simultaneously target multiple risk behaviors. Teachers can also reduce peer victimization by introducing programs that enhance adolescents’ acceptance of each other in the classroom.",what location ?,Ghana,ghana ’,False,True
"ABSTRACT The present study exploits high-resolution hyperspectral imagery acquired by the Airborne Visible/Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) sensor from the Hutti-Maski gold deposit area, India, to map hydrothermal alteration minerals. The study area is a volcanic-dominated late Archean greenstone belt that hosts major gold mineralization in the Eastern Dharwar Craton of southern India. The study encompasses pre-processing, spectral and spatial image reduction using Minimum Noise Fraction (MNF) and Fast Pixel Purity Index (FPPI), followed by endmember extraction using n-dimensional visualizer and the United States Geological Survey (USGS) mineral spectral library. Image derived endmembers such as goethite, chlorite, chlorite at the mine site (chlorite mixed with mined materials), kaolinite, and muscovite were subsequently used in spectral mapping methods such as Spectral Angle Mapper (SAM), Spectral Information Divergence (SID) and its hybrid, i.e. SIDSAMtan. Spectral similarity matrix of the target and non-target-based method has been proposed to find the possible optimum threshold needed to obtain mineral map using spectral mapping methods. Relative Spectral Discrimination Power (RSDPW) and Confusion Matrix (CM) have been used to evaluate the performance of SAM, SID, and SIDSAMtan. The RSDPW and CM illustrate that the SIDSAMtan benefits from the unique characteristics of SAM and SID to achieve better discrimination capability. The Overall Accuracy (OA) and kappa coefficient (ҡ) of SAM, SID, and SIDSAMtan were computed using 900 random validation points and obtained 90% (OA) and 0.88 (ҡ), 91.4% and 0.90, and 94.4% and 0.93, respectively. Obtained mineral map demonstrates that the northern portion of the area mainly consists of muscovite whereas the southern part is marked by chlorite, goethite, muscovite and kaolinite, indicating the propylitic alteration. Most of these minerals are associated with altered metavolcanic rocks and migmatite.",what Techniques/Methods ?,Minimum Noise Fraction (MNF),"fast pixel purity index ( fppi ), followed by endmember extraction using n - dimensional visualizer and the united states geological survey ( usgs ) mineral spectral library. image derived endmembers such as goethite, chlorite, chlorite at the mine site ( chlorite mixed with mined materials ), kaolinite, and muscovite were subsequently used in spectral mapping methods such as spectral angle mapper",False,False
"Several classification algorithms for pattern recognition had been tested in the mapping of tropical forest cover using airborne hyperspectral data. Results from the use of Maximum Likelihood (ML), Spectral Angle Mapper (SAM), Artificial Neural Network (ANN) and Decision Tree (DT) classifiers were compared and evaluated. It was found that ML performed the best followed by ANN, DT and SAM with accuracies of 86%, 84%, 51% and 49% respectively.",what Techniques/Methods ?,Spectral Angle Mapper (SAM),decision tree,False,False
"Imaging spectroscopic technique has been used for the mineral and rock geological mapping and alteration information extraction successfully with many reasonable results, but it is mainly used in arid and semi-arid land with low vegetation covering. In the case of the high vegetation covering, the outcrop of the altered rocks is small and distributes sparsely, the altered rocks is difficult to be identified directly. The target detection technique using imaging spectroscopic data should be introduced to the extraction of small geological targets under high vegetation covering area. In the paper, we take Ding-Ma gold deposit as the study area which located in Zhenan country, Shanxi province, the spectral features of the targets and the backgrounds are studied and analyzed using the field reflectance spectra, in addition to the study of the principle of the algorithms, some target detection algorithms which is appropriate to the small geological target detection are introduced. At last, the small altered rock targets under the covering of vegetation in forest are detected and discriminated in imaging spectroscopy data with the methods of spectral angle mapper (SAM), Constrained Energy Minimization (CEM) and Adaptive Cosine Estimator (ACE). The detection results are reasonable and indicate the ability of target detection algorithms in geological target detection in the forest area.",what Techniques/Methods ?,Spectral Angle Mapper (SAM),adaptive cosine estimator,False,False
"Name ambiguity in the context of bibliographic citation affects the quality of services in digital libraries. Previous methods are not widely applied in practice because of their high computational complexity and their strong dependency on excessive attributes, such as institutional affiliation, research area, address, etc., which are difficult to obtain in practice. To solve this problem, we propose a novel coarse‐to‐fine framework for name disambiguation which sequentially employs 3 common and easily accessible attributes (i.e., coauthor name, article title, and publication venue). Our proposed framework is based on multiple clustering and consists of 3 steps: (a) clustering articles by coauthorship and obtaining rough clusters, that is fragments; (b) clustering fragments obtained in step 1 by title information and getting bigger fragments; (c) and clustering fragments obtained in step 2 by the latent relations among venues. Experimental results on a Digital Bibliography and Library Project (DBLP) data set show that our method outperforms the existing state‐of‐the‐art methods by 2.4% to 22.7% on the average pairwise F1 score and is 10 to 100 times faster in terms of execution time.",what Evidence ?,Author name,"article title,",False,False
"This paper proposes a methodology which discriminates the articles by the target authors (“true” articles) from those by other homonymous authors (“false” articles). Author name searches for 2,595 “source” authors in six subject fields retrieved about 629,000 articles. In order to extract true articles from the large amount of the retrieved articles, including many false ones, two filtering stages were applied. At the first stage any retrieved article was eliminated as false if either its affiliation addresses had little similarity to those of its source article or there was no citation relationship between the journal of the retrieved article and that of its source article. At the second stage, a sample of retrieved articles was subjected to manual judgment, and utilizing the judgment results, discrimination functions based on logistic regression were defined. These discrimination functions demonstrated both the recall ratio and the precision of about 95% and the accuracy (correct answer ratio) of 90–95%. Existence of common coauthor(s), address similarity, title words similarity, and interjournal citation relationships between the retrieved and source articles were found to be the effective discrimination predictors. Whether or not the source author was from a specific country was also one of the important predictors. Furthermore, it was shown that a retrieved article is almost certainly true if it was cited by, or cocited with, its source article. The method proposed in this study would be effective when dealing with a large number of articles whose subject fields and affiliation addresses vary widely. © 2011 Wiley Periodicals, Inc.",what Evidence ?,Title words,"manual judgment, and utilizing the judgment results, discrimination functions based on logistic regression were defined. these discrimination functions demonstrated both the recall ratio and the precision",False,False
"This paper proposes a methodology which discriminates the articles by the target authors (“true” articles) from those by other homonymous authors (“false” articles). Author name searches for 2,595 “source” authors in six subject fields retrieved about 629,000 articles. In order to extract true articles from the large amount of the retrieved articles, including many false ones, two filtering stages were applied. At the first stage any retrieved article was eliminated as false if either its affiliation addresses had little similarity to those of its source article or there was no citation relationship between the journal of the retrieved article and that of its source article. At the second stage, a sample of retrieved articles was subjected to manual judgment, and utilizing the judgment results, discrimination functions based on logistic regression were defined. These discrimination functions demonstrated both the recall ratio and the precision of about 95% and the accuracy (correct answer ratio) of 90–95%. Existence of common coauthor(s), address similarity, title words similarity, and interjournal citation relationships between the retrieved and source articles were found to be the effective discrimination predictors. Whether or not the source author was from a specific country was also one of the important predictors. Furthermore, it was shown that a retrieved article is almost certainly true if it was cited by, or cocited with, its source article. The method proposed in this study would be effective when dealing with a large number of articles whose subject fields and affiliation addresses vary widely. © 2011 Wiley Periodicals, Inc.",what Evidence ?,Citation relationship,"manual judgment, and utilizing the judgment results, discrimination functions based on logistic regression were defined. these discrimination functions demonstrated both the recall ratio and the precision",False,False
"Abstract. Carrion-breeding Sarcophagidae (Diptera) can be used to estimate the post-mortem interval in forensic cases. Difficulties with accurate morphological identifications at any life stage and a lack of documented thermobiological profiles have limited their current usefulness. The molecular-based approach of DNA barcoding, which utilises a 648-bp fragment of the mitochondrial cytochrome oxidase subunit I gene, was evaluated in a pilot study for discrimination between 16 Australian sarcophagids. The current study comprehensively evaluated barcoding for a larger taxon set of 588 Australian sarcophagids. In total, 39 of the 84 known Australian species were represented by 580 specimens, which includes 92% of potentially forensically important species. A further eight specimens could not be identified, but were included nonetheless as six unidentifiable taxa. A neighbour-joining tree was generated and nucleotide sequence divergences were calculated. All species except Sarcophaga (Fergusonimyia) bancroftorum, known for high morphological variability, were resolved as monophyletic (99.2% of cases), with bootstrap support of 100. Excluding S. bancroftorum, the mean intraspecific and interspecific variation ranged from 1.12% and 2.81–11.23%, respectively, allowing for species discrimination. DNA barcoding was therefore validated as a suitable method for molecular identification of Australian Sarcophagidae, which will aid in the implementation of this fauna in forensic entomology.",what Studied taxonomic group (Biology) ?,Sarcophagidae,diptera ),False,False
"Butterfly monitoring and Red List programs in Switzerland rely on a combination of observations and collection records to document changes in species distributions through time. While most butterflies can be identified using morphology, some taxa remain challenging, making it difficult to accurately map their distributions and develop appropriate conservation measures. In this paper, we explore the use of the DNA barcode (a fragment of the mitochondrial gene COI) as a tool for the identification of Swiss butterflies and forester moths (Rhopalocera and Zygaenidae). We present a national DNA barcode reference library including 868 sequences representing 217 out of 224 resident species, or 96.9% of Swiss fauna. DNA barcodes were diagnostic for nearly 90% of Swiss species. The remaining 10% represent cases of para- and polyphyly likely involving introgression or incomplete lineage sorting among closely related taxa. We demonstrate that integrative taxonomic methods incorporating a combination of morphological and genetic techniques result in a rate of species identification of over 96% in females and over 98% in males, higher than either morphology or DNA barcodes alone. We explore the use of the DNA barcode for exploring boundaries among taxa, understanding the geographical distribution of cryptic diversity and evaluating the status of purportedly endemic taxa. Finally, we discuss how DNA barcodes may be used to improve field practices and ultimately enhance conservation strategies.",what Studied taxonomic group (Biology) ?,Zygaenidae,rhopalocera,False,False
"Dasysyrphus Enderlein (Diptera: Syrphidae) has posed taxonomic challenges to researchers in the past, primarily due to their lack of interspecific diagnostic characters. In the present study, DNA data (mitochondrial cytochrome c oxidase sub-unit I—COI) were combined with morphology to help delimit species. This led to two species being resurrected from synonymy (D. laticaudus and D. pacificus) and the discovery of one new species (D. occidualis sp. nov.). An additional new species was described based on morphology alone (D. richardi sp. nov.), as the specimens were too old to obtain COI. Part of the taxonomic challenge presented by this group arises from missing type specimens. Neotypes are designated here for D. pauxillus and D. pinastri to bring stability to these names. An illustrated key to 13 Nearctic species is presented, along with descriptions, maps and supplementary data. A phylogeny based on COI is also presented and discussed.",what Studied taxonomic group (Biology) ?,Syrphidae,diptera :,False,False
"This study summarizes results of a DNA barcoding campaign on German Diptera, involving analysis of 45,040 specimens. The resultant DNA barcode library includes records for 2,453 named species comprising a total of 5,200 barcode index numbers (BINs), including 2,700 COI haplotype clusters without species‐level assignment, so called “dark taxa.” Overall, 88 out of 117 families (75%) recorded from Germany were covered, representing more than 50% of the 9,544 known species of German Diptera. Until now, most of these families, especially the most diverse, have been taxonomically inaccessible. By contrast, within a few years this study provided an intermediate taxonomic system for half of the German Dipteran fauna, which will provide a useful foundation for subsequent detailed, integrative taxonomic studies. Using DNA extracts derived from bulk collections made by Malaise traps, we further demonstrate that species delineation using BINs and operational taxonomic units (OTUs) constitutes an effective method for biodiversity studies using DNA metabarcoding. As the reference libraries continue to grow, and gaps in the species catalogue are filled, BIN lists assembled by metabarcoding will provide greater taxonomic resolution. The present study has three main goals: (a) to provide a DNA barcode library for 5,200 BINs of Diptera; (b) to demonstrate, based on the example of bulk extractions from a Malaise trap experiment, that DNA barcode clusters, labelled with globally unique identifiers (such as OTUs and/or BINs), provide a pragmatic, accurate solution to the “taxonomic impediment”; and (c) to demonstrate that interim names based on BINs and OTUs obtained through metabarcoding provide an effective method for studies on species‐rich groups that are usually neglected in biodiversity research projects because of their unresolved taxonomy.",what Studied taxonomic group (Biology) ?,Diptera,"diptera,",True,True
"For the first time, a nearly complete barcode library for European Gelechiidae is provided. DNA barcode sequences (COI gene - cytochrome c oxidase 1) from 751 out of 865 nominal species, belonging to 105 genera, were successfully recovered. A total of 741 species represented by specimens with sequences ≥ 500bp and an additional ten species represented by specimens with shorter sequences were used to produce 53 NJ trees. Intraspecific barcode divergence averaged only 0.54% whereas distance to the Nearest-Neighbour species averaged 5.58%. Of these, 710 species possessed unique DNA barcodes, but 31 species could not be reliably discriminated because of barcode sharing or partial barcode overlap. Species discrimination based on the Barcode Index System (BIN) was successful for 668 out of 723 species which clustered from minimum one to maximum 22 unique BINs. Fifty-five species shared a BIN with up to four species and identification from DNA barcode data is uncertain. Finally, 65 clusters with a unique BIN remained unidentified to species level. These putative taxa, as well as 114 nominal species with more than one BIN, suggest the presence of considerable cryptic diversity, cases which should be examined in future revisionary studies.",what Studied taxonomic group (Biology) ?,Gelechiidae,gelechiidae,True,True
"This study provides a first, comprehensive, diagnostic use of DNA barcodes for the Canadian fauna of noctuoids or “owlet” moths (Lepidoptera: Noctuoidea) based on vouchered records for 1,541 species (99.1% species coverage), and more than 30,000 sequences. When viewed from a Canada-wide perspective, DNA barcodes unambiguously discriminate 90% of the noctuoid species recognized through prior taxonomic study, and resolution reaches 95.6% when considered at a provincial scale. Barcode sharing is concentrated in certain lineages with 54% of the cases involving 1.8% of the genera. Deep intraspecific divergence exists in 7.7% of the species, but further studies are required to clarify whether these cases reflect an overlooked species complex or phylogeographic variation in a single species. Non-native species possess higher Nearest-Neighbour (NN) distances than native taxa, whereas generalist feeders have lower NN distances than those with more specialized feeding habits. We found high concordance between taxonomic names and sequence clusters delineated by the Barcode Index Number (BIN) system with 1,082 species (70%) assigned to a unique BIN. The cases of discordance involve both BIN mergers and BIN splits with 38 species falling into both categories, most likely reflecting bidirectional introgression. One fifth of the species are involved in a BIN merger reflecting the presence of 158 species sharing their barcode sequence with at least one other taxon, and 189 species with low, but diagnostic COI divergence. A very few cases (13) involved species whose members fell into both categories. Most of the remaining 140 species show a split into two or three BINs per species, while Virbia ferruginosa was divided into 16. The overall results confirm that DNA barcodes are effective for the identification of Canadian noctuoids. This study also affirms that BINs are a strong proxy for species, providing a pathway for a rapid, accurate estimation of animal diversity.",what Studied taxonomic group (Biology) ?,Noctuoidea,lepidoptera :,False,False
"Abstract The DNA barcode reference library for Lepidoptera holds much promise as a tool for taxonomic research and for providing the reliable identifications needed for conservation assessment programs. We gathered sequences for the barcode region of the mitochondrial cytochrome c oxidase subunit I gene from 160 of the 176 nominal species of Erebidae moths (Insecta: Lepidoptera) known from the Iberian Peninsula. These results arise from a research project which constructing a DNA barcode library for the insect species of Spain. New records for 271 specimens (122 species) are coupled with preexisting data for 38 species from the Iberian fauna. Mean interspecific distance was 12.1%, while the mean nearest neighbour divergence was 6.4%. All 160 species possessed diagnostic barcode sequences, but one pair of congeneric taxa (Eublemma rosea and Eublemma rietzi) were assigned to the same BIN. As well, intraspecific sequence divergences higher than 1.5% were detected in four species which likely represent species complexes. This study reinforces the effectiveness of DNA barcoding as a tool for monitoring biodiversity in particular geographical areas and the strong correspondence between sequence clusters delineated by BINs and species recognized through detailed taxonomic analysis.",what Order (Taxonomy - biology) ?,Lepidoptera,lepidoptera,True,True
"The proliferation of DNA data is revolutionizing all fields of systematic research. DNA barcode sequences, now available for millions of specimens and several hundred thousand species, are increasingly used in algorithmic species delimitations. This is complicated by occasional incongruences between species and gene genealogies, as indicated by situations where conspecific individuals do not form a monophyletic cluster in a gene tree. In two previous reviews, non-monophyly has been reported as being common in mitochondrial DNA gene trees. We developed a novel web service “Monophylizer” to detect non-monophyly in phylogenetic trees and used it to ascertain the incidence of species non-monophyly in COI (a.k.a. cox1) barcode sequence data from 4977 species and 41,583 specimens of European Lepidoptera, the largest data set of DNA barcodes analyzed from this regard. Particular attention was paid to accurate species identification to ensure data integrity. We investigated the effects of tree-building method, sampling effort, and other methodological issues, all of which can influence estimates of non-monophyly. We found a 12% incidence of non-monophyly, a value significantly lower than that observed in previous studies. Neighbor joining (NJ) and maximum likelihood (ML) methods yielded almost equal numbers of non-monophyletic species, but 24.1% of these cases of non-monophyly were only found by one of these methods. Non-monophyletic species tend to show either low genetic distances to their nearest neighbors or exceptionally high levels of intraspecific variability. Cases of polyphyly in COI trees arising as a result of deep intraspecific divergence are negligible, as the detected cases reflected misidentifications or methodological errors. Taking into consideration variation in sampling effort, we estimate that the true incidence of non-monophyly is ∼23%, but with operational factors still being included. Within the operational factors, we separately assessed the frequency of taxonomic limitations (presence of overlooked cryptic and oversplit species) and identification uncertainties. We observed that operational factors are potentially present in more than half (58.6%) of the detected cases of non-monophyly. Furthermore, we observed that in about 20% of non-monophyletic species and entangled species, the lineages involved are either allopatric or parapatric—conditions where species delimitation is inherently subjective and particularly dependent on the species concept that has been adopted. These observations suggest that species-level non-monophyly in COI gene trees is less common than previously supposed, with many cases reflecting misidentifications, the subjectivity of species delimitation or other operational factors.",what Order (Taxonomy - biology) ?,Lepidoptera,taxonomy - biology ),False,False
"This study reports the assembly of a DNA barcode reference library for species in the lepidopteran superfamily Noctuoidea from Canada and the USA. Based on the analysis of 69,378 specimens, the library provides coverage for 97.3% of the noctuoid fauna (3565 of 3664 species). In addition to verifying the strong performance of DNA barcodes in the discrimination of these species, the results indicate close congruence between the number of species analyzed (3565) and the number of sequence clusters (3816) recognized by the Barcode Index Number (BIN) system. Distributional patterns across 12 North American ecoregions are examined for the 3251 species that have GPS data while BIN analysis is used to quantify overlap between the noctuoid faunas of North America and other zoogeographic regions. This analysis reveals that 90% of North American noctuoids are endemic and that just 7.5% and 1.8% of BINs are shared with the Neotropics and with the Palearctic, respectively. One third (29) of the latter species are recent introductions and, as expected, they possess low intraspecific divergences.",what Order (Taxonomy - biology) ?,Lepidoptera,noctuoidea,False,False
"Mosquitoes are insects of the Diptera, Nematocera, and Culicidae families, some species of which are important disease vectors. Identifying mosquito species based on morphological characteristics is difficult, particularly the identification of specimens collected in the field as part of disease surveillance programs. Because of this difficulty, we constructed DNA barcodes of the cytochrome c oxidase subunit 1, the COI gene, for the more common mosquito species in China, including the major disease vectors. A total of 404 mosquito specimens were collected and assigned to 15 genera and 122 species and subspecies on the basis of morphological characteristics. Individuals of the same species grouped closely together in a Neighborhood-Joining tree based on COI sequence similarity, regardless of collection site. COI gene sequence divergence was approximately 30 times higher for species in the same genus than for members of the same species. Divergence in over 98% of congeneric species ranged from 2.3% to 21.8%, whereas divergence in conspecific individuals ranged from 0% to 1.67%. Cryptic species may be common and a few pseudogenes were detected.",what Order (Taxonomy - biology) ?,Diptera,"diptera,",True,True
"The introduced C4 bunchgrass, Schizachyrium condensatum, is abundant in unburned, seasonally dry woodlands on the island of Hawaii, where it promotes the spread of fire. After fire, it is partially replaced by Melinis minutiflora, another invasive C4 grass. Seed bank surveys in unburned woodland showed that Melinis seed is present in locations without adult plants. Using a combination of germination tests and seedling outplant ex- periments, we tested the hypothesis that Melinis was unable to invade the unburned wood- land because of nutrient and/or light limitation. We found that Melinis germination and seedling growth are depressed by the low light levels common under Schizachyrium in unburned woodland. Outplanted Melinis seedlings grew rapidly to flowering and persisted for several years in unburned woodland without nutrient additions, but only if Schizachyrium individuals were removed. Nutrients alone did not facilitate Melinis establishment. Competition between Melinis and Schizachyrium naturally occurs when individuals of both species emerge from the seed bank simultaneously, or when seedlings of one species emerge in sites already dominated by individuals of the other species. When both species are grown from seed, we found that Melinis consistently outcompetes Schizachyrium, re- gardless of light or nutrient treatments. When seeds of Melinis were added to pots with well-established Schizachyrium (and vice versa), Melinis eventually invaded and overgrew adult Schizachyrium under high, but not low, nutrients. By contrast, Schizachyrium could not invade established Melinis pots regardless of nutrient level. A field experiment dem- onstrated that Schizachyrium individuals are suppressed by Melinis in burned sites through competition for both light and nutrients. Overall, Melinis is a dominant competitor over Schizachyrium once it becomes estab- lished, whether in a pot or in the field. We believe that the dominance of Schizachyrium, rather than Melinis, in the unburned woodland is the result of asymmetric competition due to the prior establishment of Schizachyrium in these sites. If Schizachyrium were not present, the unburned woodland could support dense stands of Melinis. Fire disrupts the priority effect of Schizachyrium and allows the dominant competitor (Melinis) to enter the system where it eventually replaces Schizachyrium through resource competition.",what Type of disturbance ?,Fire,fire.,True,True
"Context. Wildfire is a major driver of the structure and function of mallee eucalypt- and spinifex-dominated landscapes. Understanding how fire influences the distribution of biota in these fire-prone environments is essential for effective ecological and conservation-based management. Aims. We aimed to (1) determine the effects of an extensive wildfire (118 000 ha) on a small mammal community in the mallee shrublands of semiarid Australia and (2) assess the hypothesis that the fire-response patterns of small mammals can be predicted by their life-history characteristics. Methods. Small-mammal surveys were undertaken concurrently at 26 sites: once before the fire and on four occasions following the fire (including 14 sites that remained unburnt). We documented changes in small-mammal occurrence before and after the fire, and compared burnt and unburnt sites. In addition, key components of vegetation structure were assessed at each site. Key results. Wildfire had a strong influence on vegetation structure and on the occurrence of small mammals. The mallee ningaui, Ningaui yvonneae, a dasyurid marsupial, showed a marked decline in the immediate post-fire environment, corresponding with a reduction in hummock-grass cover in recently burnt vegetation. Species richness of native small mammals was positively associated with unburnt vegetation, although some species showed no clear response to wildfire. Conclusions. Our results are consistent with the contention that mammal responses to fire are associated with their known life-history traits. The species most strongly affected by wildfire, N. yvonneae, has the most specific habitat requirements and restricted life history of the small mammals in the study area. The only species positively associated with recently burnt vegetation, the introduced house mouse, Mus domesticus, has a flexible life history and non-specialised resource requirements. Implications. Maintaining sources for recolonisation after large-scale wildfires will be vital to the conservation of native small mammals in mallee ecosystems.",what Type of disturbance ?,Fire,wildfire,False,True
"Questions: How did post-wildfire understorey plant community response, including exotic species response, differ between pre-fire treated areas that were less severely burned, and pre-fire untreated areas that were more severely burned? Were these differences consistent through time? Location: East-central Arizona, southwestern US. Methods: We used a multi-year data set from the 2002 Rodeo–Chediski Fire to detect post-fire trends in plant community response in burned ponderosa pine forests. Within the burn perimeter, we examined the effects of pre-fire fuels treatments on post-fire vegetation by comparing paired treated and untreated sites on the Apache-Sitgreaves National Forest. We sampled these paired sites in 2004, 2005 and 2011. Results: There were significant differences in pre-fire treated and untreated plant communities by species composition and abundance in 2004 and 2005, but these communities were beginning to converge in 2011. Total understorey plant cover was significantly higher in untreated areas for all 3 yr. Plant cover generally increased between 2004 and 2005 and markedly decreased in 2011, with the exception of shrub cover, which steadily increased through time. The sharp decrease in forb and graminoid cover in 2011 is likely related to drought conditions since the fire. Annual/biennial forb and graminoid cover decreased relative to perennial cover through time, consistent with the initial floristics hypothesis. Exotic plant response was highly variable and not limited to the immediate post-fire, annual/biennial community. Despite low overall exotic forb and graminoid cover for all years (<2.5%), several exotic species increased in frequency, and the relative proportion of exotic to native cover increased through time. Conclusions: Pre-treatment fuel reduction treatments helped maintain foundation overstorey species and associated native plant communities following this large wildfire. The overall low cover of exotic species on these sites supports other findings that the disturbance associated with high-severity fire does not always result in exotic species invasions. The increase in relative cover and frequency though time indicates that some species are proliferating, and continued monitoring is recommended. Patterns of exotic species invasions after severe burning are not easily predicted, and are likely more dependent on site-specific factors such as propagules, weather patterns and management.",what Type of disturbance ?,Fire,wildfire,False,True
"Streams in mediterranean‐type climate regions are shaped by predictable seasonal events of flooding and drying over an annual cycle, but also present a strong interannual flow variation.",what Type of disturbance ?,Flood,flooding,False,True
"Native herbivores can establish novel interactions with alien plants after invasion. Nevertheless, it is unclear whether these new associations are quantitatively significant compared to the assemblages with native flora under natural conditions. Herbivores associated with two exotic plants, namely Senecio inaequidens and S. pterophorus, and two coexisting natives, namely S. vulgaris and S. lividus, were surveyed in a replicated long‐term field study to ascertain whether the plant–herbivore assemblages in mixed communities are related to plant novelty and insect diet breadth. Native herbivores used exotic Senecio as their host plants. Of the 19 species of Lepidoptera, Diptera, and Hemiptera found in this survey, 14 were associated with the exotic Senecio plants. Most of these species were polyphagous, yet we found a higher number of individuals with a narrow diet breadth, which is contrary to the assumption that host switching mainly occurs in generalist herbivores. The Senecio specialist Sphenella marginata (Diptera: Tephritidae) was the most abundant and widely distributed insect species (ca. 80% of the identified specimens). Sphenella was associated with S. lividus, S. vulgaris and S. inaequidens and was not found on S. pterophorus. The presence of native plant congeners in the invaded community did not ensure an instantaneous ecological fitting between insects and alien plants. We conclude that novel associations between native herbivores and introduced Senecio plants are common under natural conditions. Plant novelty is, however, not the only predictor of herbivore abundance due to the complexity of natural conditions.",what Investigated species ?,Plants,plants,True,True
"Effective management of invasive species requires that we understand the mechanisms determining community invasibility. Successful invaders must tolerate abiotic conditions and overcome resistance from native species in invaded habitats. Biotic resistance to invasions may reflect the diversity, abundance, or identity of species in a community. Few studies, however, have examined the relative importance of abiotic and biotic factors determining community invasibility. In a greenhouse experiment, we simulated the abiotic and biotic gradients typically found in vernal pools to better understand their impacts on invasibility. Specifically, we invaded plant communities differing in richness, identity, and abundance of native plants (the ""plant neighborhood"") and depth of inundation to measure their effects on growth, reproduction, and survival of five exotic plant species. Inundation reduced growth, reproduction, and survival of the five exotic species more than did plant neighborhood. Inundation reduced survival of three species and growth and reproduction of all five species. Neighboring plants reduced growth and reproduction of three species but generally did not affect survival. Brassica rapa, Centaurea solstitialis, and Vicia villosa all suffered high mortality due to inundation but were generally unaffected by neighboring plants. In contrast, Hordeum marinum and Lolium multiflorum, whose survival was unaffected by inundation, were more impacted by neighboring plants. However, the four measures describing plant neighborhood differed in their effects. Neighbor abundance impacted growth and reproduction more than did neighbor richness or identity, with growth and reproduction generally decreasing with increasing density and mass of neighbors. Collectively, these results suggest that abiotic constraints play the dominant role in determining invasibility along vernal pool and similar gradients. By reducing survival, abiotic constraints allow only species with the appropriate morphological and physiological traits to invade. In contrast, biotic resistance reduces invasibility only in more benign environments and is best predicted by the abundance, rather than diversity, of neighbors. These results suggest that stressful environments are not likely to be invaded by most exotic species. However, species, such as H. marinum, that are able to invade these habitats require careful management, especially since these environments often harbor rare species and communities.",what Investigated species ?,Plants,plants,True,True
"Theory suggests that introduction effort (propagule size or number) should be a key determinant of establishment success for exotic species. Unfortunately, however, propagule pressure is not recorded for most introductions. Studies must therefore either use proxies whose efficacy must be largely assumed, or ignore effort altogether. The results of such studies will be flawed if effort is not distributed at random with respect to other characteristics that are predicted to influence success. We use global data for more than 600 introduction events for birds to show that introduction effort is both the strongest correlate of introduction success, and correlated with a large number of variables previously thought to influence success. Apart from effort, only habitat generalism relates to establishment success in birds.",what Investigated species ?,Birds,birds,True,True
"What determines the number of alien species in a given region? ‘Native biodiversity’ and ‘human impact’ are typical answers to this question. Indeed, studies comparing different regions have frequently found positive relationships between number of alien species and measures of both native biodiversity (e.g. the number of native species) and human impact (e.g. human population). These relationships are typically explained by biotic acceptance or resistance, i.e. by influence of native biodiversity and human impact on the second step of the invasion process, establishment. The first step of the invasion process, introduction, has often been ignored. Here we investigate whether relationships between number of alien mammals and native biodiversity or human impact in 43 European countries are mainly shaped by differences in number of introduced mammals or establishment success. Our results suggest that correlation between number of native and established mammals is spurious, as it is simply explainable by the fact that both quantities are linked to country area. We also demonstrate that countries with higher human impact host more alien mammals than other countries because they received more introductions than other countries. Differences in number of alien mammals cannot be explained by differences in establishment success. Our findings highlight importance of human activities and question, at least for mammals in Europe, importance of biotic acceptance and resistance.",what Investigated species ?,Mammals,,False,False
"1 During the last centuries many alien species have established and spread in new regions, where some of them cause large ecological and economic problems. As one of the main explanations of the spread of alien species, the enemy‐release hypothesis is widely accepted and frequently serves as justification for biological control. 2 We used a global fungus–plant host distribution data set for 140 North American plant species naturalized in Europe to test whether alien plants are generally released from foliar and floral pathogens, whether they are mainly released from pathogens that are rare in the native range, and whether geographic spread of the North American plant species in Europe is associated with release from fungal pathogens. 3 We show that the 140 North American plant species naturalized in Europe were released from 58% of their foliar and floral fungal pathogen species. However, when we also consider fungal pathogens of the native North American host range that in Europe so far have only been reported on other plant species, the estimated release is reduced to 10.3%. Moreover, in Europe North American plants have mainly escaped their rare, pathogens, of which the impact is restricted to few populations. Most importantly and directly opposing the enemy‐release hypothesis, geographic spread of the alien plants in Europe was negatively associated with their release from fungal pathogens. 4 Synthesis. North American plants may have escaped particular fungal species that control them in their native range, but based on total loads of fungal species, release from foliar and floral fungal pathogens does not explain the geographic spread of North American plant species in Europe. To test whether enemy release is the major driver of plant invasiveness, we urgently require more studies comparing release of invasive and non‐invasive alien species from enemies of different guilds, and studies that assess the actual impact of the enemies.",what Investigated species ?,Plants,plants,True,True
"Disturbance is one of the most important factors promoting exotic invasion. However, if disturbance per se is sufficient to explain exotic success, then “invasion” abroad should not differ from “colonization” at home. Comparisons of the effects of disturbance on organisms in their native and introduced ranges are crucial to elucidate whether this is the case; however, such comparisons have not been conducted. We investigated the effects of disturbance on the success of Eurasian native Centaurea solstitialis in two invaded regions, California and Argentina, and one native region, Turkey, by conducting field experiments consisting of simulating different disturbances and adding locally collected C. solstitialis seeds. We also tested differences among C. solstitialis genotypes in these three regions and the effects of local soil microbes on C. solstitialis performance in greenhouse experiments. Disturbance increased C. solstitialis abundance and performance far more in nonnative ranges than in the native range, but C. solstitialis biomass and fecundity were similar among populations from all regions grown under common conditions. Eurasian soil microbes suppressed growth of C. solstitialis plants, while Californian and Argentinean soil biota did not. We suggest that escape from soil pathogens may contribute to the disproportionately powerful effect of disturbance in introduced regions.",what Investigated species ?,Plants,"plants,",True,True
"Questions: Are island vegetation communities moreinvaded than their mainland counterparts? Is thispattern consistent among community types?Location: The coastal provinces of Catalonia andthepara-oceanicBalearicIslands,bothinNESpain.These islands were connected to the continent morethan 5.35 million years ago and are now locatedo200km from the coast.Methods: We compiled a database of almost 3000phytosociological releve´s from the Balearic Islandsand Catalonia and compared the level of invasionby alien plants in island versus mainland commu-nities. Twenty distinct plant community typeswere compared between island and mainland coun-terparts.Results: The percentage of plots with alien species,number, percentage and cover percentage of alienspecies per plot was greater in Catalonia than in theBalearic Islands in most communities. Overall,across communities, more alien species were foundin the mainland (53) compared to the islands (onlynine). Despite these differences, patterns of the levelof invasion in communities were highly consistentbetween the islands and mainland. The most in-vaded communities were ruderal and riparian.Main conclusion: Our results indicate that para-oceanic island communities such as the BalearicIslands are less invaded than their mainlandcounterparts. This difference reﬂects a smaller re-gional alien species pool in the Balearic Islands thanin the adjacent mainland, probably due to differ-ences in landscape heterogeneity and propagulepressure.Keywords: alien plants; Balearic Islands; communitysimilarity; Mediterranean communities; para-ocea-nic islands; releve´; species richness.Nomenclature: Bolo`s & Vigo (1984–2001), Rivas-Martinez et al. (2001).",what Investigated species ?,Plants,plants,True,True
"The Enemy Release Hypothesis (ERH) predicts that when plant species are introduced outside their native range there is a release from natural enemies resulting in the plants becoming problematic invasive alien species (Lake & Leishman 2004; Puliafico et al. 2008). The release from natural enemies may benefit alien plants more than simply reducing herbivory because, according to the Evolution of Increased Competitive Ability (EICA) hypothesis, without pressure from herbivores more resources that were previously allocated to defence can be allocated to reproduction (Blossey & Notzold 1995). Alien invasive plants are therefore expected to have simpler herbivore communities with fewer specialist herbivores (Frenzel & Brandl 2003; Heleno et al. 2008; Heger & Jeschke 2014).",what Investigated species ?,Plants,plants,True,True
"Invasiveness may result from genetic variation and adaptation or phenotypic plasticity, and genetic variation in fitness traits may be especially critical. Pennisetum setaceum (fountain grass, Poaceae) is highly invasive in Hawaii (HI), moderately invasive in Arizona (AZ), and less invasive in southern California (CA). In common garden experiments, we examined the relative importance of quantitative trait variation, precipitation, and phenotypic plasticity in invasiveness. In two very different environments, plants showed no differences by state of origin (HI, CA, AZ) in aboveground biomass, seeds/flower, and total seed number. Plants from different states were also similar within watering treatment. Plants with supplemental watering, relative to unwatered plants, had greater biomass, specific leaf area (SLA), and total seed number, but did not differ in seeds/flower. Progeny grown from seeds produced under different watering treatments showed no maternal effects in seed mass, germination, biomass or SLA. High phenotypic plasticity, rather than local adaptation is likely responsible for variation in invasiveness. Global change models indicate that temperature and precipitation patterns over the next several decades will change, although the direction of change is uncertain. Drier summers in southern California may retard further invasion, while wetter summers may favor the spread of fountain grass.",what Investigated species ?,Plants,plants,True,True
"Non-native mammals that are disturbance agents can promote non-native plant invasions, but to date there is scant evidence on the mechanisms behind this pattern. We used wild boar (Sus scrofa) as a model species to evaluate the role of non-native mammals in promoting plant invasion by identifying the degree to which soil disturbance and endozoochorous seed dispersal drive plant invasions. To test if soil disturbance promotes plant invasion, we conducted an exclosure experiment in which we recorded emergence, establishment and biomass of seedlings of seven non-native plant species planted in no-rooting, boar-rooting and artificial rooting patches in Patagonia, Argentina. To examine the role of boar in dispersing seeds we germinated viable seeds from 181 boar droppings and compared this collection to the soil seed bank by collecting a soil sample adjacent to each dropping. We found that both establishment and biomass of non-native seedlings in boar-rooting patches were double those in no-rooting patches. Values in artificial rooting patches were intermediate between those in boar-rooting and no-rooting treatments. By contrast, we found that the proportion of non-native seedlings in the soil samples was double that in the droppings, and over 80% of the germinated seeds were native species in both samples. Lastly, an effect size test showed that soil disturbance by wild boar rather than endozoochorous dispersal facilitates plant invasions. These results have implications for both the native and introduced ranges of wild boar, where rooting disturbance may facilitate community composition shifts.",what Investigated species ?,Mammals,mammals,True,True
"In their colonized ranges, exotic plants may be released from some of the herbivores or pathogens of their home ranges but these can be replaced by novel enemies. It is of basic and practical interest to understand which characteristics of invaded communities control accumulation of the new pests. Key questions are whether enemy load on exotic species is smaller than on native competitors as suggested by the enemy release hypothesis (ERH) and whether this difference is most pronounced in resource‐rich habitats as predicted by the resource–enemy release hypothesis (R‐ERH). In 72 populations of 12 exotic invasive species, we scored all visible above‐ground damage morphotypes caused by herbivores and fungal pathogens. In addition, we quantified levels of leaf herbivory and fruit damage. We then assessed whether variation in damage diversity and levels was explained by habitat fertility, by relatedness between exotic species and the native community or rather by native species diversity. In a second part of the study, we also tested the ERH and the R‐ERH by comparing damage of plants in 28 pairs of co‐occurring native and exotic populations, representing nine congeneric pairs of native and exotic species. In the first part of the study, diversity of damage morphotypes and damage levels of exotic populations were greater in resource‐rich habitats. Co‐occurrence of closely related, native species in the community significantly increased the probability of fruit damage. Herbivory on exotics was less likely in communities with high phylogenetic diversity. In the second part of the study, exotic and native congeneric populations incurred similar damage diversity and levels, irrespective of whether they co‐occurred in nutrient‐poor or nutrient‐rich habitats. Synthesis. We identified habitat productivity as a major community factor affecting accumulation of enemy damage by exotic populations. Similar damage levels in exotic and native congeneric populations, even in species pairs from fertile habitats, suggest that the enemy release hypothesis or the R‐ERH cannot always explain the invasiveness of introduced species.",what Investigated species ?,Plants,plants,True,True
"Few field experiments have examined the effects of both resource availability and propagule pressure on plant community invasibility. Two non-native forest species, a herb and a shrub (Hesperis matronalis and Rhamnus cathartica, respectively), were sown into 60 1-m 2 sub-plots distributed across three plots. These contained reconstructed native plant communities in a replaced surface soil layer in a North American forest interior. Resource availability and propagule pressure were manipulated as follows: understorey light level (shaded/unshaded), nutrient availability (control/fertilized), and seed pressures of the two non-native species (control/low/high). Hesperis and Rhamnus cover and the above-ground biomass of Hesperis were significantly higher in shaded sub-plots and at greater propagule pressures. Similarly, the above-ground biomass of Rhamnus was significantly increased with propagule pressure, although this was a function of density. In contrast, of species that seeded into plots from the surrounding forest during the growing season, the non-native species had significantly greater cover in unshaded sub-plots. Plants in these unshaded sub-plots were significantly taller than plants in shaded sub-plots, suggesting a greater fitness. Total and non-native species richness varied significantly among plots indicating the importance of fine-scale dispersal patterns. None of the experimental treatments influenced native species. Since the forest seed bank in our study was colonized primarily by non-native ruderal species that dominated understorey vegetation, the management of invasions by non-native species in forest understoreys will have to address factors that influence light levels and dispersal pathways.",what Investigated species ?,Plants,plants,True,True
"We surveyed naturally occurring leaf herbivory in nine invasive and nine non-invasive exotic plant species sampled in natural areas in Ontario, New York and Massachusetts, and found that invasive plants experienced, on average, 96% less leaf damage than non-invasive species. Invasive plants were also more taxonomically isolated than non-invasive plants, belonging to families with 75% fewer native North American genera. However, the relationship between taxonomic isolation at the family level and herbivory was weak. We suggest that invasive plants may possess novel phytochemicals with anti-herbivore properties in addition to allelopathic and anti-microbial characteristics. Herbivory could be employed as an easily measured predictor of the likelihood that recently introduced exotic plants may become invasive.",what Investigated species ?,Plants,plants,True,True
"The vegetation of Kings Park, near the centre of Perth, Western Australia, once had an overstorey of Eucalyptus marginata (jarrah) or Eucalyptus gomphocephala (tuart), and many trees still remain in the bushland parts of the Park. Avenues and roadsides have been planted with eastern Australian species, including Eucalyptus cladocalyx (sugar gum) and Eucalyptus botryoides (southern mahogany), both of which have become invasive. The present study examined the effect of a recent burn on the level of herbivory on these native and exotic eucalypts. Leaf damage, shoot extension and number of new leaves were measured on tagged shoots of saplings of each tree species in unburnt and burnt areas over an 8-month period. Leaf macronutrient levels were quantified and the number of arthropods on saplings was measured at the end of the recording period by chemical knockdown. Leaf macronutrients were mostly higher in all four species in the burnt area, and this was associated with generally higher numbers of canopy arthropods and greater levels of leaf damage. It is suggested that the pulse of soil nutrients after the fire resulted in more nutrient-rich foliage, which in turn was more palatable to arthropods. The resulting high levels of herbivory possibly led to reduced shoot extension of E. gomphocephala, E. botryoides and, to a lesser extent, E. cladocalyx. This acts as a negative feedback mechanism that lessens the tendency for lush, post-fire regrowth to outcompete other species of plants. There was no consistent difference in the levels of the various types of leaf damage or of arthropods on the native and the exotic eucalypts, suggesting that freedom from herbivory is not contributing to the invasiveness of the two exotic species.",what Investigated species ?,Plants,plants.,True,True
"While small-scale studies show that more diverse native communities are less invasible by exotics, studies at large spatial scales often find positive correlations between native and exotic diversity. This large-scale pattern is thought to arise because landscapes with favorable conditions for native species also have favorable conditions for exotic species. From theory, we proposed an alternative hypothesis: the positive relationship at large scales is driven by spatial heterogeneity in species composition, which is driven by spatial heterogeneity in the environment. Landscapes with more spatial heterogeneity in the environment can sustain more native and more exotic species, leading to a positive correlation of native and exotic diversity at large scales. In a nested data set for grassland plants, we detected negative relationships between native and exotic diversity at small spatial scales and positive relationships at large spatial scales. Supporting our hypothesis, the positive relationships between native and exotic diversity at large scales were driven by positive relationships between native and exotic beta diversity. Further, both native and exotic diversity were positively correlated with spatial heterogeneity in abiotic conditions (variance of soil depth, soil nitrogen, and aspect) but were uncorrelated with average abiotic conditions, supporting the spatial-heterogeneity hypothesis but not the favorable-conditions",what Investigated species ?,Plants,"plants,",True,True
"The enemy release hypothesis (ERH) is often cited to explain why some plants successfully invade natural communities while others do not. This hypothesis maintains that plant populations are regulated by coevolved enemies in their native range but are relieved of this pressure where their enemies have not been co-introduced. Some studies have shown that invasive plants sustain lower levels of herbivore damage when compared to native species, but how damage affects fitness and population dynamics remains unclear. We used a system of co-occurring native and invasive Eugenia congeners in south Florida (USA) to experimentally test the ERH, addressing deficiencies in our understanding of the role of natural enemies in plant invasion at the population level. Insecticide was used to experimentally exclude insect herbivores from invasive Eugenia uniflora and its native co-occurring congeners in the field for two years. Herbivore damage, plant growth, survival, and population growth rates for the three species were then compared for control and insecticide-treated plants. Our results contradict the ERH, indicating that E. uniflora sustains more herbivore damage than its native congeners and that this damage negatively impacts stem height, survival, and population growth. In addition, most damage to E. uniflora, a native of Brazil, is carried out by Myllocerus undatus, a recently introduced weevil from Sri Lanka, and M. undatus attacks a significantly greater proportion of E. uniflora leaves than those of its native congeners. This interaction is particularly interesting because M. undatus and E. uniflora share no coevolutionary history, having arisen on two separate continents and come into contact on a third. Our study is the first to document negative population-level effects for an invasive plant as a result of the introduction of a novel herbivore. Such inhibitory interactions are likely to become more prevalent as suites of previously noninteracting species continue to accumulate and new communities assemble worldwide.",what Investigated species ?,Plants,plants,True,True
"This is an analysis of the attempts to colonize at least 208 species of parasites and predators on about 75 species of pest insects in the field in Canada. There was colonization by about 10% of the species that were introduced in totals of under 5,000 individuals, 40% of those introduced in totals of between 5,000 and 31,200, and 78% of those introduced in totals of over 31,200. Indications exist that initial colonizations may be favoured by large releases and by selection of release sites that are semi-isolated and not ecologically complex but that colonizations are hindered when the target species differs taxonomically from the species from which introduced agents originated and when the release site lacks factors needed for introduced agents to survive or when it is subject to potentially-avoidable physical disruptions. There was no evidence that the probability of colonization was increased when the numbers of individuals released were increased by laboratory propagation. About 10% of the attempts were successful from the economic viewpoint. Successes may be overestimated if the influence of causes of coincidental, actual, or supposed changes in pest abundance are overlooked. Most of the successes were by two or more kinds of agents of which at least one attacked species additional to the target pests. Unplanned consequences of colonization have not been sufficiently harmful to warrant precautions to the extent advocated by Turnbull and Chant but are sufficiently potentially dangerous to warrant the restriction of all colonization attempts to biological control experts. It is concluded that most failures were caused by inadequate procedures, rather than by any weaknesses inherent in the method, that those inadequacies can be avoided in the future, and therefore that biological control of pest insects has much unrealized potential for use in Canada.",what Investigated species ?,Insects,insects,True,True
"Surveys of recent (1973 to 1986) intentional releases of native birds and mammals to the wild in Australia, Canada, Hawaii, New Zealand, and the United States were conducted to document current activities, identify factors associated with success, and suggest guidelines for enhancing future work. Nearly 700 translocations were conducted each year. Native game species constituted 90 percent of translocations and were more successful (86 percent) than were translocations of threatened, endangered, or sensitive species (46 percent). Knowledge of habitat quality, location of release area within the species range, number of animals released, program length, and reproductive traits allowed correct classification of 81 percent of observed translocations as successful or not.",what Investigated species ?,Birds and Mammals,birds and mammals,True,True
"sempervirens L., a non-invasive native. We hypothesized that greater morphological plasticity may contribute to the ability of L. japonica to occupy more habitat types, and contribute to its invasiveness. We compared the morphology of plants provided with climbing supports with plants that had no climbing supports, and thus quantified their morphological plasticity in response to an important variable in their habitats. The two species responded differently to the treatments, with L. japonica showing greater responses in more characters. For example, Lonicera japonica responded to climbing supports with a 15.3% decrease in internode length, a doubling of internode number and a 43% increase in shoot biomass. In contrast, climbing supports did not influence internode length or shoot biomass for L. sempervirens, and only resulted in a 25% increase in internode number. This plasticity may allow L. japonica to actively place plant modules in favorable microhabitats and ultimately affect plant fitness.",what Investigated species ?,Plants,plants,True,True
"Many ecosystems are created by the presence of ecosystem engineers that play an important role in determining species' abundance and species composition. Additionally, a mosaic environment of engineered and non-engineered habitats has been shown to increase biodiversity. Non-native ecosystem engineers can be introduced into environments that do not contain or have lost species that form biogenic habitat, resulting in dramatic impacts upon native communities. Yet, little is known about how non-native ecosystem engineers interact with natives and other non-natives already present in the environment, specifically whether non-native ecosystem engineers facilitate other non-natives, and whether they increase habitat heterogeneity and alter the diversity, abundance, and distribution of benthic species. Through sampling and experimental removal of reefs, we examine the effects of a non-native reef-building tubeworm, Ficopomatus enigmaticus, on community composition in the central Californian estuary, Elkhorn Slough. Tubeworm reefs host significantly greater abundances of many non-native polychaetes and amphipods, particularly the amphipods Monocorophium insidiosum and Melita nitida, compared to nearby mudflats. Infaunal assemblages under F. enigmaticus reefs and around reef's edges show very low abundance and taxonomic diversity. Once reefs are removed, the newly exposed mudflat is colonized by opportunistic non-native species, such as M. insidiosum and the polychaete Streblospio benedicti, making removal of reefs a questionable strategy for control. These results show that provision of habitat by a non-native ecosystem engineer may be a mechanism for invasional meltdown in Elkhorn Slough, and that reefs increase spatial heterogeneity in the abundance and composition of benthic communities.",what Investigated species ?,Polychaetes,"amphipods,",False,False
"Brown, R. L. and Fridley, J. D. 2003. Control of plant species diversity andcommunity invasibility by species immigration: seed richness versus seed density. –Oikos 102: 15–24.Immigration rates of species into communities are widely understood to inﬂuencecommunity diversity, which in turn is widely expected to inﬂuence the susceptibilityof ecosystems to species invasion. For a given community, however, immigrationprocesses may impact diversity by means of two separable components: the numberof species represented in seed inputs and the density of seed per species. Theindependent effects of these components on plant species diversity and consequentrates of invasion are poorly understood. We constructed experimental plant commu-nities through repeated seed additions to independently measure the effects of seedrichness and seed density on the trajectory of species diversity during the develop-ment of annual plant communities. Because we sowed species not found in theimmediate study area, we were able to assess the invasibility of the resultingcommunities by recording the rate of establishment of species from adjacent vegeta-tion. Early in community development when species only weakly interacted, seedrichness had a strong effect on community diversity whereas seed density had littleeffect. After the plants became established, the effect of seed richness on measureddiversity strongly depended on seed density, and disappeared at the highest level ofseed density. The ability of surrounding vegetation to invade the experimentalcommunities was decreased by seed density but not by seed richness, primarilybecause the individual effects of a few sown species could explain the observedinvasion rates. These results suggest that seed density is just as important as seedrichness in the control of species diversity, and perhaps a more important determi-nant of community invasibility than seed richness in dynamic plant assemblages.",what Investigated species ?,Plants,plants,True,True
"ABSTRACT Question: Do anthropogenic activities facilitate the distribution of exotic plants along steep altitudinal gradients? Location: Sani Pass road, Grassland biome, South Africa. Methods: On both sides of this road, presence and abundance of exotic plants was recorded in four 25-m long road-verge plots and in parallel 25 m × 2 m adjacent land plots, nested at five altitudinal levels: 1500, 1800, 2100, 2400 and 2700 m a.s.l. Exotic community structure was analyzed using Canonical Correspondence Analysis while a two-level nested Generalized Linear Model was fitted for richness and cover of exotics. We tested the upper altitudinal limits for all exotics along this road for spatial clustering around four potential propagule sources using a t-test. Results: Community structure, richness and abundance of exotics were negatively correlated with altitude. Greatest invasion by exotics was recorded for adjacent land at the 1500 m level. Of the 45 exotics, 16 were found at higher altitudes than expected and observations were spatially clustered around potential propagule sources. Conclusions: Spatial clustering of upper altitudinal limits around human inhabited areas suggests that exotics originate from these areas, while exceeding expected altitudinal limits suggests that distribution ranges of exotics are presently underestimated. Exotics are generally characterised by a high propagule pressure and/or persistent seedbanks, thus future tarring of the Sani Pass may result in an increase of exotic species richness and abundance. This would initially result from construction-related soil disturbance and subsequently from increased traffic, water run-off, and altered fire frequency. We suggest examples of management actions to prevent this. Nomenclature: Germishuizen & Meyer (2003).",what Investigated species ?,Plants,plants,True,True
"Ecosystems that are heavily invaded by an exotic species often contain abundant populations of other invasive species. This may reflect shared responses to a common factor, but may also reflect positive interactions among these exotic species. Armand Bayou (Pasadena, TX) is one such ecosystem where multiple species of invasive aquatic plants are common. We used this system to investigate whether presence of one exotic species made subsequent invasions by other exotic species more likely, less likely, or if it had no effect. We performed an experiment in which we selectively removed exotic rooted and/or floating aquatic plant species and tracked subsequent colonization and growth of native and invasive species. This allowed us to quantify how presence or absence of one plant functional group influenced the likelihood of successful invasion by members of the other functional group. We found that presence of alligatorweed (rooted plant) decreased establishment of new water hyacinth (free-floating plant) patches but increased growth of hyacinth in established patches, with an overall net positive effect on success of water hyacinth. Water hyacinth presence had no effect on establishment of alligatorweed but decreased growth of existing alligatorweed patches, with an overall net negative effect on success of alligatorweed. Moreover, observational data showed positive correlations between hyacinth and alligatorweed with hyacinth, on average, more abundant. The negative effect of hyacinth on alligatorweed growth implies competition, not strong mutual facilitation (invasional meltdown), is occurring in this system. Removal of hyacinth may increase alligatorweed invasion through release from competition. However, removal of alligatorweed may have more complex effects on hyacinth patch dynamics because there were strong opposing effects on establishment versus growth. The mix of positive and negative interactions between floating and rooted aquatic plants may influence local population dynamics of each group and thus overall invasion pressure in this watershed.",what Investigated species ?,Plants,plants,True,True
"Factors such as aggressiveness and adaptation to disturbed environments have been suggested as important characteristics of invasive ant species, but diet has rarely been considered. However, because invasive ants reach extraordinary densities at introduced locations, increased feeding efficiency or increased exploitation of new foods should be important in their success. Earlier studies suggest that honeydew produced by Homoptera (e.g., aphids, mealybugs, scale insects) may be important in the diet of the invasive ant species Solenopsis invicta. To determine if this is the case, we studied associations of S. invicta and Homoptera in east Texas and conducted a regional survey for such associations throughout the species' range in the southeast United States. In east Texas, we found that S. invicta tended Ho- moptera extensively and actively constructed shelters around them. The shelters housed a variety of Homoptera whose frequency differed according to either site location or season, presumably because of differences in host plant availability and temperature. Overall, we estimate that the honeydew produced in Homoptera shelters at study sites in east Texas could supply nearly one-half of the daily energetic requirements of an S. invicta colony. Of that, 70% may come from a single species of invasive Homoptera, the mealybugAntonina graminis. Homoptera shelters were also common at regional survey sites and A. graminis occurred in shelters at nine of 11 survey sites. A comparison of shelter densities at survey sites and in east Texas suggests that our results from east Texas could apply throughout the range of S. invicta in the southeast United States. Antonina graminis may be an ex- ceptionally important nutritional resource for S. invicta in the southeast United States. While it remains largely unstudied, the tending of introduced or invasive Homoptera also appears important to other, and perhaps all, invasive ant species. Exploitative or mutually beneficial associations that occur between these insects may be an important, previously unrecognized factor promoting their success.",what Investigated species ?,Insects,ants,False,False
"Plants introduced into a new range are expected to harbour fewer specialized herbivores and to receive less damage than conspecifics in native ranges. Datura stramonium was introduced in Spain about five centuries ago. Here, we compare damage by herbivores, plant size, and leaf trichomes between plants from non-native and native ranges and perform selection analyses. Non-native plants experienced much less damage, were larger and less pubescent than plants of native populations. While plant size was related to fitness in both ranges, selection to increase resistance was only detected in the native region. We suggest this is a consequence of a release from enemies in this new environment.",what Investigated species ?,Plants,plants,True,True
"Genetic diversity is supposed to support the colonization success of expanding species, in particular in situations where microsite availability is constrained. Addressing the role of genetic diversity in plant invasion experimentally requires its manipulation independent of propagule pressure. To assess the relative importance of these components for the invasion of Senecio vernalis, we created propagule mixtures of four levels of genotype diversity by combining seeds across remote populations, across proximate populations, within single populations and within seed families. In a first container experiment with constant Festuca rupicola density as matrix, genotype diversity was crossed with three levels of seed density. In a second experiment, we tested for effects of establishment limitation and genotype diversity by manipulating Festuca densities. Increasing genetic diversity had no effects on abundance and biomass of S. vernalis but positively affected the proportion of large individuals to small individuals. Mixtures composed from proximate populations had a significantly higher proportion of large individuals than mixtures composed from within seed families only. High propagule pressure increased emergence and establishment of S. vernalis but had no effect on individual growth performance. Establishment was favoured in containers with Festuca, but performance of surviving seedlings was higher in open soil treatments. For S. vernalis invasion, we found a shift in driving factors from density dependence to effects of genetic diversity across life stages. While initial abundance was mostly linked to the amount of seed input, genetic diversity, in contrast, affected later stages of colonization probably via sampling effects and seemed to contribute to filtering the genotypes that finally grew up. In consequence, when disentangling the mechanistic relationships of genetic diversity, seed density and microsite limitation in colonization of invasive plants, a clear differentiation between initial emergence and subsequent survival to juvenile and adult stages is required.",what Investigated species ?,Plants,"plants,",True,True
"Plant distributions are in part determined by environmental heterogeneity on both large (landscape) and small (several meters) spatial scales. Plant populations can respond to environmental heterogeneity via genetic differentiation between large distinct patches, and via phenotypic plasticity in response to heterogeneity occurring at small scales relative to dispersal distance. As a result, the level of environmental heterogeneity experienced across generations, as determined by seed dispersal distance, may itself be under selection. Selection could act to increase or decrease seed dispersal distance, depending on patterns of heterogeneity in environmental quality with distance from a maternal home site. Serpentine soils, which impose harsh and variable abiotic stress on non-adapted plants, have been partially invaded by Erodium cicutarium in northern California, USA. Using nearby grassland sites characterized as either serpentine or non-serpentine, we collected seeds from dense patches of E. cicutarium on both soil types in spring 2004 and subsequently dispersed those seeds to one of four distances from their maternal home site (0, 0.5, 1, or 10 m). We examined distance-dependent patterns of variation in offspring lifetime fitness, conspecific density, soil availability, soil water content, and aboveground grass and forb biomass. ANOVA revealed a distinct fitness peak when seeds were dispersed 0.5 m from their maternal home site on serpentine patches. In non-serpentine patches, fitness was reduced only for seeds placed back into the maternal home site. Conspecific density was uniformly high within 1 m of a maternal home site on both soils, whereas soil water content and grass biomass were significantly heterogeneous among dispersal distances only on serpentine soils. Structural equation modeling and multigroup analysis revealed significantly stronger direct and indirect effects linking abiotic and biotic variation to offspring performance on serpentine soils than on non-serpentine soils, indicating the potential for soil-specific selection on seed dispersal distance in this invasive species.",what Investigated species ?,Plants,"plants,",True,True
"Propagule pressure is recognized as a fundamental driver of freshwater fish invasions, though few studies have quantified its role. Natural experiments can be used to quantify the role of this factor relative to others in driving establishment success. An irrigation network in South Africa takes water from an inter-basin water transfer (IBWT) scheme to supply multiple small irrigation ponds. We compared fish community composition upstream, within, and downstream of the irrigation network, to show that this system is a unidirectional dispersal network with a single immigration source. We then assessed the effect of propagule pressure and biological adaptation on the colonization success of nine fish species across 30 recipient ponds of varying age. Establishing species received significantly more propagules at the source than did incidental species, while rates of establishment across the ponds displayed a saturation response to propagule pressure. This shows that propagule pressure is a significant driver of establishment overall. Those species that did not establish were either extremely rare at the immigration source or lacked the reproductive adaptations to breed in the ponds. The ability of all nine species to arrive at some of the ponds illustrates how long-term continuous propagule pressure from IBWT infrastructure enables range expansion of fishes. The quantitative link between propagule pressure and success and rate of population establishment confirms the driving role of this factor in fish invasion ecology.",what Investigated species ?,Fishes,fish,False,False
"Abstract: Invasive alien organisms pose a major threat to global biodiversity. The Cape Peninsula, South Africa, provides a case study of the threat of alien plants to native plant diversity. We sought to identify where alien plants would invade the landscape and what their threat to plant diversity could be. This information is needed to develop a strategy for managing these invasions at the landscape scale. We used logistic regression models to predict the potential distribution of six important invasive alien plants in relation to several environmental variables. The logistic regression models showed that alien plants could cover over 89% of the Cape Peninsula. Acacia cyclops and Pinus pinaster were predicted to cover the greatest area. These predictions were overlaid on the current distribution of native plant diversity for the Cape Peninsula in order to quantify the threat of alien plants to native plant diversity. We defined the threat to native plant diversity as the number of native plant species (divided into all species, rare and threatened species, and endemic species) whose entire range is covered by the predicted distribution of alien plant species. We used a null model, which assumed a random distribution of invaded sites, to assess whether area invaded is confounded with threat to native plant diversity. The null model showed that most alien species threaten more plant species than might be suggested by the area they are predicted to invade. For instance, the logistic regression model predicted that P. pinaster threatens 350 more native species, 29 more rare and threatened species, and 21 more endemic species than the null model would predict. Comparisons between the null and logistic regression models suggest that species richness and invasibility are positively correlated and that species richness is a poor indicator of invasive resistance in the study site. Our results emphasize the importance of adopting a spatially explicit approach to quantifying threats to biodiversity, and they provide the information needed to prioritize threats from alien species and the sites that need urgent management intervention.",what Investigated species ?,Plants,plants,True,True
"Lionfish (Pterois volitans), venomous predators from the Indo-Pacific, are recent invaders of the Caribbean Basin and southeastern coast of North America. Quantification of invasive lionfish abundances, along with potentially important physical and biological environmental characteristics, permitted inferences about the invasion process of reefs on the island of San Salvador in the Bahamas. Environmental wave-exposure had a large influence on lionfish abundance, which was more than 20 and 120 times greater for density and biomass respectively at sheltered sites as compared with wave-exposed environments. Our measurements of topographic complexity of the reefs revealed that lionfish abundance was not driven by habitat rugosity. Lionfish abundance was not negatively affected by the abundance of large native predators (or large native groupers) and was also unrelated to the abundance of medium prey fishes (total length of 5–10 cm). These relationships suggest that (1) higher-energy environments may impose intrinsic resistance against lionfish invasion, (2) habitat complexity may not facilitate the lionfish invasion process, (3) predation or competition by native fishes may not provide biotic resistance against lionfish invasion, and (4) abundant prey fish might not facilitate lionfish invasion success. The relatively low biomass of large grouper on this island could explain our failure to detect suppression of lionfish abundance and we encourage continuing the preservation and restoration of potential lionfish predators in the Caribbean. In addition, energetic environments might exert direct or indirect resistance to the lionfish proliferation, providing native fish populations with essential refuges.",what Investigated species ?,Fishes,,False,False
"Although the species pool, dispersal, and local interactions all influence species diversity, their relative importance is debated. I examined their importance in controlling the number of native and exotic plant species occupying tussocks formed by the sedge Carex nudata along a California stream. Of particular interest were the factors underlying a downstream increase in plant diversity and biological invasions. I conducted seed addition experiments and manipulated local diversity and cover to evaluate the degree to which tussocks saturate with species, and to examine the roles of local competitive processes, abiotic factors, and seed supply in controlling the system-wide patterns. Seeds of three native and three exotic plants sown onto experimentally assembled tussock communities less successfully established on tussocks with a greater richness of resident plants. Nonetheless, even the most diverse tussocks were somewhat colonized, suggesting that tussocks are not completely saturated with species. Similarly, in an experiment where I sowed seeds onto natural tussocks along the river, colonization increased two- to three-fold when I removed the resident species. Even on intact tussocks, however, seed addition increased diversity, indicating that the tussock assemblages are seed limited. Colonization success on cleared and uncleared tussocks increased downstream from km 0 to km 3 of the study site, but showed no trends from km 3 to km 8. This suggests that while abiotic and biotic features of the tussocks may control the increase in diversity and invasions from km 0 to km 3, similar increases from km 3 to km 8 are more likely explained by potential downstream increases in seed supply. The effective water dispersal of seed mimics and prevailingly downstream winds indicated that dispersal most likely occurs in a downstream direction. These results suggest that resident species diversity, competitive interactions, and seed supply similarly influence the colonization of native and exotic species.",what Investigated species ?,Plants,plants.,True,True
"It has been suggested that more species have been successfully introduced to oceanic islands than to mainland regions. This suggestion has attracted considerable ecological interest and several theoretical mechanisms havebeen proposed. However, few data are available to test the hypotheses directly, and the pattern may simply result from many more species being transported to islands rather than mainland regions. Here I test this idea using data for global land birds and present evidence that introductions to islands have a higher probability of success than those to mainland regions. This difference between island and mainland landforms is not consistent among either taxonomic families or biogeographic regions. Instead, introduction attempts within the same biogeographic region have been significantly more successful than those that have occurred between two different biogeographic regions. Subsequently, the proportion of introduction attempts that have occurred within a single biogeographic region is thus a significant predictor of the observed variability in introduction success. I also show that the correlates of successful island introductions are probably different to those of successful mainland introductions.",what Investigated species ?,Birds,birds,True,True
"Introduced species must adapt their ecology, behaviour, and morphological traits to new conditions. The successful introduction and invasive potential of a species are related to its levels of phenotypic plasticity and genetic polymorphism. We analysed changes in the body mass and length of American mink (Neovison vison) since its introduction into the Warta Mouth National Park, western Poland, in relation to diet composition and colonization progress from 1996 to 2004. Mink body mass decreased significantly during the period of population establishment within the study area, with an average decrease of 13% from 1.36 to 1.18 kg in males and of 16% from 0.83 to 0.70 kg in females. Diet composition varied seasonally and between consecutive years. The main prey items were mammals and fish in the cold season and birds and fish in the warm season. During the study period the proportion of mammals preyed upon increased in the cold season and decreased in the warm season. The proportion of birds preyed upon decreased over the study period, whereas the proportion of fish increased. Following introduction, the strictly aquatic portion of mink diet (fish and frogs) increased over time, whereas the proportion of large prey (large birds, muskrats, and water voles) decreased. The average yearly proportion of large prey and average-sized prey in the mink diet was significantly correlated with the mean body masses of males and females. Biogeographical variation in the body mass and length of mink was best explained by the percentage of large prey in the mink diet in both sexes, and by latitude for females. Together these results demonstrate that American mink rapidly changed their body mass in relation to local conditions. This phenotypic variability may be underpinned by phenotypic plasticity and/or by adaptation of quantitative genetic variation. The potential to rapidly change phenotypic variation in this manner is an important factor determining the negative ecological impacts of invasive species. © 2012 The Linnean Society of London, Biological Journal of the Linnean Society, 2012, 105, 681–693.",what Investigated species ?,Mammals,,False,False
"Aim Charles Darwin posited that introduced species with close relatives were less likely to succeed because of fiercer competition resulting from their similarity to residents. There is much debate about the generality of this rule, and recent studies on plant and fish introductions have been inconclusive. Information on phylogenetic relatedness is potentially valuable for explaining invasion outcomes and could form part of screening protocols for minimizing future invasions. We provide the first test of this hypothesis for terrestrial vertebrates using two new molecular phylogenies for native and introduced reptiles for two regions with the best data on introduction histories.",what Investigated species ?,Reptiles,reptiles,True,True
"Abstract: We present a generic scoring system that compares the impact of alien species among members of large taxonomic groups. This scoring can be used to identify the most harmful alien species so that conservation measures to ameliorate their negative effects can be prioritized. For all alien mammals in Europe, we assessed impact reports as completely as possible. Impact was classified as either environmental or economic. We subdivided each of these categories into five subcategories (environmental: impact through competition, predation, hybridization, transmission of disease, and herbivory; economic: impact on agriculture, livestock, forestry, human health, and infrastructure). We assigned all impact reports to one of these 10 categories. All categories had impact scores that ranged from zero (minimal) to five (maximal possible impact at a location). We summed all impact scores for a species to calculate ""potential impact"" scores. We obtained ""actual impact"" scores by multiplying potential impact scores by the percentage of area occupied by the respective species in Europe. Finally, we correlated species’ ecological traits with the derived impact scores. Alien mammals from the orders Rodentia, Artiodactyla, and Carnivora caused the highest impact. In particular, the brown rat (Rattus norvegicus), muskrat (Ondathra zibethicus), and sika deer (Cervus nippon) had the highest overall scores. Species with a high potential environmental impact also had a strong potential economic impact. Potential impact also correlated with the distribution of a species in Europe. Ecological flexibility (measured as number of different habitats a species occupies) was strongly related to impact. The scoring system was robust to uncertainty in knowledge of impact and could be adjusted with weight scores to account for specific value systems of particular stakeholder groups (e.g., agronomists or environmentalists). Finally, the scoring system is easily applicable and adaptable to other taxonomic groups.",what Investigated species ?,Mammals,mammals,True,True
"A commonly cited mechanism for invasion resistance is more complete resource use by diverse plant assemblages with maximum niche complementarity. We investigated the invasion resistance of several plant functional groups against the nonindigenous forb Spotted knapweed (Centaurea maculosa). The study consisted of a factorial combination of seven functional group removals (groups singularly or in combination) and two C. maculosa treatments (addition vs. no addition) applied in a randomized complete block design replicated four times at each of two sites. We quantified aboveground plant material nutrient concentration and uptake (concentration × biomass) by indigenous functional groups: grasses, shallow‐rooted forbs, deep‐rooted forbs, spikemoss, and the nonindigenous invader C. maculosa. In 2001, C. maculosa density depended upon which functional groups were removed. The highest C. maculosa densities occurred where all vegetation or all forbs were removed. Centaurea maculosa densities were the lowest in plots where nothing, shallow‐rooted forbs, deep‐rooted forbs, grasses, or spikemoss were removed. Functional group biomass was also collected and analyzed for nitrogen, phosphorus, potassium, and sulphur. Based on covariate analyses, postremoval indigenous plot biomass did not relate to invasion by C. maculosa. Analysis of variance indicated that C. maculosa tissue nutrient percentage and net nutrient uptake were most similar to indigenous forb functional groups. Our study suggests that establishing and maintaining a diversity of plant functional groups within the plant community enhances resistance to invasion. Indigenous plants of functionally similar groups as an invader may be particularly important in invasion resistance.",what Investigated species ?,Plants,plants,True,True
"Aim Island faunas, particularly those with high levels of endemism, usually are considered especially susceptible to disruption from habitat disturbance and invasive alien species. We tested this general hypothesis by examining the distribution of small mammals along gradients of anthropogenic habitat disturbance in northern Luzon Island, an area with a very high level of mammalian endemism.",what Investigated species ?,Mammals,mammals,True,True
". The effect of fire on annual plants was examined in two vegetation types at remnant vegetation edges in the Western Australian wheatbelt. Density and cover of non-native species were consistently greatest at the reserve edges, decreasing rapidly with increasing distance from reserve edge. Numbers of native species showed little effect of distance from reserve edge. Fire had no apparent effect on abundance of non-natives in Allocasuarina shrubland but abundance of native plants increased. Density of both non-native and native plants in Acacia acuminata-Eucalyptus loxophleba woodland decreased after fire. Fewer non-native species were found in the shrubland than in the woodland in both unburnt and burnt areas, this difference being smallest between burnt areas. Levels of soil phosphorus and nitrate were higher in burnt areas of both communities and ammonium also increased in the shrubland. Levels of soil phosphorus and nitrate were higher at the reserve edge in the unburnt shrubland, but not in the woodland. There was a strong correlation between soil phosphorus levels and abundance of non-native species in the unburnt shrubland, but not after fire or in the woodland. Removal of non-native plants in the burnt shrubland had a strong positive effect on total abundance of native plants, apparently due to increases in growth of smaller, suppressed native plants in response to decreased competition. Two native species showed increased seed production in plots where non-native plants had been removed. There was a general indication that, in the short term, fire does not necessarily increase invasion of these communities by non-native species and could, therefore be a useful management tool in remnant vegetation, providing other disturbances are minimised.",what Investigated species ?,Plants,plants,True,True
"ABSTRACT Early successional ruderal plants in North America include numerous native and nonnative species, and both are abundant in disturbed areas. The increasing presence of nonnative plants may negatively impact a critical component of food web function if these species support fewer or a less diverse arthropod fauna than the native plant species that they displace. We compared arthropod communities on six species of common early successional native plants and six species of nonnative plants, planted in replicated native and nonnative plots in a farm field. Samples were taken twice each year for 2 yr. In most arthropod samples, total biomass and abundance were substantially higher on the native plants than on the nonnative plants. Native plants produced as much as five times more total arthropod biomass and up to seven times more species per 100 g of dry leaf biomass than nonnative plants. Both herbivores and natural enemies (predators and parasitoids) predominated on native plants when analyzed separately. In addition, species richness was about three times greater on native than on nonnative plants, with 83 species of insects collected exclusively from native plants, and only eight species present only on nonnatives. These results support a growing body of evidence suggesting that nonnative plants support fewer arthropods than native plants, and therefore contribute to reduced food resources for higher trophic levels.",what Investigated species ?,Plants,plants,True,True
"Species that are introduced to novel environments can lose their native pathogens and parasites during the process of introduction. The escape from the negative effects associated with these natural enemies is commonly employed as an explanation for the success and expansion of invasive species, which is termed the enemy release hypothesis (ERH). In this study, nested PCR techniques and microscopy were used to determine the prevalence and intensity (respectively) of Plasmodium spp. and Haemoproteus spp. in introduced house sparrows and native urban birds of central Brazil. Generalized linear mixed models were fitted by Laplace approximation considering a binomial error distribution and logit link function. Location and species were considered as random effects and species categorization (native or non-indigenous) as fixed effects. We found that native birds from Brazil presented significantly higher parasite prevalence in accordance with the ERH. We also compared our data with the literature, and found that house sparrows native to Europe exhibited significantly higher parasite prevalence than introduced house sparrows from Brazil, which also supports the ERH. Therefore, it is possible that house sparrows from Brazil might have experienced a parasitic release during the process of introduction, which might also be related to a demographic release (e.g. release from the negative effects of parasites on host population dynamics).",what Investigated species ?,Birds,birds,True,True
"ABSTRACT Question: Do specific environmental conditions affect the performance and growth dynamics of one of the most invasive taxa (Carpobrotus aff. acinaciformis) on Mediterranean islands? Location: Four populations located on Mallorca, Spain. Methods: We monitored growth rates of main and lateral shoots of this stoloniferous plant for over two years (2002–2003), comparing two habitats (rocky coast vs. coastal dune) and two different light conditions (sun vs. shade). In one population of each habitat type, we estimated electron transport rate and the level of plant stress (maximal photochemical efficiency Fv/Fm) by means of chlorophyll fluorescence. Results: Main shoots of Carpobrotus grew at similar rates at all sites, regardless habitat type. However, growth rate of lateral shoots was greater in shaded plants than in those exposed to sunlight. Its high phenotypic plasticity, expressed in different allocation patterns in sun and shade individuals, and its clonal growth which promotes the continuous sea...",what Investigated species ?,Plants,plants,True,True
"We assessed the field-scale plant community associations of Carduus nutans and C. acanthoides, two similar, economically important invasive thistles. Several plant species were associated with the presence of Carduus thistles while others, including an important pasture species, were associated with Carduus free areas. Thus, even within fields, areas invaded by Carduus thistles have different vegetation than uninvaded areas, either because some plants can resist invasion or because invasion changes the local plant community. Our results will allow us to target future research about the role of vegetation structure in resisting and responding to invasion.",what Investigated species ?,Plants,plants,True,True
"Abstract: The successful invasion of exotic plants is often attributed to the absence of coevolved enemies in the introduced range (i.e., the enemy release hypothesis). Nevertheless, several components of this hypothesis, including the role of generalist herbivores, remain relatively unexplored. We used repeated censuses of exclosures and paired controls to investigate the role of a generalist herbivore, white‐tailed deer (Odocoileus virginianus), in the invasion of 3 exotic plant species (Microstegium vimineum, Alliaria petiolata, and Berberis thunbergii) in eastern hemlock (Tsuga canadensis) forests in New Jersey and Pennsylvania (U.S.A.). This work was conducted in 10 eastern hemlock (T. canadensis) forests that spanned gradients in deer density and in the severity of canopy disturbance caused by an introduced insect pest, the hemlock woolly adelgid (Adelges tsugae). We used maximum likelihood estimation and information theoretics to quantify the strength of evidence for alternative models of the influence of deer density and its interaction with the severity of canopy disturbance on exotic plant abundance. Our results were consistent with the enemy release hypothesis in that exotic plants gained a competitive advantage in the presence of generalist herbivores in the introduced range. The abundance of all 3 exotic plants increased significantly more in the control plots than in the paired exclosures. For all species, the inclusion of canopy disturbance parameters resulted in models with substantially greater support than the deer density only models. Our results suggest that white‐tailed deer herbivory can accelerate the invasion of exotic plants and that canopy disturbance can interact with herbivory to magnify the impact. In addition, our results provide compelling evidence of nonlinear relationships between deer density and the impact of herbivory on exotic species abundance. These findings highlight the important role of herbivore density in determining impacts on plant abundance and provide evidence of the operation of multiple mechanisms in exotic plant invasion.",what Investigated species ?,Plants,plants,True,True
"1. Information on the approximate number of individuals released is available for 47 of the 133 exotic bird species introduced to New Zealand in the late 19th and early 20th centuries. Of these, 21 species had populations surviving in the wild in 1969-79. The long interval between introduction and assessment of outcome provides a rare opportunity to examine the factors correlated with successful establishment without the uncertainty of long-term population persistence associated with studies of short duration. 2. The probability of successful establishment was strongly influenced by the number of individuals released during the main period of introductions. Eight-three per cent of species that had more than 100 individuals released within a 10-year period became established, compared with 21% of species that had less than 100 birds released. The relationship between the probability of establishment and number of birds released was similar to that found in a previous study of introductions of exotic birds to Australia. 3. It was possible to look for a within-family influence on the success of introduction of the number of birds released in nine bird families. A positive influence was found within seven families and no effect in two families. This preponderance of families with a positive effect was statistically significant. 4. A significant effect of body weight on the probability of successful establishment was found, and negative effects of clutch size and latitude of origin. However, the statistical significance of these effects varied according to whether comparison was or was not restricted to within-family variation. After applying the Bonferroni adjustment to significance levels, to allow for the large number of variables and factors being considered, only the effect of the number of birds released was statistically significant. 5. No significant effects on the probability of successful establishment were apparent for the mean date of release, the minimum number of years in which birds were released, the hemisphere of origin (northern or southern) and the size and diversity of latitudinal distribution of the natural geographical range.",what Investigated species ?,Birds,,False,False
"Despite long-standing interest of terrestrial ecologists, freshwater ecosystems are a fertile, yet unappreciated, testing ground for applying community phylogenetics to uncover mechanisms of species assembly. We quantify phylogenetic clustering and overdispersion of native and non-native fishes of a large river basin in the American Southwest to test for the mechanisms (environmental filtering versus competitive exclusion) and spatial scales influencing community structure. Contrary to expectations, non-native species were phylogenetically clustered and related to natural environmental conditions, whereas native species were not phylogenetically structured, likely reflecting human-related changes to the basin. The species that are most invasive (in terms of ecological impacts) tended to be the most phylogenetically divergent from natives across watersheds, but not within watersheds, supporting the hypothesis that Darwin's naturalization conundrum is driven by the spatial scale. Phylogenetic distinctiveness may facilitate non-native establishment at regional scales, but environmental filtering restricts local membership to closely related species with physiological tolerances for current environments. By contrast, native species may have been phylogenetically clustered in historical times, but species loss from contemporary populations by anthropogenic activities has likely shaped the phylogenetic signal. Our study implies that fundamental mechanisms of community assembly have changed, with fundamental consequences for the biogeography of both native and non-native species.",what Investigated species ?,Fishes,fishes,True,True
"Methods of risk assessment for alien species, especially for nonagricultural systems, are largely qualitative. Using a generalizable risk assessment approach and statistical models of fish introductions into the Great Lakes, North America, we developed a quantitative approach to target prevention efforts on species most likely to cause damage. Models correctly categorized established, quickly spreading, and nuisance fishes with 87 to 94% accuracy. We then identified fishes that pose a high risk to the Great Lakes if introduced from unintentional (ballast water) or intentional pathways (sport, pet, bait, and aquaculture industries).",what Investigated species ?,Fishes,fishes,True,True
"1 Theory and empirical evidence suggest that community invasibility is influenced by propagule pressure, physical stress and biotic resistance from resident species. We studied patterns of exotic and native species richness across the Flooding Pampas of Argentina, and tested for exotic richness correlates with major environmental gradients, species pool size, and native richness, among and within different grassland habitat types. 2 Native and exotic richness were positively correlated across grassland types, increasing from lowland meadows and halophyte steppes, through humid to mesophyte prairies in more elevated topographic positions. Species pool size was positively correlated with local richness of native and exotic plants, being larger for mesophyte and humid prairies. Localities in the more stressful meadow and halophyte steppe habitats contained smaller fractions of their landscape species pools. 3 Native and exotic species numbers decreased along a gradient of increasing soil salinity and decreasing soil depth, and displayed a unimodal relationship with soil organic carbon. When covarying habitat factors were held constant, exotic and native richness residuals were still positively correlated across sites. Within grassland habitat types, exotic and native species richness were positively associated in meadows and halophyte steppes but showed no consistent relationship in the least stressful, prairie habitat types. 4 Functional group composition differed widely between native and exotic species pools. Patterns suggesting biotic resistance to invasion emerged only within humid prairies, where exotic richness decreased with increasing richness of native warm‐season grasses. This negative relationship was observed for other descriptors of invasion such as richness and cover of annual cool‐season forbs, the commonest group of exotics. 5 Our results support the view that ecological factors correlated with differences in invasion success change with the range of environmental heterogeneity encompassed by the analysis. Within narrow habitat ranges, invasion resistance may be associated with either physical stress or resident native diversity. Biotic resistance through native richness, however, appeared to be effective only at intermediate locations along a stress/fertility gradient. 6 We show that certain functional groups, not just total native richness, may be critical to community resistance to invasion. Identifying such native species groups is important for directing management and conservation efforts.",what Investigated species ?,Plants,"plants,",True,True
"Abstract. An emerging body of literature suggests that the richness of native and naturalized plant species are often positively correlated. It is unclear, however, whether this relationship is robust across spatial scales, and how a disturbance regime may affect it. Here, I examine the relationships of both richness and abundance between native and naturalized species of plants in two mediterranean scrub communities: coastal sage scrub (CSS) in California and xeric‐sloped matorral (XSM) in Chile. In each vegetation type I surveyed multiple sites, where I identified vascular plant species and estimated their relative cover. Herbaceous species richness was higher in XSM, while cover of woody species was higher in CSS, where woody species have a strong impact upon herbaceous species. As there were few naturalized species with a woody growth form, the analyses performed here relate primarily to herbaceous species. Relationships between the herbaceous cover of native and naturalized species were not significant in CSS, but were nearly significant in XSM. The herbaceous species richness of native and naturalized plants were not significantly correlated on sites that had burned less than one year prior to sampling in CSS, and too few sites were available to examine this relationship in XSM. In post 1‐year burn sites, however, herbaceous richness of native and naturalized species were positively correlated in both CSS and XSM. This relationship occurred at all spatial scales, from 400 m2 to 1 m2 plots. The consistency of this relationship in this study, together with its reported occurrence in the literature, suggests that this relationship may be general. Finally, the residuals from the correlations between native and naturalized species richness and cover, when plotted against site age (i.e. time since the last fire), show that richness and cover of naturalized species are strongly favoured on recently burned sites in XSM; this suggests that herbaceous species native to Chile are relatively poorly adapted to fire.",what Investigated species ?,Plants,plants,True,True
"European countries in general, and England in particular, have a long history of introducing non-native fish species, but there exist no detailed studies of the introduction pathways and propagules pressure for any European country. Using the nine regions of England as a preliminary case study, the potential relationship between the occurrence in the wild of non-native freshwater fishes (from a recent audit of non-native species) and the intensity (i.e. propagule pressure) and diversity of fish imports was investigated. The main pathways of introduction were via imports of fishes for ornamental use (e.g. aquaria and garden ponds) and sport fishing, with no reported or suspected cases of ballast water or hull fouling introductions. The recorded occurrence of non-native fishes in the wild was found to be related to the time (number of years) since the decade of introduction. A shift in the establishment rate, however, was observed in the 1970s after which the ratio of established-to-introduced species declined. The number of established non-native fish species observed in the wild was found to increase significantly (P < 0·05) with increasing import intensity (log10x + 1 of the numbers of fish imported for the years 2000–2004) and with increasing consignment diversity (log10x + 1 of the numbers of consignment types imported for the years 2000–2004). The implications for policy and management are discussed.",what Investigated species ?,Fishes,fishes,True,True
"Alien plants invade many ecosystems worldwide and often have substantial negative effects on ecosystem structure and functioning. Our ability to quantitatively predict these impacts is, in part, limited by the absence of suitable plant-spread models and by inadequate parameter estimates for such models. This paper explores the effects of model, plant, and environmental attributes on predicted rates and patterns of spread of alien pine trees (Pinus spp.) in South African fynbos (a mediterranean-type shrubland). A factorial experimental design was used to: (1) compare the predictions of a simple reaction-diffusion model and a spatially explicit, individual-based simulation model; (2) investigate the sensitivity of predicted rates and patterns of spread to parameter values; and (3) quantify the effects of the simulation model's spatial grain on its predictions. The results show that the spatial simulation model places greater emphasis on interactions among ecological processes than does the reaction-diffusion model. This ensures that the predictions of the two models differ substantially for some factor combinations. The most important factor in the model is dispersal ability. Fire frequency, fecundity, and age of reproductive maturity are less important, while adult mortality has little effect on the model's predictions. The simulation model's predictions are sensitive to the model's spatial grain. This suggests that simulation models that use matrices as a spatial framework should ensure that the spatial grain of the model is compatible with the spatial processes being modeled. We conclude that parameter estimation and model development must be integrated pro- cedures. This will ensure that the model's structure is compatible with the biological pro- cesses being modeled. Failure to do so may result in spurious predictions.",what Investigated species ?,Plants,plants,True,True
"Horticulture is an important source of naturalized plants, but our knowledge about naturalization frequencies and potential patterns of naturalization in horticultural plants is limited. We analyzed a unique set of data derived from the detailed sales catalogs (1887-1930) of the most important early Florida, USA, plant nursery (Royal Palm Nursery) to detect naturalization patterns of these horticultural plants in the state. Of the 1903 nonnative species sold by the nursery, 15% naturalized. The probability of plants becoming naturalized increases significantly with the number of years the plants were marketed. Plants that became invasive and naturalized were sold for an average of 19.6 and 14.8 years, respectively, compared to 6.8 years for non-naturalized plants, and the naturalization of plants sold for 30 years or more is 70%. Unexpectedly, plants that were sold earlier were less likely to naturalize than those sold later. The nursery's inexperience, which caused them to grow and market many plants unsuited to Florida during their early period, may account for this pattern. Plants with pantropical distributions and those native to both Africa and Asia were more likely to naturalize (42%), than were plants native to other smaller regions, suggesting that plants with large native ranges were more likely to naturalize. Naturalization percentages also differed according to plant life form, with the most naturalization occurring in aquatic herbs (36.8%) and vines (30.8%). Plants belonging to the families Araceae, Apocynaceae, Convolvulaceae, Moraceae, Oleaceae, and Verbenaceae had higher than expected naturalization. Information theoretic model selection indicated that the number of years a plant was sold, alone or together with the first year a plant was sold, was the strongest predictor of naturalization. Because continued importation and marketing of nonnative horticultural plants will lead to additional plant naturalization and invasion, a comprehensive approach to address this problem, including research to identifyand select noninvasive forms and types of horticultural plants is urgently needed.",what Investigated species ?,Plants,,False,False
"Ecological filters and availability of propagules play key roles structuring natural communities. Propagule pressure has recently been suggested to be a fundamental factor explaining the success or failure of biological introductions. We tested this hypothesis with a remarkable data set on trees introduced to Isla Victoria, Nahuel Huapi National Park, Argentina. More than 130 species of woody plants, many known to be highly invasive elsewhere, were introduced to this island early in the 20th century, as part of an experiment to test their suitability as commercial forestry trees for this region. We obtained detailed data on three estimates of propagule pressure (number of introduced individuals, number of areas where introduced, and number of years during which the species was planted) for 18 exotic woody species. We matched these data with a survey of the species and number of individuals currently invading the island. None of the three estimates of propagule pressure predicted the current pattern of invasion. We suggest that other factors, such as biotic resistance, may be operating to determine the observed pattern of invasion, and that propagule pressure may play a relatively minor role in explaining at least some observed patterns of invasion success and failure.",what Investigated species ?,Plants,"plants,",True,True
"Island communities are generally viewed as being more susceptible to invasion than those of mainland areas, yet empirical evidence is almost lacking. A species-by-species examination of introduced birds in two independent island-mainland comparisons is not consistent with this hypothesis. In the New Zealand-mainland Australia comparison, 16 species were successful in both regions, 19 always failed and only eight had mixed outcomes. Mixed results were observed less often than expected by chance, and in only 5 cases was the relationship in the predicted direction. This result is not biased by differences in introduction effort because, within species, the number of individuals released in New Zealand did not differ significantly from those released in mainland Australia. A similar result emerged in the Hawaiian islands-mainland USA comparison: among the 35 species considered, 15 were successful in both regions, seven always failed and 13 had mixed outcomes. In this occasion, the results fit well to those expected by chance, and in only seven cases was the relationship in the direction predicted. I therefore conclude that, if true, the view that islands are less resistant than continents to invasions is far from universal.",what Investigated species ?,Birds,birds,True,True
"1 Alien species may form plant–animal mutualistic complexes that contribute to their invasive potential. Using multivariate techniques, we examined the structure of a plant–pollinator web comprising both alien and native plants and flower visitors in the temperate forests of north‐west Patagonia, Argentina. Our main objective was to assess whether plant species origin (alien or native) influences the composition of flower visitor assemblages. We also examined the influence of other potential confounding intrinsic factors such as flower symmetry and colour, and extrinsic factors such as flowering time, site and habitat disturbance. 2 Flowers of alien and native plant species were visited by a similar number of species and proportion of insects from different orders, but the composition of the assemblages of flower‐visiting species differed between alien and native plants. 3 The influence of plant species origin on the composition of flower visitor assemblages persisted after accounting for other significant factors such as flowering time, bearing red corollas, and habitat disturbance. This influence was at least in part determined by the fact that alien flower visitors were more closely associated with alien plants than with native plants. The main native flower visitors were, on average, equally associated with native and alien plant species. 4 In spite of representing a minor fraction of total species richness (3.6% of all species), alien flower visitors accounted for > 20% of all individuals recorded on flowers. Thus, their high abundance could have a significant impact in terms of pollination. 5 The mutualistic web of alien plants and flower‐visiting insects is well integrated into the overall community‐wide pollination web. However, in addition to their use of the native biota, invasive plants and flower visitors may benefit from differential interactions with their alien partners. The existence of these invader complexes could contribute to the spread of aliens into novel environments.",what Ecological Level of evidence ?,Individual,symmetry,False,False
"A Ponto-Caspian amphipod Dikerogammarus haemobaphes has recently invaded European waters. In the recipient area, it encountered Dreissena polymorpha, a habitat-forming bivalve, co-occurring with the gammarids in their native range. We assumed that interspecific interactions between these two species, which could develop during their long-term co-evolution, may affect the gammarid behaviour in novel areas. We examined the gammarid ability to select a habitat containing living mussels and searched for cues used in that selection. We hypothesized that they may respond to such traits of a living mussel as byssal threads, activity (e.g. valve movements, filtration) and/or shell surface properties. We conducted the pairwise habitat-choice experiments in which we offered various objects to single gammarids in the following combinations: (1) living mussels versus empty shells (the general effect of living Dreissena); (2) living mussels versus shells with added byssal threads and shells with byssus versus shells without it (the effect of byssus); (3) living mussels versus shells, both coated with nail varnish to neutralize the shell surface (the effect of mussel activity); (4) varnished versus clean living mussels (the effect of shell surface); (5) varnished versus clean stones (the effect of varnish). We checked the gammarid positions in the experimental tanks after 24 h. The gammarids preferred clean living mussels over clean shells, regardless of the presence of byssal threads under the latter. They responded to the shell surface, exhibiting preferences for clean mussels over varnished individuals. They were neither affected by the presence of byssus nor by mussel activity. The ability to detect and actively select zebra mussel habitats may be beneficial for D. haemobaphes and help it establish stable populations in newly invaded areas.",what Ecological Level of evidence ?,Individual,stable,False,False
"Summary Introduction of an exotic species has the potential to alter interactions between fish and bivalves; yet our knowledge in this field is limited, not least by lack of studies involving fish early life stages (ELS). Here, for the first time, we examine glochidial infection of fish ELS by native and exotic bivalves in a system recently colonised by two exotic gobiid species (round goby Neogobius melanostomus, tubenose goby Proterorhinus semilunaris) and the exotic Chinese pond mussel Anodonta woodiana. The ELS of native fish were only rarely infected by native glochidia. By contrast, exotic fish displayed significantly higher native glochidia prevalence and mean intensity of infection than native fish (17 versus 2% and 3.3 versus 1.4 respectively), inferring potential for a parasite spillback/dilution effect. Exotic fish also displayed a higher parasitic load for exotic glochidia, inferring potential for invasional meltdown. Compared to native fish, presence of gobiids increased the total number of glochidia transported downstream on drifting fish by approximately 900%. We show that gobiid ELS are a novel, numerous and ‘attractive’ resource for unionid glochidia. As such, unionids could negatively affect gobiid recruitment through infection-related mortality of gobiid ELS and/or reinforce downstream unionid populations through transport on drifting gobiid ELS. These implications go beyond what is suggested in studies of older life stages, thereby stressing the importance of an holistic ontogenetic approach in ecological studies.",what Ecological Level of evidence ?,Population,,False,False
"ABSTRACT Questions: 1. Is there any post-dispersal positive effect of the exotic shrub Pyracantha angustifolia on the success of Ligustrum lucidum seedlings, as compared to the effect of the native Condalia montana or the open herbaceous patches between shrubs? 2. Is the possible facilitation by Pyracantha and/or Condalia related to differential emergence, growth, or survival of Ligustrum seedlings under their canopies? Location: Cordoba, central Argentina. Methods: We designed three treatments, in which ten mature individuals of Pyracantha, ten of the dominant native shrub Condalia montana, and ten patches without shrub cover were involved. In each treatment we planted seeds and saplings of Ligustrum collected from nearby natural populations. Seedlings emerging from the planted seeds were harvested after one year to measure growth. Survival of the transplanted saplings was recorded every two month during a year. Half of the planted seeds and transplanted saplings were cage-protected from rodents. Results...",what Ecological Level of evidence ?,Individual,positive,False,False
"Abstract The impact of human‐induced stressors, such as invasive species, is often measured at the organismal level, but is much less commonly scaled up to the population level. Interactions with invasive species represent an increasingly common source of stressor in many habitats. However, due to the increasing abundance of invasive species around the globe, invasive species now commonly cause stresses not only for native species in invaded areas, but also for other invasive species. I examine the European green crab Carcinus maenas, an invasive species along the northeast coast of North America, which is known to be negatively impacted in this invaded region by interactions with the invasive Asian shore crab Hemigrapsus sanguineus. Asian shore crabs are known to negatively impact green crabs via two mechanisms: by directly preying on green crab juveniles and by indirectly reducing green crab fecundity via interference (and potentially exploitative) competition that alters green crab diets. I used life‐table analyses to scale these two mechanistic stressors up to the population level in order to examine their relative impacts on green crab populations. I demonstrate that lost fecundity has larger impacts on per capita population growth rates, but that both predation and lost fecundity are capable of reducing population growth sufficiently to produce the declines in green crab populations that have been observed in areas where these two species overlap. By scaling up the impacts of one invader on a second invader, I have demonstrated that multiple documented interactions between these species are capable of having population‐level impacts and that both may be contributing to the decline of European green crabs in their invaded range on the east coast of North America.",what Ecological Level of evidence ?,Population,population,True,True
"Abstract: Recent experiments suggest that introduced, non-migratory Canada geese (Branta canadensis) may be facilitating the spread of exotic grasses and decline of native plant species abundance on small islets in the Georgia Basin, British Columbia, which otherwise harbour outstanding examples of threatened maritime meadow ecosystems. We examined this idea by testing if the presence of geese predicted the abundance of exotic grasses and native competitors at 2 spatial scales on 39 islands distributed throughout the Southern Gulf and San Juan Islands of Canada and the United States, respectively. At the plot level, we found significant positive relationships between the percent cover of goose feces and exotic annual grasses. However, this trend was absent at the scale of whole islands. Because rapid population expansion of introduced geese in the region only began in the 1980s, our results are consistent with the hypothesis that the deleterious effects of geese on the cover of exotic annual grasses have yet to proceed beyond the local scale, and that a window of opportunity now exists in which to implement management strategies to curtail this emerging threat to native ecosystems. Research is now needed to test if the removal of geese results in the decline of exotic annual grasses.",what Ecological Level of evidence ?,Population,2 spatial scales,False,False
"Plants introduced into a new range are expected to harbour fewer specialized herbivores and to receive less damage than conspecifics in native ranges. Datura stramonium was introduced in Spain about five centuries ago. Here, we compare damage by herbivores, plant size, and leaf trichomes between plants from non-native and native ranges and perform selection analyses. Non-native plants experienced much less damage, were larger and less pubescent than plants of native populations. While plant size was related to fitness in both ranges, selection to increase resistance was only detected in the native region. We suggest this is a consequence of a release from enemies in this new environment.",what Indicator for enemy release ?,Damage,"damage than conspecifics in native ranges. datura stramonium was introduced in spain about five centuries ago. here, we compare damage",False,True
"Grasslands have been lost and degraded in the United States since Euro-American settlement due to agriculture, development, introduced invasive species, and changes in fire regimes. Fire is frequently used in prairie restoration to control invasion by trees and shrubs, but may have additional consequences. For example, fire might reduce damage by herbivore and pathogen enemies by eliminating litter, which harbors eggs and spores. Less obviously, fire might influence enemy loads differently for native and introduced plant hosts. We used a controlled burn in a Willamette Valley (Oregon) prairie to examine these questions. We expected that, without fire, introduced host plants should have less damage than native host plants because the introduced species are likely to have left many of their enemies behind when they were transported to their new range (the enemy release hypothesis, or ERH). If the ERH holds, then fire, which should temporarily reduce enemies on all species, should give an advantage to the natives because they should see greater total reduction in damage by enemies. Prior to the burn, we censused herbivore and pathogen attack on eight plant species (five of nonnative origin: Bromus hordaceous, Cynosuros echinatus, Galium divaricatum, Schedonorus arundinaceus (= Festuca arundinacea), and Sherardia arvensis; and three natives: Danthonia californica, Epilobium minutum, and Lomatium nudicale). The same plots were monitored for two years post-fire. Prior to the burn, native plants had more kinds of damage and more pathogen damage than introduced plants, consistent with the ERH. Fire reduced pathogen damage relative to the controls more for the native than the introduced species, but the effects on herbivory were negligible. Pathogen attack was correlated with plant reproductive fitness, whereas herbivory was not. These results suggest that fire may be useful for promoting some native plants in prairies due to its negative effects on their pathogens.",what Indicator for enemy release ?,Damage,fire,False,False
"During the past centuries, humans have introduced many plant species in areas where they do not naturally occur. Some of these species establish populations and in some cases become invasive, causing economic and ecological damage. Which factors determine the success of non-native plants is still incompletely understood, but the absence of natural enemies in the invaded area (Enemy Release Hypothesis; ERH) is one of the most popular explanations. One of the predictions of the ERH, a reduced herbivore load on non-native plants compared with native ones, has been repeatedly tested. However, many studies have either used a community approach (sampling from native and non-native species in the same community) or a biogeographical approach (sampling from the same plant species in areas where it is native and where it is non-native). Either method can sometimes lead to inconclusive results. To resolve this, we here add to the small number of studies that combine both approaches. We do so in a single study of insect herbivory on 47 woody plant species (trees, shrubs, and vines) in the Netherlands and Japan. We find higher herbivore diversity, higher herbivore load and more herbivory on native plants than on non-native plants, generating support for the enemy release hypothesis.",what Indicator for enemy release ?,Damage,erh ),False,False
"ABSTRACT One explanation for the success of exotic plants in their introduced habitats is that, upon arriving to a new continent, plants escaped their native herbivores or pathogens, resulting in less damage and lower abundance of enemies than closely related native species (enemy release hypothesis). We tested whether the three exotic plant species, Rubus phoenicolasius (wineberry), Fallopia japonica (Japanese knotweed), and Persicaria perfoliata (mile-a-minute weed), suffered less herbivory or pathogen attack than native species by comparing leaf damage and invertebrate herbivore abundance and diversity on the invasive species and their native congeners. Fallopia japonica and R. phoenicolasius received less leaf damage than their native congeners, and F. japonica also contained a lower diversity and abundance of invertebrate herbivores. If the observed decrease in damage experienced by these two plant species contributes to increased fitness, then escape from enemies may provide at least a partial explanation for their invasiveness. However, P. perfoliata actually received greater leaf damage than its native congener. Rhinoncomimus latipes, a weevil previously introduced in the United States as a biological control for P. perfoliata, accounted for the greatest abundance of insects collected from P. perfoliata. Therefore, it is likely that the biocontrol R. latipes was responsible for the greater damage on P. perfoliata, suggesting this insect may be effective at controlling P. perfoliata populations if its growth and reproduction is affected by the increased herbivore damage.",what Indicator for enemy release ?,Damage,damage,True,True
"1 Acer platanoides (Norway maple) is an important non‐native invasive canopy tree in North American deciduous forests, where native species diversity and abundance are greatly reduced under its canopy. We conducted a field experiment in North American forests to compare planted seedlings of A. platanoides and Acer saccharum (sugar maple), a widespread, common native that, like A. platanoides, is shade tolerant. Over two growing seasons in three forests we compared multiple components of seedling success: damage from natural enemies, ecophysiology, growth and survival. We reasoned that equal or superior performance by A. platanoides relative to A. saccharum indicates seedling characteristics that support invasiveness, while inferior performance indicates potential barriers to invasion. 2 Acer platanoides seedlings produced more leaves and allocated more biomass to roots, A. saccharum had greater water use efficiency, and the two species exhibited similar photosynthesis and first‐season mortality rates. Acer platanoides had greater winter survival and earlier spring leaf emergence, but second‐season mortality rates were similar. 3 The success of A. platanoides seedlings was not due to escape from natural enemies, contrary to the enemy release hypothesis. Foliar insect herbivory and disease symptoms were similarly high for both native and non‐native, and seedling biomass did not differ. Rather, A. platanoides compared well with A. saccharum because of its equivalent ability to photosynthesize in the low light herb layer, its higher leaf production and greater allocation to roots, and its lower winter mortality coupled with earlier spring emergence. Its only potential barrier to seedling establishment, relative to A. saccharum, was lower water use efficiency, which possibly could hinder its invasion into drier forests. 4 The spread of non‐native canopy trees poses an especially serious problem for native forest communities, because canopy trees strongly influence species in all forest layers. Success at reaching the canopy depends on a tree's ecology in previous life‐history stages, particularly as a vulnerable seedling, but little is known about seedling characteristics that promote non‐native tree invasion. Experimental field comparison with ecologically successful native trees provides insight into why non‐native trees succeed as seedlings, which is a necessary stage on their journey into the forest canopy.",what Indicator for enemy release ?,Damage,water,False,False
"Abstract Enemy release is a commonly accepted mechanism to explain plant invasions. Both the diploid Leucanthemum vulgare and the morphologically very similar tetraploid Leucanthemum ircutianum have been introduced into North America. To verify which species is more prevalent in North America we sampled 98 Leucanthemum populations and determined their ploidy level. Although polyploidy has repeatedly been proposed to be associated with increased invasiveness in plants, only two of the populations surveyed in North America were the tetraploid L. ircutianum . We tested the enemy release hypothesis by first comparing 20 populations of L. vulgare and 27 populations of L. ircutianum in their native range in Europe, and then comparing the European L. vulgare populations with 31 L. vulgare populations sampled in North America. Characteristics of the site and associated vegetation, plant performance and invertebrate herbivory were recorded. In Europe, plant height and density of the two species were similar but L. vulgare produced more flower heads than L. ircutianum . Leucanthemum vulgare in North America was 17 % taller, produced twice as many flower heads and grew much denser compared to L. vulgare in Europe. Attack rates by root- and leaf-feeding herbivores on L. vulgare in Europe (34 and 75 %) was comparable to that on L. ircutianum (26 and 71 %) but higher than that on L. vulgare in North America (10 and 3 %). However, herbivore load and leaf damage were low in Europe. Cover and height of the co-occurring vegetation was higher in L. vulgare populations in the native than in the introduced range, suggesting that a shift in plant competition may more easily explain the invasion success of L. vulgare than escape from herbivory.",what Indicator for enemy release ?,Damage,"enemy release? [sep] abstract enemy release is a commonly accepted mechanism to explain plant invasions. both the diploid leucanthemum vulgare and the morphologically very similar tetraploid leucanthemum ircutianum have been introduced into north america. to verify which species is more prevalent in north america we sampled 98 leucanthemum populations and determined their ploidy level. although polyploidy has repeatedly been proposed to be associated with increased invasiveness in plants,",False,False
"The vegetation of Kings Park, near the centre of Perth, Western Australia, once had an overstorey of Eucalyptus marginata (jarrah) or Eucalyptus gomphocephala (tuart), and many trees still remain in the bushland parts of the Park. Avenues and roadsides have been planted with eastern Australian species, including Eucalyptus cladocalyx (sugar gum) and Eucalyptus botryoides (southern mahogany), both of which have become invasive. The present study examined the effect of a recent burn on the level of herbivory on these native and exotic eucalypts. Leaf damage, shoot extension and number of new leaves were measured on tagged shoots of saplings of each tree species in unburnt and burnt areas over an 8-month period. Leaf macronutrient levels were quantified and the number of arthropods on saplings was measured at the end of the recording period by chemical knockdown. Leaf macronutrients were mostly higher in all four species in the burnt area, and this was associated with generally higher numbers of canopy arthropods and greater levels of leaf damage. It is suggested that the pulse of soil nutrients after the fire resulted in more nutrient-rich foliage, which in turn was more palatable to arthropods. The resulting high levels of herbivory possibly led to reduced shoot extension of E. gomphocephala, E. botryoides and, to a lesser extent, E. cladocalyx. This acts as a negative feedback mechanism that lessens the tendency for lush, post-fire regrowth to outcompete other species of plants. There was no consistent difference in the levels of the various types of leaf damage or of arthropods on the native and the exotic eucalypts, suggesting that freedom from herbivory is not contributing to the invasiveness of the two exotic species.",what Indicator for enemy release ?,Damage,burn,False,False
"Several hypotheses proposed to explain the success of introduced species focus on altered interspecific interactions. One of the most prominent, the Enemy Release Hypothesis, posits that invading species benefit compared to their native counterparts if they lose their herbivores and pathogens during the invasion process. We previously reported on a common garden experiment (from 2002) in which we compared levels of herbivory between 30 taxonomically paired native and introduced old-field plants. In this phyloge- netically controlled comparison, herbivore damage tended to be higher on introduced than on native plants. This striking pattern, the opposite of current theory, prompted us to further investigate herbivory and several other interspecific interactions in a series of linked ex- periments with the same set of species. Here we show that, in these new experiments, introduced plants, on average, received less insect herbivory and were subject to half the negative soil microbial feedback compared to natives; attack by fungal and viral pathogens also tended to be reduced on introduced plants compared to natives. Although plant traits (foliar C:N, toughness, and water content) suggested that introduced species should be less resistant to generalist consumers, they were not consistently more heavily attacked. Finally, we used meta-analysis to combine data from this study with results from our previous work to show that escape generally was inconsistent among guilds of enemies: there were few instances in which escape from multiple guilds occurred for a taxonomic pair, and more cases in which the patterns of escape from different enemies canceled out. Our examination of multiple interspecific interactions demonstrates that escape from one guild of enemies does not necessarily imply escape from other guilds. Because the effects of each guild are likely to vary through space and time, the net effect of all enemies is also likely to be variable. The net effect of these interactions may create ''invasion opportunity windows'': times when introduced species make advances in native communities.",what Indicator for enemy release ?,Damage,plants.,False,False
"To quantify the relative importance of propagule pressure, climate‐matching and host availability for the invasion of agricultural pest arthropods in Europe and to forecast newly emerging pest species and European areas with the highest risk of arthropod invasion under current climate and a future climate scenario (A1F1).",what hypothesis ?,Propagule pressure,"propagule pressure,",True,True
"In their colonized ranges, exotic plants may be released from some of the herbivores or pathogens of their home ranges but these can be replaced by novel enemies. It is of basic and practical interest to understand which characteristics of invaded communities control accumulation of the new pests. Key questions are whether enemy load on exotic species is smaller than on native competitors as suggested by the enemy release hypothesis (ERH) and whether this difference is most pronounced in resource‐rich habitats as predicted by the resource–enemy release hypothesis (R‐ERH). In 72 populations of 12 exotic invasive species, we scored all visible above‐ground damage morphotypes caused by herbivores and fungal pathogens. In addition, we quantified levels of leaf herbivory and fruit damage. We then assessed whether variation in damage diversity and levels was explained by habitat fertility, by relatedness between exotic species and the native community or rather by native species diversity. In a second part of the study, we also tested the ERH and the R‐ERH by comparing damage of plants in 28 pairs of co‐occurring native and exotic populations, representing nine congeneric pairs of native and exotic species. In the first part of the study, diversity of damage morphotypes and damage levels of exotic populations were greater in resource‐rich habitats. Co‐occurrence of closely related, native species in the community significantly increased the probability of fruit damage. Herbivory on exotics was less likely in communities with high phylogenetic diversity. In the second part of the study, exotic and native congeneric populations incurred similar damage diversity and levels, irrespective of whether they co‐occurred in nutrient‐poor or nutrient‐rich habitats. Synthesis. We identified habitat productivity as a major community factor affecting accumulation of enemy damage by exotic populations. Similar damage levels in exotic and native congeneric populations, even in species pairs from fertile habitats, suggest that the enemy release hypothesis or the R‐ERH cannot always explain the invasiveness of introduced species.",what hypothesis ?,Enemy release,enemy release hypothesis ( erh ),False,True
"Anthropogenic disturbance is considered a risk factor in the establishment of non‐indigenous species (NIS); however, few studies have investigated the role of anthropogenic disturbance in facilitating the establishment and spread of NIS in marine environments. A baseline survey of native and NIS was undertaken in conjunction with a manipulative experiment to determine the effect that heavy metal pollution had on the diversity and invasibility of marine hard‐substrate assemblages. The study was repeated at two sites in each of two harbours in New South Wales, Australia. The survey sampled a total of 47 sessile invertebrate taxa, of which 15 (32%) were identified as native, 19 (40%) as NIS, and 13 (28%) as cryptogenic. Increasing pollution exposure decreased native species diversity at all study sites by between 33% and 50%. In contrast, there was no significant change in the numbers of NIS. Percentage cover was used as a measure of spatial dominance, with increased pollution exposure leading to increased NIS dominance across all sites. At three of the four study sites, assemblages that had previously been dominated by natives changed to become either extensively dominated by NIS or equally occupied by native and NIS alike. No single native or NIS was repeatedly responsible for the observed changes in native species diversity or NIS dominance at all sites. Rather, the observed effects of pollution were driven by a diverse range of taxa and species. These findings have important implications for both the way we assess pollution impacts, and for the management of NIS. When monitoring the response of assemblages to pollution, it is not sufficient to simply assess changes in community diversity. Rather, it is important to distinguish native from NIS components since both are expected to respond differently. In order to successfully manage current NIS, we first need to address levels of pollution within recipient systems in an effort to bolster the resilience of native communities to invasion.",what hypothesis ?,Disturbance,anthropogenic disturbance,False,True
"The success of introduced species is frequently explained by their escape from natural enemies in the introduced region. We tested the enemy release hypothesis with respect to two well studied blood parasite genera (Plasmodium and Haemoproteus) in native and six introduced populations of the common myna Acridotheres tristis. Not all comparisons of introduced populations to the native population were consistent with expectations of the enemy release hypothesis. Native populations show greater overall parasite prevalence than introduced populations, but the lower prevalence in introduced populations is driven by low prevalence in two populations on oceanic islands (Fiji and Hawaii). When these are excluded, prevalence does not differ significantly. We found a similar number of parasite lineages in native populations compared to all introduced populations. Although there is some evidence that common mynas may have carried parasite lineages from native to introduced locations, and also that introduced populations may have become infected with novel parasite lineages, it may be difficult to differentiate between parasites that are native and introduced, because malarial parasite lineages often do not show regional or host specificity.",what hypothesis ?,Enemy release,enemy release,True,True
"Biological invasions are facilitated by the global transportation of species and climate change. Given that invasions may cause ecological and economic damage and pose a major threat to biodiversity, understanding the mechanisms behind invasion success is essential. Both the release of non-native populations from natural enemies, such as parasites, and the genetic diversity of these populations may play key roles in their invasion success. We investigated the roles of parasite communities, through enemy release and parasite acquisition, and genetic diversity in the invasion success of the non-native bumblebee, Bombus hypnorum, in the United Kingdom. The invasive B. hypnorum had higher parasite prevalence than most, or all native congeners for two high-impact parasites, probably due to higher susceptibility and parasite acquisition. Consequently parasites had a higher impact on B. hypnorum queens’ survival and colony-founding success than on native species. Bombus hypnorum also had lower functional genetic diversity at the sex-determining locus than native species. Higher parasite prevalence and lower genetic diversity have not prevented the rapid invasion of the United Kingdom by B. hypnorum. These data may inform our understanding of similar invasions by commercial bumblebees around the world. This study suggests that concerns about parasite impacts on the small founding populations common to re-introduction and translocation programs may be less important than currently believed.",what hypothesis ?,Enemy release,,False,False
"Studying plant invasions along environmental gradients is a promising approach to dissect the relative importance of multiple interacting factors that affect the spread of a species in a new range. Along altitudinal gradients, factors such as propagule pressure, climatic conditions and biotic interactions change simultaneously across rather small geographic scales. Here we investigate the distribution of eight Asteraceae forbs along mountain roads in both their native and introduced ranges in the Valais (southern Swiss Alps) and the Wallowa Mountains (northeastern Oregon, USA). We hypothesised that a lack of adaptation and more limiting propagule pressure at higher altitudes in the new range restricts the altitudinal distribution of aliens relative to the native range. However, all but one of the species reached the same or even a higher altitude in the new range. Thus neither the need to adapt to changing climatic conditions nor lower propagule pressure at higher altitudes appears to have prevented the altitudinal spread of introduced populations. We found clear differences between regions in the relative occurrence of alien species in ruderal sites compared to roadsides, and in the degree of invasion away from the roadside, presumably reflecting differences in disturbance patterns between regions. Whilst the upper altitudinal limits of these plant invasions are apparently climatically constrained, factors such as anthropogenic disturbance and competition with native vegetation appear to have greater influence than changing climatic conditions on the distribution of these alien species along altitudinal gradients.",what hypothesis ?,Disturbance,"propagule pressure,",False,False
"Species that are introduced to novel environments can lose their native pathogens and parasites during the process of introduction. The escape from the negative effects associated with these natural enemies is commonly employed as an explanation for the success and expansion of invasive species, which is termed the enemy release hypothesis (ERH). In this study, nested PCR techniques and microscopy were used to determine the prevalence and intensity (respectively) of Plasmodium spp. and Haemoproteus spp. in introduced house sparrows and native urban birds of central Brazil. Generalized linear mixed models were fitted by Laplace approximation considering a binomial error distribution and logit link function. Location and species were considered as random effects and species categorization (native or non-indigenous) as fixed effects. We found that native birds from Brazil presented significantly higher parasite prevalence in accordance with the ERH. We also compared our data with the literature, and found that house sparrows native to Europe exhibited significantly higher parasite prevalence than introduced house sparrows from Brazil, which also supports the ERH. Therefore, it is possible that house sparrows from Brazil might have experienced a parasitic release during the process of introduction, which might also be related to a demographic release (e.g. release from the negative effects of parasites on host population dynamics).",what hypothesis ?,Enemy release,enemy release,True,True
"The enemy release hypothesis (ERH) is often cited to explain why some plants successfully invade natural communities while others do not. This hypothesis maintains that plant populations are regulated by coevolved enemies in their native range but are relieved of this pressure where their enemies have not been co-introduced. Some studies have shown that invasive plants sustain lower levels of herbivore damage when compared to native species, but how damage affects fitness and population dynamics remains unclear. We used a system of co-occurring native and invasive Eugenia congeners in south Florida (USA) to experimentally test the ERH, addressing deficiencies in our understanding of the role of natural enemies in plant invasion at the population level. Insecticide was used to experimentally exclude insect herbivores from invasive Eugenia uniflora and its native co-occurring congeners in the field for two years. Herbivore damage, plant growth, survival, and population growth rates for the three species were then compared for control and insecticide-treated plants. Our results contradict the ERH, indicating that E. uniflora sustains more herbivore damage than its native congeners and that this damage negatively impacts stem height, survival, and population growth. In addition, most damage to E. uniflora, a native of Brazil, is carried out by Myllocerus undatus, a recently introduced weevil from Sri Lanka, and M. undatus attacks a significantly greater proportion of E. uniflora leaves than those of its native congeners. This interaction is particularly interesting because M. undatus and E. uniflora share no coevolutionary history, having arisen on two separate continents and come into contact on a third. Our study is the first to document negative population-level effects for an invasive plant as a result of the introduction of a novel herbivore. Such inhibitory interactions are likely to become more prevalent as suites of previously noninteracting species continue to accumulate and new communities assemble worldwide.",what hypothesis ?,Enemy release,enemy release,True,True
"Escape from natural enemies is a widely held generalization for the success of exotic plants. We conducted a large-scale experiment in Hawaii (USA) to quantify impacts of ungulate removal on plant growth and performance, and to test whether elimination of an exotic generalist herbivore facilitated exotic success. Assessment of impacted and control sites before and after ungulate exclusion using airborne imaging spectroscopy and LiDAR, time series satellite observations, and ground-based field studies over nine years indicated that removal of generalist herbivores facilitated exotic success, but the abundance of native species was unchanged. Vegetation cover <1 m in height increased in ungulate-free areas from 48.7% +/- 1.5% to 74.3% +/- 1.8% over 8.4 years, corresponding to an annualized growth rate of lambda = 1.05 +/- 0.01 yr(-1) (median +/- SD). Most of the change was attributable to exotic plant species, which increased from 24.4% +/- 1.4% to 49.1% +/- 2.0%, (lambda = 1.08 +/- 0.01 yr(-1)). Native plants experienced no significant change in cover (23.0% +/- 1.3% to 24.2% +/- 1.8%, lambda = 1.01 +/- 0.01 yr(-1)). Time series of satellite phenology were indistinguishable between the treatment and a 3.0-km2 control site for four years prior to ungulate removal, but they diverged immediately following exclusion of ungulates. Comparison of monthly EVI means before and after ungulate exclusion and between the managed and control areas indicates that EVI strongly increased in the managed area after ungulate exclusion. Field studies and airborne analyses show that the dominant invader was Senecio madagascariensis, an invasive annual forb that increased from < 0.01% to 14.7% fractional cover in ungulate-free areas (lambda = 1.89 +/- 0.34 yr(-1)), but which was nearly absent from the control site. A combination of canopy LAI, water, and fractional cover were expressed in satellite EVI time series and indicate that the invaded region maintained greenness during drought conditions. These findings demonstrate that enemy release from generalist herbivores can facilitate exotic success and suggest a plausible mechanism by which invasion occurred. They also show how novel remote-sensing technology can be integrated with conservation and management to help address exotic plant invasions.",what hypothesis ?,Enemy release,ungulate removal,False,False
"1 Assembly rules are broadly defined as any filter imposed on a regional species pool that acts to determine the local community structure and composition. Environmental filtering is thought to result in the formation of groups of species with similar traits that tend to co‐occur more often than expected by chance alone, known as Beta guilds. At a smaller scale, within a single Beta guild, species may be partitioned into Alpha guilds – groups of species that have similar resource use and hence should tend not to co‐occur at small scales due the principle of limiting similarity. 2 This research investigates the effects of successional age and the presence of an invasive exotic species on Alpha and Beta guild structuring within plant communities along two successional river terrace sequences in the Waimakariri braided river system in New Zealand. 3 Fifteen sites were sampled, six with and nine without the Russel lupin (Lupinus polyphyllus), an invasive exotic species. At each site, species presence/absence was recorded in 100 circular quadrats (5 cm in diameter) at 30‐cm intervals along a 30‐m transect. Guild proportionality (Alpha guild structuring) was tested for using two a priori guild classifications each containing three guilds, and cluster analysis was used to test for environmental structuring between sites. 4 Significant assembly rules based on Alpha guild structuring were found, particularly for the monocot and dicot guild. Guild proportionality increased with increasing ecological age, which indicated an increase in the relative importance of competitive structuring at later stages of succession. This provides empirical support for Weiher and Keddy's theoretical model of community assembly. 5 Lupins were associated with altered Alpha and Beta guild structuring at early mid successional sites. Lupin‐containing sites had higher silt content than sites without lupins, and this could have altered the strength and scale of competitive structuring within the communities present. 6 This research adds to the increasing evidence for the existence of assembly rules based on limiting similarity within plant communities, and demonstrates the need to incorporate gradients of environmental and competitive adversity when investigating the rules that govern community assembly.",what hypothesis ?,limiting similarity,,False,False
"BACKGROUND AND AIMS The successful spread of invasive plants in new environments is often linked to multiple introductions and a diverse gene pool that facilitates local adaptation to variable environmental conditions. For clonal plants, however, phenotypic plasticity may be equally important. Here the primary adaptive strategy in three non-native, clonally reproducing macrophytes (Egeria densa, Elodea canadensis and Lagarosiphon major) in New Zealand freshwaters were examined and an attempt was made to link observed differences in plant morphology to local variation in habitat conditions. METHODS Field populations with a large phenotypic variety were sampled in a range of lakes and streams with different chemical and physical properties. The phenotypic plasticity of the species before and after cultivation was studied in a common garden growth experiment, and the genetic diversity of these same populations was also quantified. KEY RESULTS For all three species, greater variation in plant characteristics was found before they were grown in standardized conditions. Moreover, field populations displayed remarkably little genetic variation and there was little interaction between habitat conditions and plant morphological characteristics. CONCLUSIONS The results indicate that at the current stage of spread into New Zealand, the primary adaptive strategy of these three invasive macrophytes is phenotypic plasticity. However, while limited, the possibility that genetic diversity between populations may facilitate ecotypic differentiation in the future cannot be excluded. These results thus indicate that invasive clonal aquatic plants adapt to new introduced areas by phenotypic plasticity. Inorganic carbon, nitrogen and phosphorous were important in controlling plant size of E. canadensis and L. major, but no other relationships between plant characteristics and habitat conditions were apparent. This implies that within-species differences in plant size can be explained by local nutrient conditions. All together this strongly suggests that invasive clonal aquatic plants adapt to a wide range of habitats in introduced areas by phenotypic plasticity rather than local adaptation.",what hypothesis ?,Phenotypic plasticity,phenotypic plasticity,True,True
"To understand the role of leaf-level plasticity and variability in species invasiveness, foliar characteristics were studied in relation to seasonal average integrated quantum flux density (Qint) in the understorey evergreen species Rhododendron ponticum and Ilex aquifolium at two sites. A native relict population of R. ponticum was sampled in southern Spain (Mediterranean climate), while an invasive alien population was investigated in Belgium (temperate maritime climate). Ilex aquifolium was native at both sites. Both species exhibited a significant plastic response to Qint in leaf dry mass per unit area, thickness, photosynthetic potentials, and chlorophyll contents at the two sites. However, R. ponticum exhibited a higher photosynthetic nitrogen use efficiency and larger investment of nitrogen in chlorophyll than I. aquifolium. Since leaf nitrogen (N) contents per unit dry mass were lower in R. ponticum, this species formed a larger foliar area with equal photosynthetic potential and light-harvesting efficiency compared with I. aquifolium. The foliage of R. ponticum was mechanically more resistant with larger density in the Belgian site than in the Spanish site. Mean leaf-level phenotypic plasticity was larger in the Belgian population of R. ponticum than in the Spanish population of this species and the two populations of I. aquifolium. We suggest that large fractional investments of foliar N in photosynthetic function coupled with a relatively large mean, leaf-level phenotypic plasticity may provide the primary explanation for the invasive nature and superior performance of R. ponticum at the Belgian site. With alleviation of water limitations from Mediterranean to temperate maritime climates, the invasiveness of R. ponticum may also be enhanced by the increased foliage mechanical resistance observed in the alien populations.",what hypothesis ?,Phenotypic plasticity,phenotypic plasticity,True,True
"In the Bonin Islands of the western Pacific where the light environment is characterized by high fluctuations due to frequent typhoon disturbance, we hypothesized that the invasive success of Bischofia javanica Blume (invasive tree, mid-successional) may be attributable to a high acclimation capacity under fluctuating light availability. The physiological and morphological responses of B. javanica to both simulated canopy opening and closure were compared against three native species of different successional status: Trema orientalis Blume (pioneer), Schima mertensiana (Sieb. et Zucc.) Koidz (mid-successional) and Elaeocarpus photiniaefolius Hook.et Arn (late-successional). The results revealed significant species-specific differences in the timing of physiological maturity and phenotypic plasticity in leaves developed under constant high and low light levels. For example, the photosynthetic capacity of T. orientalis reached a maximum in leaves that had just fully expanded when grown under constant high light (50% of full sun) whereas that of E. photiniaefolius leaves continued to increase until 50 d after full expansion. For leaves that had just reached full expansion, T. orientalis, having high photosynthetic plasticity between high and low light, exhibited low acclimation capacity under the changing light (from high to low or low to high light). In comparison with native species, B. javanica showed a higher degree of physiological and morphological acclimation following transfer to a new light condition in leaves of all age classes (i.e. before and after reaching full expansion). The high acclimation ability of B. javanica in response to changes in light availability may be a part of its pre-adaptations for invasiveness in the fluctuating environment of the Bonin Islands.",what hypothesis ?,Phenotypic plasticity,phenotypic plasticity,True,True
"The enemy release hypothesis predicts that invasive species will receive less damage from enemies, compared to co-occurring native and noninvasive exotic species in their introduced range. However, release operating early in invasion could be lost over time and with increased range size as introduced species acquire new enemies. We used three years of data, from 61 plant species planted into common gardens, to determine whether (1) invasive, noninvasive exotic, and native species experience differential damage from insect herbivores. and mammalian browsers, and (2) enemy release is lost with increased residence time and geographic spread in the introduced range. We find no evidence suggesting enemy release is a general mechanism contributing to invasiveness in this region. Invasive species received the most insect herbivory, and damage increased with longer residence times and larger range sizes at three spatial scales. Our results show that invasive and exotic species fail to escape enemies, particularly over longer temporal and larger spatial scales.",what hypothesis ?,Enemy release,enemy release,True,True
"Invasiveness may result from genetic variation and adaptation or phenotypic plasticity, and genetic variation in fitness traits may be especially critical. Pennisetum setaceum (fountain grass, Poaceae) is highly invasive in Hawaii (HI), moderately invasive in Arizona (AZ), and less invasive in southern California (CA). In common garden experiments, we examined the relative importance of quantitative trait variation, precipitation, and phenotypic plasticity in invasiveness. In two very different environments, plants showed no differences by state of origin (HI, CA, AZ) in aboveground biomass, seeds/flower, and total seed number. Plants from different states were also similar within watering treatment. Plants with supplemental watering, relative to unwatered plants, had greater biomass, specific leaf area (SLA), and total seed number, but did not differ in seeds/flower. Progeny grown from seeds produced under different watering treatments showed no maternal effects in seed mass, germination, biomass or SLA. High phenotypic plasticity, rather than local adaptation is likely responsible for variation in invasiveness. Global change models indicate that temperature and precipitation patterns over the next several decades will change, although the direction of change is uncertain. Drier summers in southern California may retard further invasion, while wetter summers may favor the spread of fountain grass.",what hypothesis ?,Phenotypic plasticity,"phenotypic plasticity,",True,True
"Ecological filters and availability of propagules play key roles structuring natural communities. Propagule pressure has recently been suggested to be a fundamental factor explaining the success or failure of biological introductions. We tested this hypothesis with a remarkable data set on trees introduced to Isla Victoria, Nahuel Huapi National Park, Argentina. More than 130 species of woody plants, many known to be highly invasive elsewhere, were introduced to this island early in the 20th century, as part of an experiment to test their suitability as commercial forestry trees for this region. We obtained detailed data on three estimates of propagule pressure (number of introduced individuals, number of areas where introduced, and number of years during which the species was planted) for 18 exotic woody species. We matched these data with a survey of the species and number of individuals currently invading the island. None of the three estimates of propagule pressure predicted the current pattern of invasion. We suggest that other factors, such as biotic resistance, may be operating to determine the observed pattern of invasion, and that propagule pressure may play a relatively minor role in explaining at least some observed patterns of invasion success and failure.",what hypothesis ?,Propagule pressure,propagule pressure,True,True
"The role of novel ecological interactions between mammals, fungi and plants in invaded ecosystems remains unresolved, but may play a key role in the widespread successful invasion of pines and their ectomycorrhizal fungal associates, even where mammal faunas originate from different continents to trees and fungi as in New Zealand. We examine the role of novel mammal associations in dispersal of ectomycorrhizal fungal inoculum of North American pines (Pinus contorta, Pseudotsuga menziesii), and native beech trees (Lophozonia menziesii) using faecal analyses, video monitoring and a bioassay experiment. Both European red deer (Cervus elaphus) and Australian brushtail possum (Trichosurus vulpecula) pellets contained spores and DNA from a range of native and non‐native ectomycorrhizal fungi. Faecal pellets from both animals resulted in ectomycorrhizal infection of pine seedlings with fungal genera Rhizopogon and Suillus, but not with native fungi or the invasive fungus Amanita muscaria, despite video and DNA evidence of consumption of these fungi. Native L. menziesii seedlings never developed any ectomycorrhizal infection from faecal pellet inoculation. Synthesis. Our results show that introduced mammals from Australia and Europe facilitate the co‐invasion of invasive North American trees and Northern Hemisphere fungi in New Zealand, while we find no evidence that introduced mammals benefit native trees or fungi. This novel tripartite ‘invasional meltdown’, comprising taxa from three kingdoms and three continents, highlights unforeseen consequences of global biotic homogenization.",what hypothesis ?,Invasional meltdown,invasional meltdown ’,False,True
"We report the impact of an extreme weather event, the October 1987 severe storm, on fragmented woodlands in southern Britain. We analysed ecological changes between 1971 and 2002 in 143 200‐m2 plots in 10 woodland sites exposed to the storm with an ecologically equivalent sample of 150 plots in 16 non‐exposed sites. Comparing both years, understorey plant species‐richness, species composition, soil pH and woody basal area of the tree and shrub canopy were measured. We tested the hypothesis that the storm had deflected sites from the wider national trajectory of an increase in woody basal area and reduced understorey species‐richness associated with ageing canopies and declining woodland management. We also expected storm disturbance to amplify the background trend of increasing soil pH, a UK‐wide response to reduced atmospheric sulphur deposition. Path analysis was used to quantify indirect effects of storm exposure on understorey species richness via changes in woody basal area and soil pH. By 2002, storm exposure was estimated to have increased mean species richness per 200 m2 by 32%. Woody basal area changes were highly variable and did not significantly differ with storm exposure. Increasing soil pH was associated with a 7% increase in richness. There was no evidence that soil pH increased more as a function of storm exposure. Changes in species richness and basal area were negatively correlated: a 3.4% decrease in richness occurred for every 0.1‐m2 increase in woody basal area per plot. Despite all sites substantially exceeding the empirical critical load for nitrogen deposition, there was no evidence that in the 15 years since the storm, disturbance had triggered a eutrophication effect associated with dominance of gaps by nitrophilous species. Synthesis. Although the impacts of the 1987 storm were spatially variable in terms of impacts on woody basal area, the storm had a positive effect on understorey species richness. There was no evidence that disturbance had increased dominance of gaps by invasive species. This could change if recovery from acidification results in a soil pH regime associated with greater macronutrient availability.",what hypothesis ?,Disturbance,"storm,",False,False
"Abstract: The successful invasion of exotic plants is often attributed to the absence of coevolved enemies in the introduced range (i.e., the enemy release hypothesis). Nevertheless, several components of this hypothesis, including the role of generalist herbivores, remain relatively unexplored. We used repeated censuses of exclosures and paired controls to investigate the role of a generalist herbivore, white‐tailed deer (Odocoileus virginianus), in the invasion of 3 exotic plant species (Microstegium vimineum, Alliaria petiolata, and Berberis thunbergii) in eastern hemlock (Tsuga canadensis) forests in New Jersey and Pennsylvania (U.S.A.). This work was conducted in 10 eastern hemlock (T. canadensis) forests that spanned gradients in deer density and in the severity of canopy disturbance caused by an introduced insect pest, the hemlock woolly adelgid (Adelges tsugae). We used maximum likelihood estimation and information theoretics to quantify the strength of evidence for alternative models of the influence of deer density and its interaction with the severity of canopy disturbance on exotic plant abundance. Our results were consistent with the enemy release hypothesis in that exotic plants gained a competitive advantage in the presence of generalist herbivores in the introduced range. The abundance of all 3 exotic plants increased significantly more in the control plots than in the paired exclosures. For all species, the inclusion of canopy disturbance parameters resulted in models with substantially greater support than the deer density only models. Our results suggest that white‐tailed deer herbivory can accelerate the invasion of exotic plants and that canopy disturbance can interact with herbivory to magnify the impact. In addition, our results provide compelling evidence of nonlinear relationships between deer density and the impact of herbivory on exotic species abundance. These findings highlight the important role of herbivore density in determining impacts on plant abundance and provide evidence of the operation of multiple mechanisms in exotic plant invasion.",what hypothesis ?,Enemy release,enemy release,True,True
"During the past centuries, humans have introduced many plant species in areas where they do not naturally occur. Some of these species establish populations and in some cases become invasive, causing economic and ecological damage. Which factors determine the success of non-native plants is still incompletely understood, but the absence of natural enemies in the invaded area (Enemy Release Hypothesis; ERH) is one of the most popular explanations. One of the predictions of the ERH, a reduced herbivore load on non-native plants compared with native ones, has been repeatedly tested. However, many studies have either used a community approach (sampling from native and non-native species in the same community) or a biogeographical approach (sampling from the same plant species in areas where it is native and where it is non-native). Either method can sometimes lead to inconclusive results. To resolve this, we here add to the small number of studies that combine both approaches. We do so in a single study of insect herbivory on 47 woody plant species (trees, shrubs, and vines) in the Netherlands and Japan. We find higher herbivore diversity, higher herbivore load and more herbivory on native plants than on non-native plants, generating support for the enemy release hypothesis.",what hypothesis ?,Enemy release,enemy release,True,True
"In recent decades the grass Phragmites australis has been aggressively in- vading coastal, tidal marshes of North America, and in many areas it is now considered a nuisance species. While P. australis has historically been restricted to the relatively benign upper border of brackish and salt marshes, it has been expanding seaward into more phys- iologically stressful regions. Here we test a leading hypothesis that the spread of P. australis is due to anthropogenic modification of coastal marshes. We did a field experiment along natural borders between stands of P. australis and the other dominant grasses and rushes (i.e., matrix vegetation) in a brackish marsh in Rhode Island, USA. We applied a pulse disturbance in one year by removing or not removing neighboring matrix vegetation and adding three levels of nutrients (specifically nitrogen) in a factorial design, and then we monitored the aboveground performance of P. australis and the matrix vegetation. Both disturbances increased the density, height, and biomass of shoots of P. australis, and the effects of fertilization were more pronounced where matrix vegetation was removed. Clear- ing competing matrix vegetation also increased the distance that shoots expanded and their reproductive output, both indicators of the potential for P. australis to spread within and among local marshes. In contrast, the biomass of the matrix vegetation decreased with increasing severity of disturbance. Disturbance increased the total aboveground production of plants in the marsh as matrix vegetation was displaced by P. australis. A greenhouse experiment showed that, with increasing nutrient levels, P. australis allocates proportionally more of its biomass to aboveground structures used for spread than to belowground struc- tures used for nutrient acquisition. Therefore, disturbances that enrich nutrients or remove competitors promote the spread of P. australis by reducing belowground competition for nutrients between P. australis and the matrix vegetation, thus allowing P. australis, the largest plant in the marsh, to expand and displace the matrix vegetation. Reducing nutrient load and maintaining buffers of matrix vegetation along the terrestrial-marsh ecotone will, therefore, be important methods of control for this nuisance species.",what hypothesis ?,Disturbance,disturbance,True,True
"Theory suggests that introduction effort (propagule size or number) should be a key determinant of establishment success for exotic species. Unfortunately, however, propagule pressure is not recorded for most introductions. Studies must therefore either use proxies whose efficacy must be largely assumed, or ignore effort altogether. The results of such studies will be flawed if effort is not distributed at random with respect to other characteristics that are predicted to influence success. We use global data for more than 600 introduction events for birds to show that introduction effort is both the strongest correlate of introduction success, and correlated with a large number of variables previously thought to influence success. Apart from effort, only habitat generalism relates to establishment success in birds.",what hypothesis ?,Propagule pressure,propagule pressure,True,True
"The enemy release hypothesis predicts that native herbivores prefer native, rather than exotic plants, giving invaders a competitive advantage. In contrast, the biotic resistance hypothesis states that many invaders are prevented from establishing because of competitive interactions, including herbivory, with native fauna and flora. Success or failure of spread and establishment might also be influenced by the presence or absence of mutualists, such as pollinators. Senecio madagascariensis (fireweed), an annual weed from South Africa, inhabits a similar range in Australia to the related native S. pinnatifolius. The aim of this study was to determine, within the context of invasion biology theory, whether the two Senecio species share insect fauna, including floral visitors and herbivores. Surveys were carried out in south-east Queensland on allopatric populations of the two Senecio species, with collected insects identified to morphospecies. Floral visitor assemblages were variable between populations. However, the two Senecio species shared the two most abundant floral visitors, honeybees and hoverflies. Herbivore assemblages, comprising mainly hemipterans of the families Cicadellidae and Miridae, were variable between sites and no patterns could be detected between Senecio species at the morphospecies level. However, when insect assemblages were pooled (i.e. community level analysis), S. pinnatifolius was shown to host a greater total abundance and richness of herbivores. Senecio madagascariensis is unlikely to be constrained by lack of pollinators in its new range and may benefit from lower levels of herbivory compared to its native congener S. pinnatifolius.",what hypothesis ?,Enemy release,enemy release,True,True
"Factors that influence the early stages of invasion can be critical to invasion success, yet are seldom studied. In particular, broad pre-adaptation to recipient climate may importantly influence early colonization success, yet few studies have explicitly examined this. I performed an experiment to determine how similarity between seed source and transplant site latitude, as a general indicator of pre-adaptation to climate, interacts with propagule pressure (100, 200 and 400 seeds/pot) to influence early colonization success of the widespread North American weed, St. John's wort Hypericum perforatum. Seeds originating from seven native European source populations were sown in pots buried in the ground in a field in western Montana. Seed source populations were either similar or divergent in latitude to the recipient transplant site. Across seed density treatments, the match between seed source and recipient latitude did not affect the proportion of pots colonized or the number of individual colonists per pot. In contrast, propagule pressure had a significant and positive effect on colonization. These results suggest that propagules from many climatically divergent source populations can be viable invaders.",what hypothesis ?,Propagule pressure,propagule pressure,True,True
"In multiply invaded ecosystems, introduced species should interact with each other as well as with native species. Invader-invader interactions may affect the success of further invaders by altering attributes of recipient communities and propagule pressure. The invasional meltdown hypothesis (IMH) posits that positive interactions among invaders initiate positive population-level feedback that intensifies impacts and promotes secondary invasions. IMH remains controversial: few studies show feedback between invaders that amplifies their effects, and none yet demonstrate facilitation of entry and spread of secondary invaders. Our results show that supercolonies of an alien ant, promoted by mutualism with introduced honeydew-secreting scale insects, permitted invasion by an exotic land snail on Christmas Island, Indian Ocean. Modeling of land snail spread over 750 sites across 135 km2 over seven years showed that the probability of land snail invasion was facilitated 253-fold in ant supercolonies but impeded in intact forest where predaceous native land crabs remained abundant. Land snail occurrence at neighboring sites, a measure of propagule pressure, also promoted land snail spread. Site comparisons and experiments revealed that ant supercolonies, by killing land crabs but not land snails, disrupted biotic resistance and provided enemy-free space. Predation pressure on land snails was lower (28.6%), survival 115 times longer, and abundance 20-fold greater in supercolonies than in intact forest. Whole-ecosystem suppression of supercolonies reversed the probability of land snail invasion by allowing recolonization of land crabs; land snails were much less likely (0.79%) to invade sites where supercolonies were suppressed than where they remained intact. Our results provide strong empirical evidence for IMH by demonstrating that mutualism between invaders reconfigures key interactions in the recipient community. This facilitates entry of secondary invaders and elevates propagule pressure, propagating their spread at the whole-ecosystem level. We show that identification and management of key facilitative interactions in invaded ecosystems can be used to reverse impacts and restore resistance to further invasions.",what hypothesis ?,Invasional meltdown,propagule pressure.,False,False
"The enemy release hypothesis, which posits that exotic species are less regulated by enemies than native species, has been well-supported in terrestrial systems but rarely tested in marine systems. Here, the enemy release hypothesis was tested in a marine system by excluding large enemies (>1.3 cm) in dock fouling communities in Washington, USA. After documenting the distribution and abundance of potential enemies such as chitons, gastropods and flatworms at 4 study sites, exclusion experiments were conducted to test the hypotheses that large grazing ene- mies (1) reduced recruitment rates in the exotic ascidian Botrylloides violaceus and native species, (2) reduced B. violaceus and native species abundance, and (3) altered fouling community struc- ture. Experiments demonstrated that, as predicted by the enemy release hypothesis, exclusion of large enemies did not significantly alter B. violaceus recruitment or abundance and it did signifi- cantly increase abundance or recruitment of 2 common native species. However, large enemy exclusion had no significant effects on most native species or on overall fouling community struc- ture. Furthermore, neither B. violaceus nor total exotic species abundance correlated positively with abundance of large enemies across sites. I therefore conclude that release from large ene- mies is likely not an important mechanism for the success of exotic species in Washington fouling communities.",what hypothesis ?,Enemy release,"enemy release hypothesis,",False,True
"Woodlands comprised of planted, nonnative trees are increasing in extent globally, while native woodlands continue to decline due to human activities. The ecological impacts of planted woodlands may include changes to the communities of understory plants and animals found among these nonnative trees relative to native woodlands, as well as invasion of adjacent habitat areas through spread beyond the originally planted areas. Eucalypts (Eucalyptus spp.) are among the most widely planted trees worldwide, and are very common in California, USA. The goals of our investigation were to compare the biological communities of nonnative eucalypt woodlands to native oak woodlands in coastal central California, and to examine whether planted eucalypt groves have increased in size over the past decades. We assessed site and habitat attributes and characterized biological communities using understory plant, ground-dwelling arthropod, amphibian, and bird communities as indicators. Degree of difference between native and nonnative woodlands depended on the indicator used. Eucalypts had significantly greater canopy height and cover, and significantly lower cover by perennial plants and species richness of arthropods than oaks. Community composition of arthropods also differed significantly between eucalypts and oaks. Eucalypts had marginally significantly deeper litter depth, lower abundance of native plants with ranges limited to western North America, and lower abundance of amphibians. In contrast to these differences, eucalypt and oak groves had very similar bird community composition, species richness, and abundance. We found no evidence of ""invasional meltdown,"" documenting similar abundance and richness of nonnatives in eucalypt vs. oak woodlands. Our time-series analysis revealed that planted eucalypt groves increased 271% in size, on average, over six decades, invading adjacent areas. Our results inform science-based management of California woodlands, revealing that while bird communities would probably not be affected by restoration of eucalypt to oak woodlands, such a restoration project would not only stop the spread of eucalypts into adjacent habitats but would also enhance cover by western North American native plants and perennials, enhance amphibian abundance, and increase arthropod richness.",what hypothesis ?,Invasional meltdown,"invasional meltdown, """,False,True
"Phenotypic plasticity has been suggested as the main mechanism for species persistence under a global change scenario, and also as one of the main mechanisms that alien species use to tolerate and invade broad geographic areas. However, contrasting with this central role of phenotypic plasticity, standard models aimed to predict the effect of climatic change on species distributions do not allow for the inclusion of differences in plastic responses among populations. In this context, the climatic variability hypothesis (CVH), which states that higher thermal variability at higher latitudes should determine an increase in phenotypic plasticity with latitude, could be considered a timely and promising hypothesis. Accordingly, in this study we evaluated, for the first time in a plant species (Taraxacum officinale), the prediction of the CVH. Specifically, we measured plastic responses at different environmental temperatures (5 and 20°C), in several ecophysiological and fitness-related traits for five populations distributed along a broad latitudinal gradient. Overall, phenotypic plasticity increased with latitude for all six traits analyzed, and mean trait values increased with latitude at both experimental temperatures, the change was noticeably greater at 20° than at 5°C. Our results suggest that the positive relationship found between phenotypic plasticity and geographic latitude could have very deep implications on future species persistence and invasion processes under a scenario of climate change.",what hypothesis ?,Phenotypic plasticity,phenotypic plasticity,True,True
"Many ecosystems receive a steady stream of non-native species. How biotic resistance develops over time in these ecosystems will depend on how established invaders contribute to subsequent resistance. If invasion success and defence capacity (i.e. contribution to resistance) are correlated, then community resistance should increase as species accumulate. If successful invaders also cause most impact (through replacing native species with low defence capacity) then the effect will be even stronger. If successful invaders instead have weak defence capacity or even facilitative attributes, then resistance should decrease with time, as proposed by the invasional meltdown hypothesis. We analysed 1157 introductions of freshwater fish in Swedish lakes and found that species' invasion success was positively correlated with their defence capacity and impact, suggesting that these communities will develop stronger resistance over time. These insights can be used to identify scenarios where invading species are expected to cause large impact.",what hypothesis ?,Invasional meltdown,invasional meltdown,True,True
"We used three congeneric annual thistles, which vary in their ability to invade California (USA) annual grasslands, to test whether invasiveness is related to differences in life history traits. We hypothesized that populations of these summer-flowering Centaurea species must pass through a demographic gauntlet of survival and reproduction in order to persist and that the most invasive species (C. solstitialis) might possess unique life history characteristics. Using the idea of a demographic gauntlet as a conceptual framework, we compared each congener in terms of (1) seed germination and seedling establishment, (2) survival of rosettes subjected to competition from annual grasses, (3) subsequent growth and flowering in adult plants, and (4) variation in breeding system. Grazing and soil disturbance is thought to affect Centaurea establishment, growth, and reproduction, so we also explored differences among congeners in their response to clipping and to different sizes of soil disturbance. We found minimal differences among congeners in either seed germination responses or seedling establishment and survival. In contrast, differential growth responses of congeners to different sizes of canopy gaps led to large differences in adult size and fecundity. Canopy-gap size and clipping affected the fecundity of each species, but the most invasive species (C. solstitialis) was unique in its strong positive response to combinations of clipping and canopy gaps. In addition, the phenology of C. solstitialis allows this species to extend its growing season into the summer—a time when competition from winter annual vegetation for soil water is minimal. Surprisingly, C. solstitialis was highly self-incompatible while the less invasive species were highly self-compatible. Our results suggest that the invasiveness of C. solstitialis arises, in part, from its combined ability to persist in competition with annual grasses and its plastic growth and reproductive responses to open, disturbed habitat patches. Corresponding Editor: D. P. C. Peters.",what hypothesis ?,Disturbance,disturbance,True,True
"Background: Weedy non-native species have long been predicted to be more phenotypically plastic than native species. Question: Are weedy non-native species more plastic than natives? Organisms: Fourteen perennial plant species: Acer platanoides, Acer saccharum, Bromus inermis, Bromus latiglumis, Celastrus orbiculatus, Celastrus scandens, Elymus repens, Elymus trachycaulus, Plantago major, Plantago rugelii, Rosa multiflora, Rosa palustris, Solanum dulcamara, and Solanum carolinense. Field site: Mesic old-field in Dryden, NY (422749″N, 762640″W). Methods: We grew seven pairs of native and non-native plant congeners in the field and tested their responses to reduced competition and the addition of fertilizer. We measured the plasticity of six traits related to growth and leaf palatability (total length, leaf dry mass, maximum relative growth rate, leaf toughness, trichome density, and specific leaf area). Conclusions: Weedy non-native species did not differ consistently from natives in their phenotypic plasticity. Instead, relatedness was a better predictor of plasticity.",what hypothesis ?,Phenotypic plasticity,"phenotypic plasticity. instead, relatedness was a better predictor of plasticity.",False,True
"Although biological invasions are of considerable concern to ecologists, relatively little attention has been paid to the potential for and consequences of indirect interactions between invasive species. Such interactions are generally thought to enhance invasives' spread and impact (i.e., the ""invasional meltdown"" hypothesis); however, exotic species might also act indirectly to slow the spread or blunt the impact of other invasives. On the east coast of the United States, the invasive hemlock woolly adelgid (Adelges tsugae, HWA) and elongate hemlock scale (Fiorinia externa, EHS) both feed on eastern hemlock (Tsuga canadensis). Of the two insects, HWA is considered far more damaging and disproportionately responsible for hemlock mortality. We describe research assessing the interaction between HWA and EHS, and the consequences of this interaction for eastern hemlock. We conducted an experiment in which uninfested hemlock branches were experimentally infested with herbivores in a 2 x 2 factorial design (either, both, or neither herbivore species). Over the 2.5-year course of the experiment, each herbivore's density was approximately 30% lower in mixed- vs. single-species treatments. Intriguingly, however, interspecific competition weakened rather than enhanced plant damage: growth was lower in the HWA-only treatment than in the HWA + EHS, EHS-only, or control treatments. Our results suggest that, for HWA-infested hemlocks, the benefit of co-occurring EHS infestations (reduced HWA density) may outweigh the cost (increased resource depletion).",what hypothesis ?,Invasional meltdown,invasional meltdown,True,True
"Hanley ME (2012). Seedling defoliation, plant growth and flowering potential in native- and invasive-range Plantago lanceolata populations. Weed Research52, 252–259. Summary The plastic response of weeds to new environmental conditions, in particular the likely relaxation of herbivore pressure, is considered vital for successful colonisation and spread. However, while variation in plant anti-herbivore resistance between native- and introduced-range populations is well studied, few authors have considered herbivore tolerance, especially at the seedling stage. This study examines variation in seedling tolerance in native (European) and introduced (North American) Plantago lanceolata populations following cotyledon removal at 14 days old. Subsequent effects on plant growth were quantified at 35 days, along with effects on flowering potential at maturity. Cotyledon removal reduced early growth for all populations, with no variation between introduced- or native-range plants. Although more variable, the effects of cotyledon loss on flowering potential were also unrelated to range. The likelihood that generalist seedling herbivores are common throughout North America may explain why no difference in seedling tolerance was apparent. However, increased flowering potential in plants from North American P. lanceolata populations was observed. As increased flowering potential was not lost, even after severe cotyledon damage, the manifestation of phenotypic plasticity in weeds at maturity may nonetheless still be shaped by plasticity in the ability to tolerate herbivory during seedling establishment.",what hypothesis ?,Phenotypic plasticity,phenotypic plasticity,True,True
"The current rate of invasive species introductions is unprecedented, and the dramatic impacts of exotic invasive plants on community and ecosystem properties have been well documented. Despite the pressing management implications, the mechanisms that control exotic plant invasion remain poorly understood. Several factors, such as disturbance, propagule pressure, species diversity, and herbivory, are widely believed to play a critical role in exotic plant invasions. However, few studies have examined the relative importance of these factors, and little is known about how propagule pressure interacts with various mechanisms of ecological resistance to determine invasion success. We quantified the relative importance of canopy disturbance, propagule pressure, species diversity, and herbivory in determining exotic plant invasion in 10 eastern hemlock forests in Pennsylvania and New Jersey (USA). Use of a maximum-likelihood estimation framework and information theoretics allowed us to quantify the strength of evidence for alternative models of the influence of these factors on changes in exotic plant abundance. In addition, we developed models to determine the importance of interactions between ecosystem properties and propagule pressure. These analyses were conducted for three abundant, aggressive exotic species that represent a range of life histories: Alliaria petiolata, Berberis thunbergii, and Microstegium vimineum. Of the four hypothesized determinants of exotic plant invasion considered in this study, canopy disturbance and propagule pressure appear to be the most important predictors of A. petiolata, B. thunbergii, and M. vimineum invasion. Herbivory was also found to be important in contributing to the invasion of some species. In addition, we found compelling evidence of an important interaction between propagule pressure and canopy disturbance. This is the first study to demonstrate the dominant role of the interaction between canopy disturbance and propagule pressure in determining forest invasibility relative to other potential controlling factors. The importance of the disturbance-propagule supply interaction, and its nonlinear functional form, has profound implications for the management of exotic plant species populations. Improving our ability to predict exotic plant invasions will require enhanced understanding of the interaction between propagule pressure and ecological resistance mechanisms.",what hypothesis ?,Propagule pressure,,False,False
"Species become invasive if they (i) are introduced to a new range, (ii) establish themselves, and (iii) spread. To address the global problems caused by invasive species, several studies investigated steps ii and iii of this invasion process. However, only one previous study looked at step i and examined the proportion of species that have been introduced beyond their native range. We extend this research by investigating all three steps for all freshwater fish, mammals, and birds native to Europe or North America. A higher proportion of European species entered North America than vice versa. However, the introduction rate from Europe to North America peaked in the late 19th century, whereas it is still rising in the other direction. There is no clear difference in invasion success between the two directions, so neither the imperialism dogma (that Eurasian species are exceptionally successful invaders) is supported, nor is the contradictory hypothesis that North America offers more biotic resistance to invaders than Europe because of its less disturbed and richer biota. Our results do not support the tens rule either: that approximately 10% of all introduced species establish themselves and that approximately 10% of established species spread. We find a success of approximately 50% at each step. In comparison, only approximately 5% of native vertebrates were introduced in either direction. These figures show that, once a vertebrate is introduced, it has a high potential to become invasive. Thus, it is crucial to minimize the number of species introductions to effectively control invasive vertebrates.",what hypothesis ?, Tens rule,imperialism dogma,False,False
"1 During the last centuries many alien species have established and spread in new regions, where some of them cause large ecological and economic problems. As one of the main explanations of the spread of alien species, the enemy‐release hypothesis is widely accepted and frequently serves as justification for biological control. 2 We used a global fungus–plant host distribution data set for 140 North American plant species naturalized in Europe to test whether alien plants are generally released from foliar and floral pathogens, whether they are mainly released from pathogens that are rare in the native range, and whether geographic spread of the North American plant species in Europe is associated with release from fungal pathogens. 3 We show that the 140 North American plant species naturalized in Europe were released from 58% of their foliar and floral fungal pathogen species. However, when we also consider fungal pathogens of the native North American host range that in Europe so far have only been reported on other plant species, the estimated release is reduced to 10.3%. Moreover, in Europe North American plants have mainly escaped their rare, pathogens, of which the impact is restricted to few populations. Most importantly and directly opposing the enemy‐release hypothesis, geographic spread of the alien plants in Europe was negatively associated with their release from fungal pathogens. 4 Synthesis. North American plants may have escaped particular fungal species that control them in their native range, but based on total loads of fungal species, release from foliar and floral fungal pathogens does not explain the geographic spread of North American plant species in Europe. To test whether enemy release is the major driver of plant invasiveness, we urgently require more studies comparing release of invasive and non‐invasive alien species from enemies of different guilds, and studies that assess the actual impact of the enemies.",what hypothesis ?,Enemy release,enemy ‐ release,False,False
"Numerous studies have shown how interactions between nonindigenous spe- cies (NIS) can accelerate the rate at which they establish and spread in invaded habitats, leading to an ""invasional meltdown."" We investigated facilitation at an earlier stage in the invasion process: during entrainment of propagules in a transport pathway. The introduced bryozoan Watersipora subtorquata is tolerant of several antifouling biocides and a common component of hull-fouling assemblages, a major transport pathway for aquatic NIS. We predicted that colonies of W. subtorquata act as nontoxic refugia for other, less tolerant species to settle on. We compared rates of recruitment of W. subtorquata and other fouling organisms to surfaces coated with three antifouling paints and a nontoxic primer in coastal marinas in Queensland, Australia. Diversity and abundance of fouling taxa were compared between bryozoan colonies and adjacent toxic or nontoxic paint surfaces. After 16 weeks immersion, W. subtorquata covered up to 64% of the tile surfaces coated in antifouling paint. Twenty-two taxa occurred exclusively on W. subtorquata and were not found on toxic surfaces. Other fouling taxa present on toxic surfaces were up to 248 times more abundant on W. subtorquata. Because biocides leach from the paint surface, we expected a positive relationship between the size of W. subtorquata colonies and the abundance and diversity of epibionts. To test this, we compared recruitment of fouling organisms to mimic W. subtorquata colonies of three different sizes that had the same total surface area. Sec- ondary recruitment to mimic colonies was greater when the surrounding paint surface contained biocides. Contrary to our predictions, epibionts were most abundant on small mimic colonies with a large total perimeter. This pattern was observed in encrusting and erect bryozoans, tubiculous amphipods, and serpulid and sabellid polychaetes, but only in the presence of toxic paint. Our results show that W. subtorquata acts as a foundation species for fouling assemblages on ship hulls and facilitates the transport of other species at greater abundance and frequency than would otherwise be possible. Invasion success may be increased by positive interactions between NIS that enhance the delivery of prop- agules by human transport vectors.",what hypothesis ?,Invasional meltdown,invasional meltdown.,True,True
"1 Understanding why some alien plant species become invasive when others fail is a fundamental goal in invasion ecology. We used detailed historical planting records of alien plant species introduced to Amani Botanical Garden, Tanzania and contemporary surveys of their invasion status to assess the relative ability of phylogeny, propagule pressure, residence time, plant traits and other factors to explain the success of alien plant species at different stages of the invasion process. 2 Species with native ranges centred in the tropics and with larger seeds were more likely to regenerate, whereas naturalization success was explained by longer residence time, faster growth rate, fewer seeds per fruit, smaller seed mass and shade tolerance. 3 Naturalized species spreading greater distances from original plantings tended to have more seeds per fruit, whereas species dispersed by canopy‐feeding animals and with native ranges centred on the tropics tended to have spread more widely in the botanical garden. Species dispersed by canopy‐feeding animals and with greater seed mass were more likely to be established in closed forest. 4 Phylogeny alone made a relatively minor contribution to the explanatory power of statistical models, but a greater proportion of variation in spread within the botanical garden and in forest establishment was explained by phylogeny alone than for other models. Phylogeny jointly with variables also explained a greater proportion of variation in forest establishment than in other models. Phylogenetic correction weakened the importance of dispersal syndrome in explaining compartmental spread, seed mass in the forest establishment model, and all factors except for growth rate and residence time in the naturalization model. 5 Synthesis. This study demonstrates that it matters considerably how invasive species are defined when trying to understand the relative ability of multiple variables to explain invasion success. By disentangling different invasion stages and using relatively objective criteria to assess species status, this study highlights that relatively simple models can help to explain why some alien plants are able to naturalize, spread and even establish in closed tropical forests.",what hypothesis ?,Propagule pressure,,False,False
"Abstract: Roads are believed to be a major contributing factor to the ongoing spread of exotic plants. We examined the effect of road improvement and environmental variables on exotic and native plant diversity in roadside verges and adjacent semiarid grassland, shrubland, and woodland communities of southern Utah ( U.S.A. ). We measured the cover of exotic and native species in roadside verges and both the richness and cover of exotic and native species in adjacent interior communities ( 50 m beyond the edge of the road cut ) along 42 roads stratified by level of road improvement ( paved, improved surface, graded, and four‐wheel‐drive track ). In roadside verges along paved roads, the cover of Bromus tectorum was three times as great ( 27% ) as in verges along four‐wheel‐drive tracks ( 9% ). The cover of five common exotic forb species tended to be lower in verges along four‐wheel‐drive tracks than in verges along more improved roads. The richness and cover of exotic species were both more than 50% greater, and the richness of native species was 30% lower, at interior sites adjacent to paved roads than at those adjacent to four‐wheel‐drive tracks. In addition, environmental variables relating to dominant vegetation, disturbance, and topography were significantly correlated with exotic and native species richness and cover. Improved roads can act as conduits for the invasion of adjacent ecosystems by converting natural habitats to those highly vulnerable to invasion. However, variation in dominant vegetation, soil moisture, nutrient levels, soil depth, disturbance, and topography may render interior communities differentially susceptible to invasions originating from roadside verges. Plant communities that are both physically invasible ( e.g., characterized by deep or fertile soils ) and disturbed appear most vulnerable. Decision‐makers considering whether to build, improve, and maintain roads should take into account the potential spread of exotic plants.",what hypothesis ?,Disturbance,"disturbance,",True,True
"The expression of defensive morphologies in prey often is correlated with predator abundance or diversity over a range of temporal and spatial scales. These patterns are assumed to reflect natural selection via differential predation on genetically determined, fixed phenotypes. Phenotypic variation, however, also can reflect within-generation developmental responses to environmental cues (phenotypic plasticity). For example, water-borne effluents from predators can induce the production of defensive morphologies in many prey taxa. This phenomenon, however, has been examined only on narrow scales. Here, we demonstrate adaptive phenotypic plasticity in prey from geographically separated populations that were reared in the presence of an introduced predator. Marine snails exposed to predatory crab effluent in the field increased shell thickness rapidly compared with controls. Induced changes were comparable to (i) historical transitions in thickness previously attributed to selection by the invading predator and (ii) present-day clinal variation predicted from water temperature differences. Thus, predator-induced phenotypic plasticity may explain broad-scale geographic and temporal phenotypic variation. If inducible defenses are heritable, then selection on the reaction norm may influence coevolution between predator and prey. Trade-offs may explain why inducible rather than constitutive defenses have evolved in several gastropod species.",what hypothesis ?,Phenotypic plasticity,phenotypic plasticity ),True,True
"Aim We used alien plant species introduced to a botanic garden to investigate the relative importance of species traits (leaf traits, dispersal syndrome) and introduction characteristics (propagule pressure, residence time and distance to forest) in explaining establishment success in surrounding tropical forest. We also used invasion scores from a weed risk assessment protocol as an independent measure of invasion risk and assessed differences in variables between high‐ and low‐risk species.",what hypothesis ?,Propagule pressure,propagule pressure,True,True
"We studied the relative importance of residence time, propagule pressure, and species traits in three stages of invasion of alien woody plants cultivated for about 150 years in the Czech Republic, Central Europe. The probability of escape from cultivation, naturalization, and invasion was assessed using classification trees. We compared 109 escaped-not-escaped congeneric pairs, 44 naturalized-not-naturalized, and 17 invasive-not-invasive congeneric pairs. We used the following predictors of the above probabilities: date of introduction to the target region as a measure of residence time; intensity of planting in the target area as a proxy for propagule pressure; the area of origin; and 21 species-specific biological and ecological traits. The misclassification rates of the naturalization and invasion model were low, at 19.3% and 11.8%, respectively, indicating that the variables used included the major determinants of these processes. The probability of escape increased with residence time in the Czech Republic, whereas the probability of naturalization increased with the residence time in Europe. This indicates that some species were already adapted to local conditions when introduced to the Czech Republic. Apart from residence time, the probability of escape depends on planting intensity (propagule pressure), and that of naturalization on the area of origin and fruit size; it is lower for species from Asia and those with small fruits. The probability of invasion is determined by a long residence time and the ability to tolerate low temperatures. These results indicate that a simple suite of factors determines, with a high probability, the invasion success of alien woody plants, and that the relative role of biological traits and other factors is stage dependent. High levels of propagule pressure as a result of planting lead to woody species eventually escaping from cultivation, regardless of biological traits. However, the biological traits play a role in later stages of invasion.",what hypothesis ?,Propagule pressure,"propagule pressure,",True,True
"Propagule pressure is recognized as a fundamental driver of freshwater fish invasions, though few studies have quantified its role. Natural experiments can be used to quantify the role of this factor relative to others in driving establishment success. An irrigation network in South Africa takes water from an inter-basin water transfer (IBWT) scheme to supply multiple small irrigation ponds. We compared fish community composition upstream, within, and downstream of the irrigation network, to show that this system is a unidirectional dispersal network with a single immigration source. We then assessed the effect of propagule pressure and biological adaptation on the colonization success of nine fish species across 30 recipient ponds of varying age. Establishing species received significantly more propagules at the source than did incidental species, while rates of establishment across the ponds displayed a saturation response to propagule pressure. This shows that propagule pressure is a significant driver of establishment overall. Those species that did not establish were either extremely rare at the immigration source or lacked the reproductive adaptations to breed in the ponds. The ability of all nine species to arrive at some of the ponds illustrates how long-term continuous propagule pressure from IBWT infrastructure enables range expansion of fishes. The quantitative link between propagule pressure and success and rate of population establishment confirms the driving role of this factor in fish invasion ecology.",what hypothesis ?,Propagule pressure,propagule pressure,True,True
"We used multiscale plots to sample vascular plant diversity and soil characteristics in and adjacent to 26 long-term grazing exclosure sites in Colorado, Wyoming, Montana, and South Dakota, USA. The exclosures were 7–60 yr old (31.2 ± 2.5 yr, mean ± 1 se). Plots were also randomly placed in the broader landscape in open rangeland in the same vegetation type at each site to assess spatial variation in grazed landscapes. Consistent sampling in the nine National Parks, Wildlife Refuges, and other management units yielded data from 78 1000-m2 plots and 780 1-m2 subplots. We hypothesized that native species richness would be lower in the exclosures than in grazed sites, due to competitive exclusion in the absence of grazing. We also hypothesized that grazed sites would have higher native and exotic species richness compared to ungrazed areas, due to disturbance (i.e., the intermediate-disturbance hypothesis) and the conventional wisdom that grazing may accelerate weed invasion. Both hypotheses were soundly rej...",what hypothesis ?,Disturbance,"disturbance ( i. e., the intermediate - disturbance hypothesis ) and the conventional wisdom that grazing may accelerate weed invasion.",False,True
"European countries in general, and England in particular, have a long history of introducing non-native fish species, but there exist no detailed studies of the introduction pathways and propagules pressure for any European country. Using the nine regions of England as a preliminary case study, the potential relationship between the occurrence in the wild of non-native freshwater fishes (from a recent audit of non-native species) and the intensity (i.e. propagule pressure) and diversity of fish imports was investigated. The main pathways of introduction were via imports of fishes for ornamental use (e.g. aquaria and garden ponds) and sport fishing, with no reported or suspected cases of ballast water or hull fouling introductions. The recorded occurrence of non-native fishes in the wild was found to be related to the time (number of years) since the decade of introduction. A shift in the establishment rate, however, was observed in the 1970s after which the ratio of established-to-introduced species declined. The number of established non-native fish species observed in the wild was found to increase significantly (P < 0·05) with increasing import intensity (log10x + 1 of the numbers of fish imported for the years 2000–2004) and with increasing consignment diversity (log10x + 1 of the numbers of consignment types imported for the years 2000–2004). The implications for policy and management are discussed.",what hypothesis ?,Propagule pressure,propagules pressure,False,False
"In introduced organisms, dispersal propensity is expected to increase during range expansion. This prediction is based on the assumption that phenotypic plasticity is low compared to genetic diversity, and an increase in dispersal can be counteracted by the Allee effect. Empirical evidence in support of these hypotheses is however lacking. The present study tested for evidence of differentiation in dispersal-related traits and the Allee effect in the wind-dispersed invasive Senecio inaequidens (Asteraceae). We collected capitula from individuals in ten field populations, along an invasion route including the original introduction site in southern France. In addition, we conducted a common garden experiment from field-collected seeds and obtained capitula from individuals representing the same ten field populations. We analysed phenotypic variation in dispersal traits between field and common garden environments as a function of the distance between populations and the introduction site. Our results revealed low levels of phenotypic differentiation among populations. However, significant clinal variation in dispersal traits was demonstrated in common garden plants representing the invasion route. In field populations, similar trends in dispersal-related traits and evidence of an Allee effect were not detected. In part, our results supported expectations of increased dispersal capacity with range expansion, and emphasized the contribution of phenotypic plasticity under natural conditions.",what hypothesis ?,Phenotypic plasticity,phenotypic plasticity,True,True
"Enemy release of exotic plants from soil pathogens has been tested by examining plant-soil feedback effects in repetitive growth cycles. However, positive soil feedback may also be due to enhanced benefit from the local arbuscular mycorrhizal fungi (AMF). Few studies actually have tested pathogen effects, and none of them did so in arid savannas. In the Kalahari savanna in Botswana, we compared the soil feedback of the exotic grass Cenchrus biflorus with that of two dominant native grasses, Eragrostis lehmanniana and Aristida meridionalis. The exotic grass had neutral to positive soil feedback, whereas both native grasses showed neutral to negative feedback effects. Isolation and testing of root-inhabiting fungi of E. lehmanniana yielded two host-specific pathogens that did not influence the exotic C. biflorus or the other native grass, A. meridionalis. None of the grasses was affected by the fungi that were isolated from the roots of the exotic C. biflorus. We isolated and compared the AMF community of the native and exotic grasses by polymerase chain reaction-denaturing gradient gel elecrophoresis (PCR-DGGE), targeting AMF 18S rRNA. We used roots from monospecific field stands and from plants grown in pots with mixtures of soils from the monospecific field stands. Three-quarters of the root samples of the exotic grass had two nearly identical sequences, showing 99% similarity with Glomus versiforme. The two native grasses were also associated with distinct bands, but each of these bands occurred in only a fraction of the root samples. The native grasses contained a higher diversity of AMF bands than the exotic grass. Canonical correspondence analyses of the AMF band patterns revealed almost as much difference between the native and exotic grasses as between the native grasses. In conclusion, our results support the hypothesis that release from soil-borne enemies may facilitate local abundance of exotic plants, and we provide the first evidence that these processes may occur in arid savanna ecosystems. Pathogenicity tests implicated the involvement of soil pathogens in the soil feedback responses, and further studies should reveal the functional consequences of the observed high infection with a low diversity of AMF in the roots of exotic plants.",what hypothesis ?,Enemy release,enemy release,True,True
"Abstract In hardwood subtropical forests of southern Florida, nonnative vines have been hypothesized to be detrimental, as many species form dense “vine blankets” that shroud the forest. To investigate the effects of nonnative vines in post-hurricane regeneration, we set up four large (two pairs of 30 × 60 m) study areas in each of three study sites. One of each pair was unmanaged and the other was managed by removal of nonnative plants, predominantly vines. Within these areas, we sampled vegetation in 5 × 5 m plots for stems 2 cm DBH (diameter at breast height) or greater and in 2 × 0.5 m plots for stems of all sizes. For five years, at annual censuses, we tagged and measured stems of vines, trees, shrubs and herbs in these plots. For each 5 × 5 m plot, we estimated percent coverage by individual vine species, using native and nonnative vines as classes. We investigated the hypotheses that: (1) plot coverage, occurrence and recruitment of nonnative vines were greater than that of native vines in unmanaged plots; (2) the management program was effective at reducing cover by nonnative vines; and (3) reduction of cover by nonnative vines improved recruitment of seedlings and saplings of native trees, shrubs, and herbs. In unmanaged plots, nonnative vines recruited more seedlings and had a significantly higher plot-cover index, but not a higher frequency of occurrence. Management significantly reduced cover by nonnative vines and had a significant overall positive effect on recruitment of seedlings and saplings of native trees, shrubs and herbs. Management also affected the seedling community (which included vines, trees, shrubs, and herbs) in some unanticipated ways, favoring early successional species for a longer period of time. The vine species with the greatest potential to “strangle” gaps were those that rapidly formed dense cover, had shade tolerant seedling recruitment, and were animal-dispersed. This suite of traits was more common in the nonnative vines than in the native vines. Our results suggest that some vines may alter the spatiotemporal pattern of recruitment sites in a forest ecosystem following a natural disturbance by creating many very shady spots very quickly.",what hypothesis ?,Disturbance,disturbance,True,True
"Abstract: We studied 28 alien tree species currently planted for forestry purposes in the Czech Republic to determine the probability of their escape from cultivation and naturalization. Indicators of propagule pressure (number of administrative units in which a species is planted and total planting area) and time of introduction into cultivation were used as explanatory variables in multiple regression models. Fourteen species escaped from cultivation, and 39% of the variance was explained by the number of planting units and the time of introduction, the latter being more important. Species introduced early had a higher probability of escape than those introduced later, with more than 95% probability of escape for those introduced before 1801 and <5% for those introduced after 1892. Probability of naturalization was more difficult to predict, and eight species were misclassified. A model omitting two species with the largest influence on the model yielded similar predictors of naturalization as did the probability of escape. Both phases of invasion therefore appear to be driven by planting and introduction history in a similar way. Our results demonstrate the importance of forestry for recruitment of invasive trees. Six alien forestry trees, classified as invasive in the Czech Republic, are currently reported in nature reserves. In addition, forestry authorities want to increase the diversity of alien species and planting area in the country.",what hypothesis ?,Propagule pressure,propagule pressure,True,True
"Disturbances have the potential to increase the success of bi ological invasions. Norway maple {Acer platanoides), a common street tree native to Europe, is a foreign invasive with greater tolerance and more effi cient resource utilization than the native sugar maple (Acer saccharum). This study examined the role disturbances from a road and path played in the invasion of Norway maple and in the distribution of sugar maple. Disturbed areas on the path and nearby undisturbed areas were surveyed for both species along transects running perpendicular to a road. Norway maples were present in greater number closer to the road and on the path, while the number of sugar maples was not significantly associated with either the road or the path. These results suggest that human-caused disturbances have a role in facili tating the establishment of an invasive species.",what hypothesis ?,Disturbance,disturbances,False,True
"The ability to succeed in diverse conditions is a key factor allowing introduced species to successfully invade and spread across new areas. Two non-exclusive factors have been suggested to promote this ability: adaptive phenotypic plasticity of individuals, and the evolution of locally adapted populations in the new range. We investigated these individual and population-level factors in Polygonum cespitosum, an Asian annual that has recently become invasive in northeastern North America. We characterized individual fitness, life-history, and functional plasticity in response to two contrasting glasshouse habitat treatments (full sun/dry soil and understory shade/moist soil) in 165 genotypes sampled from nine geographically separate populations representing the range of light and soil moisture conditions the species inhabits in this region. Polygonum cespitosum genotypes from these introduced-range populations expressed broadly similar plasticity patterns. In response to full sun, dry conditions, genotypes from all populations increased photosynthetic rate, water use efficiency, and allocation to root tissues, dramatically increasing reproductive fitness compared to phenotypes expressed in simulated understory shade. Although there were subtle among-population differences in mean trait values as well as in the slope of plastic responses, these population differences did not reflect local adaptation to environmental conditions measured at the population sites of origin. Instead, certain populations expressed higher fitness in both glasshouse habitat treatments. We also compared the introduced-range populations to a single population from the native Asian range, and found that the native population had delayed phenology, limited functional plasticity, and lower fitness in both experimental environments compared with the introduced-range populations. Our results indicate that the future spread of P. cespitosum in its introduced range will likely be fueled by populations consisting of individuals able to express high fitness across diverse light and moisture conditions, rather than by the evolution of locally specialized populations.",what hypothesis ?,Phenotypic plasticity,phenotypic plasticity,True,True
"Abstract Extensive areas in the mountain grasslands of central Argentina are heavily invaded by alien species from Europe. A decrease in biodiversity and a loss of palatable species is also observed. The invasibility of the tall-grass mountain grassland community was investigated in an experiment of factorial design. Six alien species which are widely distributed in the region were sown in plots where soil disturbance, above-ground biomass removal by cutting and burning were used as treatments. Alien species did not establish in undisturbed plots. All three types of disturbances increased the number and cover of alien species; the effects of soil disturbance and biomass removal was cumulative. Cirsium vulgare and Oenothera erythrosepala were the most efficient alien colonizers. In conditions where disturbances did not continue the cover of aliens started to decrease in the second year, by the end of the third season, only a few adults were established. Consequently, disturbances are needed to maintain ali...",what hypothesis ?,Disturbance,"disturbance,",True,True
"The differences in phenotypic plasticity between invasive (North American) and native (German) provenances of the invasive plant Lythrum salicaria (purple loosestrife) were examined using a multivariate reaction norm approach testing two important attributes of reaction norms described by multivariate vectors of phenotypic change: the magnitude and direction of mean trait differences between environments. Data were collected for six life history traits from native and invasive plants using a split-plot design with experimentally manipulated water and nutrient levels. We found significant differences between native and invasive plants in multivariate phenotypic plasticity for comparisons between low and high water treatments within low nutrient levels, between low and high nutrient levels within high water treatments, and for comparisons that included both a water and nutrient level change. The significant genotype x environment (G x E) effects support the argument that invasiveness of purple loosestrife is closely associated with the interaction of high levels of soil nutrient and flooding water regime. Our results indicate that native and invasive plants take different strategies for growth and reproduction; native plants flowered earlier and allocated more to flower production, while invasive plants exhibited an extended period of vegetative growth before flowering to increase height and allocation to clonal reproduction, which may contribute to increased fitness and invasiveness in subsequent years.",what Species name ?,Lythrum salicaria,lythrum salicaria ( purple loosestrife ),False,True
"In introduced organisms, dispersal propensity is expected to increase during range expansion. This prediction is based on the assumption that phenotypic plasticity is low compared to genetic diversity, and an increase in dispersal can be counteracted by the Allee effect. Empirical evidence in support of these hypotheses is however lacking. The present study tested for evidence of differentiation in dispersal-related traits and the Allee effect in the wind-dispersed invasive Senecio inaequidens (Asteraceae). We collected capitula from individuals in ten field populations, along an invasion route including the original introduction site in southern France. In addition, we conducted a common garden experiment from field-collected seeds and obtained capitula from individuals representing the same ten field populations. We analysed phenotypic variation in dispersal traits between field and common garden environments as a function of the distance between populations and the introduction site. Our results revealed low levels of phenotypic differentiation among populations. However, significant clinal variation in dispersal traits was demonstrated in common garden plants representing the invasion route. In field populations, similar trends in dispersal-related traits and evidence of an Allee effect were not detected. In part, our results supported expectations of increased dispersal capacity with range expansion, and emphasized the contribution of phenotypic plasticity under natural conditions.",what Species name ?,Senecio inaequidens,senecio inaequidens,True,True
"An unresolved question in ecology concerns why the ecological effects of invasions vary in magnitude. Many introduced species fail to interact strongly with the recipient biota, whereas others profoundly disrupt the ecosystems they invade through predation, competition, and other mechanisms. In the context of ecological impacts, research on biological invasions seldom considers phenotypic or microevolutionary changes that occur following introduction. Here, we show how plasticity in key life history traits (colony size and longevity), together with omnivory, magnifies the predatory impacts of an invasive social wasp (Vespula pensylvanica) on a largely endemic arthropod fauna in Hawaii. Using a combination of molecular, experimental, and behavioral approaches, we demonstrate (i) that yellowjackets consume an astonishing diversity of arthropod resources and depress prey populations in invaded Hawaiian ecosystems and (ii) that their impact as predators in this region increases when they shift from small annual colonies to large perennial colonies. Such trait plasticity may influence invasion success and the degree of disruption that invaded ecosystems experience. Moreover, postintroduction phenotypic changes may help invaders to compensate for reductions in adaptive potential resulting from founder events and small population sizes. The dynamic nature of biological invasions necessitates a more quantitative understanding of how postintroduction changes in invader traits affect invasion processes.",what Species name ?,Vespula pensylvanica,vespula pensylvanica ),True,True
"Background: Brown trout (Salmo trutta) were introduced into, and subsequently colonized, a number of disparate watersheds on the island of Newfoundland, Canada (110,638 km 2 ), starting in 1883. Questions: Do environmental features of recently invaded habitats shape population-level phenotypic variability? Are patterns of phenotypic variability suggestive of parallel adaptive divergence? And does the extent of phenotypic divergence increase as a function of distance between populations? Hypotheses: Populations that display similar phenotypes will inhabit similar environments. Patterns in morphology, coloration, and growth in an invasive stream-dwelling fish should be consistent with adaptation, and populations closer to each other should be more similar than should populations that are farther apart. Organism and study system: Sixteen brown trout populations of probable common descent, inhabiting a gradient of environments. These populations include the most ancestral (∼130 years old) and most recently established (∼20 years old). Analytical methods: We used multivariate statistical techniques to quantify morphological (e.g. body shape via geometric morphometrics and linear measurements of traits), meristic (e.g. counts of pigmentation spots), and growth traits from 1677 individuals. To account for ontogenetic and allometric effects on morphology, we conducted separate analyses on three distinct size/age classes. We used the BIO-ENV routine and Mantel tests to measure the correlation between phenotypic and habitat features. Results: Phenotypic similarity was significantly correlated with environmental similarity, especially in the larger size classes of fish. The extent to which these associations between phenotype and habitat result from parallel evolution, adaptive phenotypic plasticity, or historical founder effects is not known. Observed patterns of body shape and fin sizes were generally consistent with predictions of adaptive trait patterns, but other traits showed less consistent patterns with habitat features. Phenotypic differences increased as a function of straight-line distance (km) between watersheds and to a lesser extent fish dispersal distances, which suggests habitat has played a more significant role in shaping population phenotypes compared with founder effects.",what Species name ?,Salmo trutta,salmo trutta ),True,True
"Introduced species must adapt their ecology, behaviour, and morphological traits to new conditions. The successful introduction and invasive potential of a species are related to its levels of phenotypic plasticity and genetic polymorphism. We analysed changes in the body mass and length of American mink (Neovison vison) since its introduction into the Warta Mouth National Park, western Poland, in relation to diet composition and colonization progress from 1996 to 2004. Mink body mass decreased significantly during the period of population establishment within the study area, with an average decrease of 13% from 1.36 to 1.18 kg in males and of 16% from 0.83 to 0.70 kg in females. Diet composition varied seasonally and between consecutive years. The main prey items were mammals and fish in the cold season and birds and fish in the warm season. During the study period the proportion of mammals preyed upon increased in the cold season and decreased in the warm season. The proportion of birds preyed upon decreased over the study period, whereas the proportion of fish increased. Following introduction, the strictly aquatic portion of mink diet (fish and frogs) increased over time, whereas the proportion of large prey (large birds, muskrats, and water voles) decreased. The average yearly proportion of large prey and average-sized prey in the mink diet was significantly correlated with the mean body masses of males and females. Biogeographical variation in the body mass and length of mink was best explained by the percentage of large prey in the mink diet in both sexes, and by latitude for females. Together these results demonstrate that American mink rapidly changed their body mass in relation to local conditions. This phenotypic variability may be underpinned by phenotypic plasticity and/or by adaptation of quantitative genetic variation. The potential to rapidly change phenotypic variation in this manner is an important factor determining the negative ecological impacts of invasive species. © 2012 The Linnean Society of London, Biological Journal of the Linnean Society, 2012, 105, 681–693.",what Species name ?,Neovison vison,neovison vison ),True,True
"Alliaria petiolata is a Eurasian biennial herb that is invasive in North America and for which phenotypic plasticity has been noted as a potentially important invasive trait. Using four European and four North American populations, we explored variation among populations in the response of a suite of antioxidant, antiherbivore, and morphological traits to the availability of water and nutrients and to jasmonic acid treatment. Multivariate analyses revealed substantial variation among populations in mean levels of these traits and in the response of this suite of traits to environmental variation, especially water availability. Univariate analyses revealed variation in plasticity among populations in the expression of all of the traits measured to at least one of these environmental factors, with the exception of leaf length. There was no evidence for continentally distinct plasticity patterns, but there was ample evidence for variation in phenotypic plasticity among the populations within continents. This implies that A. petiolata has the potential to evolve distinct phenotypic plasticity patterns within populations but that invasive populations are no more plastic than native populations.",what Species name ?,Alliaria petiolata,alliaria petiolata,True,True
"The mechanisms underlying successful biological invasions often remain unclear. In the case of the tropical water flea Daphnia lumholtzi, which invaded North America, it has been suggested that this species possesses a high thermal tolerance, which in the course of global climate change promotes its establishment and rapid spread. However, D. lumholtzi has an additional remarkable feature: it is the only water flea that forms rigid head spines in response to chemicals released in the presence of fishes. These morphologically (phenotypically) plastic traits serve as an inducible defence against these predators. Here, we show in controlled mesocosm experiments that the native North American species Daphnia pulicaria is competitively superior to D. lumholtzi in the absence of predators. However, in the presence of fish predation the invasive species formed its defences and became dominant. This observation of a predator-mediated switch in dominance suggests that the inducible defence against fish predation may represent a key adaptation for the invasion success of D. lumholtzi.",what Species name ?,Daphnia lumholtzi,"daphnia lumholtzi,",True,True
"Invasive species have been hypothesized to out-compete natives though either a Jack-of-all-trades strategy, where they are able to utilize resources effectively in unfavourable environments, a master-of-some, where resource utilization is greater than its competitors in favourable environments, or a combination of the two (Jack-and-master). We examined the invasive strategy of Berberis darwinii in New Zealand compared with four co-occurring native species by examining germination, seedling survival, photosynthetic characteristics and water-use efficiency of adult plants, in sun and shade environments. Berberis darwinii seeds germinated more in shady sites than the other natives, but survival was low. In contrast, while germination of B. darwinii was the same as the native species in sunny sites, seedling survival after 18 months was nearly twice that of the all native species. The maximum photosynthetic rate of B. darwinii was nearly double that of all native species in the sun, but was similar among all species in the shade. Other photosynthetic traits (quantum yield and stomatal conductance) did not generally differ between B. darwinii and the native species, regardless of light environment. Berberis darwinii had more positive values of δ13C than the four native species, suggesting that it gains more carbon per unit water transpired than the competing native species. These results suggest that the invasion success of B. darwinii may be partially explained by combination of a Jack-of-all-trades scenario of widespread germination with a master-of-some scenario through its ability to photosynthesize at higher rates in the sun and, hence, gain a rapid height and biomass advantage over native species in favourable environments.",what Species name ?,Berberis darwinii,berberis darwinii,True,True
"Invasiveness may result from genetic variation and adaptation or phenotypic plasticity, and genetic variation in fitness traits may be especially critical. Pennisetum setaceum (fountain grass, Poaceae) is highly invasive in Hawaii (HI), moderately invasive in Arizona (AZ), and less invasive in southern California (CA). In common garden experiments, we examined the relative importance of quantitative trait variation, precipitation, and phenotypic plasticity in invasiveness. In two very different environments, plants showed no differences by state of origin (HI, CA, AZ) in aboveground biomass, seeds/flower, and total seed number. Plants from different states were also similar within watering treatment. Plants with supplemental watering, relative to unwatered plants, had greater biomass, specific leaf area (SLA), and total seed number, but did not differ in seeds/flower. Progeny grown from seeds produced under different watering treatments showed no maternal effects in seed mass, germination, biomass or SLA. High phenotypic plasticity, rather than local adaptation is likely responsible for variation in invasiveness. Global change models indicate that temperature and precipitation patterns over the next several decades will change, although the direction of change is uncertain. Drier summers in southern California may retard further invasion, while wetter summers may favor the spread of fountain grass.",what Species name ?,Pennisetum setaceum,pennisetum setaceum,True,True
"Abstract The selection and introduction of drought tolerant species is a common method of restoring degraded grasslands in arid environments. This study investigated the effects of water stress on growth, water relations, Na+ and K+ accumulation, and stomatal development in the native plant species Zygophyllum xanthoxylum (Bunge) Maxim., and an introduced species, Caragana korshinskii Kom., under three watering regimes. Moderate drought significantly reduced pre‐dawn water potential, leaf relative water content, total biomass, total leaf area, above‐ground biomass, total number of leaves and specific leaf area, but it increased the root/total weight ratio (0.23 versus 0.33) in C. korshinskii. Only severe drought significantly affected water status and growth in Z. xanthoxylum. In any given watering regime, a significantly higher total biomass was observed in Z. xanthoxylum (1.14 g) compared to C. korshinskii (0.19 g). Moderate drought significantly increased Na+ accumulation in all parts of Z. xanthoxylum, e.g., moderate drought increased leaf Na+ concentration from 1.14 to 2.03 g/100 g DW, however, there was no change in Na+ (0.11 versus 0.12) in the leaf of C. korshinskii when subjected to moderate drought. Stomatal density increased as water availability was reduced in both C. korshinskii and Z. xanthoxylum, but there was no difference in stomatal index of either species. Stomatal length and width, and pore width were significantly reduced by moderate water stress in Z. xanthoxylum, but severe drought was required to produce a significant effect in C. korshinskii. These results indicated that C. korshinskii is more responsive to water stress and exhibits strong phenotypic plasticity especially in above‐ground/below‐ground biomass allocation. In contrast, Z. xanthoxylum was more tolerant to water deficit, with a lower specific leaf area and a strong ability to maintain water status through osmotic adjustment and stomatal closure, thereby providing an effective strategy to cope with local extreme arid environments.",what Species name ?,Caragana korshinskii,"caragana korshinskii kom.,",False,True
"The relative importance of plasticity vs. adaptation for the spread of invasive species has rarely been studied. We examined this question in a clonal population of invasive freshwater snails (Potamopyrgus antipodarum) from the western United States by testing whether observed plasticity in life history traits conferred higher fitness across a range of temperatures. We raised isofemale lines from three populations from different climate regimes (high- and low-elevation rivers and an estuary) in a split-brood, common-garden design in three temperatures. We measured life history and growth traits and calculated population growth rate (as a measure of fitness) using an age-structured projection matrix model. We found a strong effect of temperature on all traits, but no evidence for divergence in the average level of traits among populations. Levels of genetic variation and significant reaction norm divergence for life history traits suggested some role for adaptation. Plasticity varied among traits and was lowest for size and reproductive traits compared to age-related traits and fitness. Plasticity in fitness was intermediate, suggesting that invasive populations are not general-purpose genotypes with respect to the range of temperatures studied. Thus, by considering plasticity in fitness and its component traits, we have shown that trait plasticity alone does not yield the same fitness across a relevant set of temperature conditions.",what Species name ?,Potamopyrgus antipodarum,potamopyrgus antipodarum ),True,True
"sempervirens L., a non-invasive native. We hypothesized that greater morphological plasticity may contribute to the ability of L. japonica to occupy more habitat types, and contribute to its invasiveness. We compared the morphology of plants provided with climbing supports with plants that had no climbing supports, and thus quantified their morphological plasticity in response to an important variable in their habitats. The two species responded differently to the treatments, with L. japonica showing greater responses in more characters. For example, Lonicera japonica responded to climbing supports with a 15.3% decrease in internode length, a doubling of internode number and a 43% increase in shoot biomass. In contrast, climbing supports did not influence internode length or shoot biomass for L. sempervirens, and only resulted in a 25% increase in internode number. This plasticity may allow L. japonica to actively place plant modules in favorable microhabitats and ultimately affect plant fitness.",what Species name ?,Lonicera japonic,"sempervirens l.,",False,False
"We investigated whether plasticity in growth responses to nutrients could predict invasive potential in aquatic plants by measuring the effects of nutrients on growth of eight non‐invasive native and six invasive exotic aquatic plant species. Nutrients were applied at two levels, approximating those found in urbanized and relatively undisturbed catchments, respectively. To identify systematic differences between invasive and non‐invasive species, we compared the growth responses (total biomass, root:shoot allocation, and photosynthetic surface area) of native species with those of related invasive species after 13 weeks growth. The results were used to seek evidence of invasive potential among four recently naturalized species. There was evidence that invasive species tend to accumulate more biomass than native species (P = 0.0788). Root:shoot allocation did not differ between native and invasive plant species, nor was allocation affected by nutrient addition. However, the photosynthetic surface area of invasive species tended to increase with nutrients, whereas it did not among native species (P = 0.0658). Of the four recently naturalized species, Hydrocleys nymphoides showed the same nutrient‐related plasticity in photosynthetic area displayed by known invasive species. Cyperus papyrus showed a strong reduction in photosynthetic area with increased nutrients. H. nymphoides and C. papyrus also accumulated more biomass than their native relatives. H. nymphoides possesses both of the traits we found to be associated with invasiveness, and should thus be regarded as likely to be invasive.",what Species name ?,Aquatic plant species,hydrocleys nymphoides,False,False
"The Natural Language Processing (NLP) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in Knowledge Graphs (KGs). Considering Wikidata as the background KG, there are still limited tools to link knowledge within the text to Wikidata. In this paper, we present Falcon 2.0, the first joint entity and relation linking tool over Wikidata. It receives a short natural language text in the English language and outputs a ranked list of entities and relations annotated with the proper candidates in Wikidata. The candidates are represented by their Internationalized Resource Identifier (IRI) in Wikidata. Falcon 2.0 resorts to the English language model for the recognition task (e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach for the linking task. We have empirically studied the performance of Falcon 2.0 on Wikidata and concluded that it outperforms all the existing baselines. Falcon 2.0 is open source and can be reused by the community; all the required instructions of Falcon 2.0 are well-documented at our GitHub repository (https://github.com/SDM-TIB/falcon2.0). We also demonstrate an online API, which can be run without any technical expertise. Falcon 2.0 and its background knowledge bases are available as resources at https://labs.tib.eu/falcon/falcon2/.",what contains ?,Model,wikidata,False,False
"This conceptual paper reviews the current status of goal setting in the area of technology enhanced learning and education. Besides a brief literature review, three current projects on goal setting are discussed. The paper shows that the main barriers for goal setting applications in education are not related to the technology, the available data or analytical methods, but rather the human factor. The most important bottlenecks are the lack of students goal setting skills and abilities, and the current curriculum design, which, especially in the observed higher education institutions, provides little support for goal setting interventions.",what contains ?,Methods,"literature review,",False,False
"Due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving. In the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated. However, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes. To integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities. Based on this survey, a taxonomy has been created which is used to classify the existing research. Furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments.",what contains ?,Methods,,False,False
"Abstract Motivation Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. Availability and implementation We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.",what contains ?,Results,"https : / / github. com / naver / biobert - pretrained,",False,False
"ABSTRACT Contested heritage has increasingly been studied by scholars over the last two decades in multiple disciplines, however, there is still limited knowledge about what contested heritage is and how it is realized in society. Therefore, the purpose of this paper is to produce a systematic literature review on this topic to provide a holistic understanding of contested heritage, and delineate its current state, trends and gaps. Methodologically, four electronic databases were searched, and 102 journal articles published before 2020 were extracted. A content analysis of each article was then conducted to identify key themes and variables for classification. Findings show that while its research often lacks theoretical underpinnings, contested heritage is marked by its diversity and complexity as it becomes a global issue for both tourism and urbanization. By presenting a holistic understanding of contested heritage, this review offers an extensive investigation of the topic area to help move literature pertaining contested heritage forward.",what has subject domain ?,Contested heritage,contested heritage,True,True
"Abstract Hackathons, time-bounded events where participants write computer code and build apps, have become a popular means of socializing tech students and workers to produce “innovation” despite little promise of material reward. Although they offer participants opportunities for learning new skills and face-to-face networking and set up interaction rituals that create an emotional “high,” potential advantage is even greater for the events’ corporate sponsors, who use them to outsource work, crowdsource innovation, and enhance their reputation. Ethnographic observations and informal interviews at seven hackathons held in New York during the course of a single school year show how the format of the event and sponsors’ discursive tropes, within a dominant cultural frame reflecting the appeal of Silicon Valley, reshape unpaid and precarious work as an extraordinary opportunity, a ritual of ecstatic labor, and a collective imaginary for fictional expectations of innovation that benefits all, a powerful strategy for manufacturing workers’ consent in the “new” economy.",what has subject domain ?,Hackathons,"computer code and build apps, have become a popular means of socializing tech students and workers to produce “ innovation ”",False,False
"ABSTRACT ‘Heritage Interpretation’ has always been considered as an effective learning, communication and management tool that increases visitors’ awareness of and empathy to heritage sites or artefacts. Yet the definition of ‘digital heritage interpretation’ is still wide and so far, no significant method and objective are evident within the domain of ‘digital heritage’ theory and discourse. Considering ‘digital heritage interpretation’ as a process rather than as a tool to present or communicate with end-users, this paper presents a critical application of a theoretical construct ascertained from multiple disciplines and explicates four objectives for a comprehensive interpretive process. A conceptual model is proposed and further developed into a conceptual framework with fifteen considerations. This framework is then implemented and tested on an online platform to assess its impact on end-users’ interpretation level. We believe the presented interpretive framework (PrEDiC) will help heritage professionals and media designers to develop interpretive heritage project.",what has subject domain ?,Heritage Interpretation,digital heritage interpretation ’,False,True
"Background The COVID-19 outbreak has affected the lives of millions of people by causing a dramatic impact on many health care systems and the global economy. This devastating pandemic has brought together communities across the globe to work on this issue in an unprecedented manner. Objective This case study describes the steps and methods employed in the conduction of a remote online health hackathon centered on challenges posed by the COVID-19 pandemic. It aims to deliver a clear implementation road map for other organizations to follow. Methods This 4-day hackathon was conducted in April 2020, based on six COVID-19–related challenges defined by frontline clinicians and researchers from various disciplines. An online survey was structured to assess: (1) individual experience satisfaction, (2) level of interprofessional skills exchange, (3) maturity of the projects realized, and (4) overall quality of the event. At the end of the event, participants were invited to take part in an online survey with 17 (+5 optional) items, including multiple-choice and open-ended questions that assessed their experience regarding the remote nature of the event and their individual project, interprofessional skills exchange, and their confidence in working on a digital health project before and after the hackathon. Mentors, who guided the participants through the event, also provided feedback to the organizers through an online survey. Results A total of 48 participants and 52 mentors based in 8 different countries participated and developed 14 projects. A total of 75 mentorship video sessions were held. Participants reported increased confidence in starting a digital health venture or a research project after successfully participating in the hackathon, and stated that they were likely to continue working on their projects. Of the participants who provided feedback, 60% (n=18) would not have started their project without this particular hackathon and indicated that the hackathon encouraged and enabled them to progress faster, for example, by building interdisciplinary teams, gaining new insights and feedback provided by their mentors, and creating a functional prototype. Conclusions This study provides insights into how online hackathons can contribute to solving the challenges and effects of a pandemic in several regions of the world. The online format fosters team diversity, increases cross-regional collaboration, and can be executed much faster and at lower costs compared to in-person events. Results on preparation, organization, and evaluation of this online hackathon are useful for other institutions and initiatives that are willing to introduce similar event formats in the fight against COVID-19.",what has subject domain ?,challenges posed by the COVID-19 pandemic,health care systems and the global economy.,False,False
"Abstract. In 2017 we published a seminal research study in the International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences about how smart city tools, solutions and applications underpinned historical and cultural heritage of cities at that time (Angelidou et al. 2017). We now return to investigate the progress that has been made during the past three years, and specifically whether the weak substantiation of cultural heritage in smart city strategies that we observed in 2017 has been improved. The newest literature suggests that smart cities should capitalize on local strengths and give prominence to local culture and traditions and provides a handful of solutions to this end. However, a more thorough examination of what has been actually implemented reveals a (still) rather immature approach. The smart city cases that were selected for the purposes of this research include Tarragona (Spain), Budapest (Hungary) and Karlsruhe (Germany). For each one we collected information regarding the overarching structure of the initiative, the positioning of cultural heritage and the inclusion of heritage-related smart city applications. We then performed a comparative analysis based on a simplified version of the Digital Strategy Canvas. Our findings suggest that a rich cultural heritage and a broader strategic focus on touristic branding and promotion are key ingredients of smart city development in this domain; this is a commonality of all the investigated cities. Moreover, three different strategy architectures emerge, representing the different interplays among the smart city, cultural heritage and sustainable urban development. We conclude that a new generation of smart city initiatives is emerging, in which cultural heritage is of increasing importance. This generation tends to associate cultural heritage with social and cultural values, liveability and sustainable urban development.",what has subject domain ?,cultural heritage,,False,False
"<p>A nonfullerene electron acceptor (IEIC) based on indaceno[1,2-<italic>b</italic>:5,6-<italic>b</italic>′]dithiophene and 2-(3-oxo-2,3-dihydroinden-1-ylidene)malononitrile was designed and synthesized, and fullerene-free polymer solar cells based on the IEIC acceptor showed power conversion efficiencies of up to 6.31%.</p>",what Mobility ?,Electron,6. 31 %,False,False
"A new electron‐rich central building block, 5,5,12,12‐tetrakis(4‐hexylphenyl)‐indacenobis‐(dithieno[3,2‐b:2′,3′‐d]pyrrol) (INP), and two derivative nonfullerene acceptors (INPIC and INPIC‐4F) are designed and synthesized. The two molecules reveal broad (600–900 nm) and strong absorption due to the satisfactory electron‐donating ability of INP. Compared with its counterpart INPIC, fluorinated nonfullerene acceptor INPIC‐4F exhibits a stronger near‐infrared absorption with a narrower optical bandgap of 1.39 eV, an improved crystallinity with higher electron mobility, and down‐shifted highest occupied molecular orbital and lowest unoccupied molecular orbital energy levels. Organic solar cells (OSCs) based on INPIC‐4F exhibit a high power conversion efficiency (PCE) of 13.13% and a relatively low energy loss of 0.54 eV, which is among the highest efficiencies reported for binary OSCs in the literature. The results demonstrate the great potential of the new INP as an electron‐donating building block for constructing high‐performance nonfullerene acceptors for OSCs.",what Mobility ?,Electron,13. 13 %,False,False
"Two cheliform non-fullerene acceptors, DTPC-IC and DTPC-DFIC, based on a highly electron-rich core, dithienopicenocarbazole (DTPC), are synthesized, showing ultra-narrow bandgaps (as low as 1.21 eV). The two-dimensional nitrogen-containing conjugated DTPC possesses strong electron-donating capability, which induces intense intramolecular charge transfer and intermolecular π-π stacking in derived acceptors. The solar cell based on DTPC-DFIC and a spectrally complementary polymer donor, PTB7-Th, showed a high power conversion efficiency of 10.21% and an extremely low energy loss of 0.45 eV, which is the lowest among reported efficient OSCs.",what Mobility ?,Electron,10. 21 %,False,False
"Low-bandgap polymers/molecules are an interesting family of semiconductor materials, and have enabled many recent exciting breakthroughs in the field of organic electronics, especially for organic photovoltaics (OPVs). Here, such a low-bandgap (1.43 eV) non-fullerene electron acceptor (BT-IC) bearing a fused 7-heterocyclic ring with absorption edge extending to the near-infrared (NIR) region was specially designed and synthesized. Benefitted from its NIR light harvesting, high performance OPVs were fabricated with medium bandgap polymers (J61 and J71) as donors, showing power conversion efficiencies of 9.6% with J61 and 10.5% with J71 along with extremely low energy loss (0.56 eV for J61 and 0.53 eV for J71). Interestingly, femtosecond transient absorption spectroscopy studies on both systems show that efficient charge generation was observed despite the fact that the highest occupied molecular orbital (HOMO)–HOMO offset (ΔEH) in the blends was as low as 0.10 eV, suggesting that such a small ΔEH is not a crucial limitation in realizing high performance of NIR non-fullerene based OPVs. Our results indicated that BT-IC is an interesting NIR non-fullerene acceptor with great potential application in tandem/multi-junction, semitransparent, and ternary blend solar cells.",what Mobility ?,Electron,,False,False
"The development of multidrug resistance (due to drug efflux by P-glycoproteins) is a major drawback with the use of paclitaxel (PTX) in the treatment of cancer. The rationale behind this study is to prepare PTX nanoparticles (NPs) for the reversal of multidrug resistance based on the fact that PTX loaded into NPs is not recognized by P-glycoproteins and hence is not effluxed out of the cell. Also, the intracellular penetration of the NPs could be enhanced by anchoring transferrin (Tf) on the PTX-PLGA-NPs. PTX-loaded PLGA NPs (PTX-PLGA-NPs), Pluronic®P85-coated PLGA NPs (P85-PTX-PLGA-NPs), and Tf-anchored PLGA NPs (Tf-PTX-PLGA-NPs) were prepared and evaluted for cytotoxicity and intracellular uptake using C6 rat glioma cell line. A significant increase in cytotoxicity was observed in the order of Tf-PTX-PLGA-NPs > P85-PTX-PLGA-NPs > PTX-PLGA-NPs in comparison to drug solution. In vivo biodistribution on male Sprague–Dawley rats bearing C6 glioma (subcutaneous) showed higher tumor PTX concentrations in animals administered with PTX-NPs compared to drug solution.",what Polymer ?,Transferrin (Tf),"paclitaxel ( ptx ) in the treatment of cancer. the rationale behind this study is to prepare ptx nanoparticles ( nps ) for the reversal of multidrug resistance based on the fact that ptx loaded into nps is not recognized by p - glycoproteins and hence is not effluxed out of the cell. also, the intracellular penetration of the nps could be enhanced by anchoring transferrin ( tf ) on the ptx - plga - nps. ptx - loaded plga nps ( ptx - plga - nps ), pluronic®p85 -",False,False
"AIM Drug targeting to the CNS is challenging due to the presence of blood-brain barrier. We investigated chitosan (Cs) nanoparticles (NPs) as drug transporter system across the blood-brain barrier, based on mAb OX26 modified Cs. MATERIALS & METHODS Cs NPs functionalized with PEG, modified and unmodified with OX26 (Cs-PEG-OX26) were prepared and chemico-physically characterized. These NPs were administered (intraperitoneal) in mice to define their ability to reach the brain. RESULTS Brain uptake of OX26-conjugated NPs is much higher than of unmodified NPs, because: long-circulating abilities (conferred by PEG), interaction between cationic Cs and brain endothelium negative charges and OX26 TfR receptor affinity. CONCLUSION Cs-PEG-OX26 NPs are promising drug delivery system to the CNS.",what Polymer ?,Chitosan,ox26,False,False
"PurposeTo develop a novel nanoparticle drug delivery system consisting of chitosan and glyceryl monooleate (GMO) for the delivery of a wide variety of therapeutics including paclitaxel.MethodsChitosan/GMO nanoparticles were prepared by multiple emulsion (o/w/o) solvent evaporation methods. Particle size and surface charge were determined. The morphological characteristics and cellular adhesion were evaluated with surface or transmission electron microscopy methods. The drug loading, encapsulation efficiency, in vitro release and cellular uptake were determined using HPLC methods. The safety and efficacy were evaluated by MTT cytotoxicity assay in human breast cancer cells (MDA-MB-231).ResultsThese studies provide conceptual proof that chitosan/GMO can form polycationic nano-sized particles (400 to 700 nm). The formulation demonstrates high yields (98 to 100%) and similar entrapment efficiencies. The lyophilized powder can be stored and easily be resuspended in an aqueous matrix. The nanoparticles have a hydrophobic inner-core with a hydrophilic coating that exhibits a significant positive charge and sustained release characteristics. This novel nanoparticle formulation shows evidence of mucoadhesive properties; a fourfold increased cellular uptake and a 1000-fold reduction in the IC50 of PTX.ConclusionThese advantages allow lower doses of PTX to achieve a therapeutic effect, thus presumably minimizing the adverse side effects.",what Polymer ?,Chitosan,,False,False
"Beech lignin was oxidatively cleaved in ionic liquids to give phenols, unsaturated propylaromatics, and aromatic aldehydes. A multiparallel batch reactor system was used to screen different ionic liquids and metal catalysts. Mn(NO(3))(2) in 1-ethyl-3-methylimidazolium trifluoromethanesulfonate [EMIM][CF(3)SO(3)] proved to be the most effective reaction system. A larger scale batch reaction with this system in a 300 mL autoclave (11 g lignin starting material) resulted in a maximum conversion of 66.3 % (24 h at 100 degrees C, 84x10(5) Pa air). By adjusting the reaction conditions and catalyst loading, the selectivity of the process could be shifted from syringaldehyde as the predominant product to 2,6-dimethoxy-1,4-benzoquinone (DMBQ). Surprisingly, the latter could be isolated as a pure substance in 11.5 wt % overall yield by a simple extraction/crystallization process.",what Product ?,Aromatic aldehydes,"phenols, unsaturated propylaromatics, and aromatic aldehydes.",False,True
"Gold nanoparticles on a number of supporting materials, including anatase TiO2 (TiO2-A, in 40 nm and 45 μm), rutile TiO2 (TiO2-R), ZrO2, Al2O3, SiO2 , and activated carbon, were evaluated for hydrodeoxygenation of guaiacol in 6.5 MPa initial H2 pressure at 300 °C. The presence of gold nanoparticles on the supports did not show distinguishable performance compared to that of the supports alone in the conversion level and in the product distribution, except for that on a TiO2-A-40 nm. The lack of marked catalytic activity on supports other than TiO2-A-40 nm suggests that Au nanoparticles are not catalytically active on these supports. Most strikingly, the gold nanoparticles on the least-active TiO2-A-40 nm support stood out as the best catalyst exhibiting high activity with excellent stability and remarkable selectivity to phenolics from guaiacol hydrodeoxygenation. The conversion of guaiacol (∼43.1%) over gold on the TiO2-A-40 nm was about 33 times that (1.3%) over the TiO2-A-40 nm alone. The selectivity o...",what Product ?,phenol,,False,False
"Biaryl scaffolds were constructed via Ni-catalyzed aryl C-O activation by avoiding cleavage of the more reactive acyl C-O bond of aryl carboxylates. Now aryl esters, in general, can be successfully employed in cross-coupling reactions for the first time. The substrate scope and synthetic utility of the chemistry were demonstrated by the syntheses of more than 40 biaryls and by constructing complex organic molecules. Water was observed to play an important role in facilitating this transformation.",what Product ?,Biaryl,water,False,False
"A considerable effort has been made to extract biological and chemical entities, as well as their relationships, from the scientific literature, either manually through traditional literature curation or by using information extraction and text mining technologies. Medicinal chemistry patents contain a wealth of information, for instance to uncover potential biomarkers that might play a role in cancer treatment and prognosis. However, current biomedical annotation databases do not cover such information, partly due to limitations of publicly available biomedical patent mining software. As part of the BioCreative V CHEMDNER patents track, we present the results of the first named entity recognition (NER) assignment carried out to detect mentions of chemical compounds and genes/proteins in running patent text. More specifically, this task aimed to evaluate the performance of automatic name recognition strategies capable of isolating chemical names and gene and gene product mentions from surrounding text within patent titles and abstracts. A total of 22 unique teams submitted results for at least one of the three CHEMDNER subtasks. The first subtask, called the CEMP (chemical entity mention in patents) task, focused on the detection of chemical named entity mentions in patents, requesting teams to return the start and end indices corresponding to all the chemical entities found in a given record. A total of 21 teams submitted 93 runs, for this subtask. The top performing team reached an f-measure of 0.89 with a precision of 0.87 and a recall of 0.91. The CPD (chemical passage detection) task required the classification of patent titles and abstracts whether they do or do not contain chemical compound mentions. Nine teams returned predictions for this task (40 runs). The top run in terms of Matthew’s correlation coefficient (MCC) had a score of 0.88, the highest sensitivity ? Corresponding author",what Evaluation metrics ?,Recall,precision,False,False
"A considerable effort has been made to extract biological and chemical entities, as well as their relationships, from the scientific literature, either manually through traditional literature curation or by using information extraction and text mining technologies. Medicinal chemistry patents contain a wealth of information, for instance to uncover potential biomarkers that might play a role in cancer treatment and prognosis. However, current biomedical annotation databases do not cover such information, partly due to limitations of publicly available biomedical patent mining software. As part of the BioCreative V CHEMDNER patents track, we present the results of the first named entity recognition (NER) assignment carried out to detect mentions of chemical compounds and genes/proteins in running patent text. More specifically, this task aimed to evaluate the performance of automatic name recognition strategies capable of isolating chemical names and gene and gene product mentions from surrounding text within patent titles and abstracts. A total of 22 unique teams submitted results for at least one of the three CHEMDNER subtasks. The first subtask, called the CEMP (chemical entity mention in patents) task, focused on the detection of chemical named entity mentions in patents, requesting teams to return the start and end indices corresponding to all the chemical entities found in a given record. A total of 21 teams submitted 93 runs, for this subtask. The top performing team reached an f-measure of 0.89 with a precision of 0.87 and a recall of 0.91. The CPD (chemical passage detection) task required the classification of patent titles and abstracts whether they do or do not contain chemical compound mentions. Nine teams returned predictions for this task (40 runs). The top run in terms of Matthew’s correlation coefficient (MCC) had a score of 0.88, the highest sensitivity ? Corresponding author",what Evaluation metrics ?,Precision,precision,True,True
"The BioCreative LitCovid track calls for a community effort to tackle automated topic annotation for COVID-19 literature. The number of COVID-19-related articles in the literature is growing by about 10,000 articles per month, significantly challenging curation efforts and downstream interpretation. LitCovid is a literature database of COVID-19related articles in PubMed, which has accumulated more than 180,000 articles with millions of accesses each month by users worldwide. The rapid literature growth significantly increases the burden of LitCovid curation, especially for topic annotations. Topic annotation in LitCovid assigns one or more (up to eight) labels to articles. The annotated topics have been widely used both directly in LitCovid (e.g., accounting for ~20% of total uses) and downstream studies such as knowledge network generation and citation analysis. It is, therefore, important to develop innovative text mining methods to tackle the challenge. We organized the BioCreative LitCovid track to call for a community effort to tackle automated topic annotation for COVID-19 literature. This article summarizes the BioCreative LitCovid track in terms of data collection and team participation. The dataset is publicly available via https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/biocreative/. It consists of over 30K PubMed articles, one of the largest multilabel classification datasets on biomedical literature. There were 80 submissions in total from 19 teams worldwide. The highestperforming submissions achieved 0.8875, 0.9181, and 0.9394 for macro F1-score, micro F1-score, and instance-based F1-score, respectively. We look forward to further participation in developing biomedical text mining methods in response to the rapid growth of the COVID-19 literature. Keywords—biomedical text mining; natural language processing; artificial intelligence; machine learning; deep learning; multi-label classification; COVID-19; LitCovid;",what Evaluation metrics ?,Macro F1,downstream,False,False
"Abstract Background Since the onset of the pandemic, only few studies focused on longitudinal immune monitoring in critically ill COVID-19 patients with acute respiratory distress syndrome (ARDS) whereas their hospital stay may last for several weeks. Consequently, the question of whether immune parameters may drive or associate with delayed unfavorable outcome in these critically ill patients remains unsolved. Methods We present a dynamic description of immuno-inflammatory derangements in 64 critically ill COVID-19 patients including plasma IFNα2 levels and IFN-stimulated genes (ISG) score measurements. Results ARDS patients presented with persistently decreased lymphocyte count and mHLA-DR expression and increased cytokine levels. Type-I IFN response was initially induced with elevation of IFNα2 levels and ISG score followed by a rapid decrease over time. Survivors and non-survivors presented with apparent common immune responses over the first 3 weeks after ICU admission mixing gradual return to normal values of cellular markers and progressive decrease of cytokines levels including IFNα2. Only plasma TNF-α presented with a slow increase over time and higher values in non-survivors compared with survivors. This paralleled with an extremely high occurrence of secondary infections in COVID-19 patients with ARDS. Conclusions Occurrence of ARDS in response to SARS-CoV2 infection appears to be strongly associated with the intensity of immune alterations upon ICU admission of COVID-19 patients. In these critically ill patients, immune profile presents with similarities with the delayed step of immunosuppression described in bacterial sepsis.",what Study population ?,Critically ill COVID-19 patients,covid - 19 patients,False,False
00-5712/$ see front matter q 200 i:10.1016/j.jdent.2004.08.007 * Corresponding author. Tel.: C44 7 5 1282. E-mail address: r.bedi@eastman.uc Summary Objective. To describe the prevalence of dental erosion and associated factors in preschool children in Guangxi and Hubei provinces of China. Methods. Dental examinations were carried out on 1949 children aged 3–5 years. Measurement of erosion was confined to primary maxillary incisors. The erosion index used was based upon the 1993 UK National Survey of Children’s Dental Health. The children’s general information as well as social background and dietary habits were collected based on a structured questionnaire. Results. A total of 112 children (5.7%) showed erosion on their maxillary incisors. Ninety-five (4.9%) was scored as being confined to enamel and 17 (0.9%) as erosion extending into dentine or pulp. There was a positive association between erosion and social class in terms of parental education. A significantly higher prevalence of erosion was observed in children whose parents had post-secondary education than those whose parents had secondary or lower level of education. There was also a correlation between the presence of dental erosion and intake of fruit drink from a feeding bottle or consumption of fruit drinks at bedtime. Conclusion. Erosion is not a serious problem for dental heath in Chinese preschool children. The prevalence of erosion is associated with social and dietary factors in this sample of children. q 2004 Elsevier Ltd. All rights reserved.,what Study population ?,Preschool children,preschool children,True,True
"Acidic soft drinks, including sports drinks, have been implicated in dental erosion with limited supporting data in scarce erosion studies worldwide. The purpose of this study was to determine the prevalence of dental erosion in a sample of athletes at a large Midwestern state university in the USA, and to evaluate whether regular consumption of sports drinks was associated with dental erosion. A cross-sectional, observational study was done using a convenience sample of 304 athletes, selected irrespective of sports drinks usage. The Lussi Index was used in a blinded clinical examination to grade the frequency and severity of erosion of all tooth surfaces excluding third molars and incisal surfaces of anterior teeth. A self-administered questionnaire was used to gather details on sports drink usage, lifestyle, health problems, dietary and oral health habits. Intraoral color slides were taken of all teeth with erosion. Sports drinks usage was found in 91.8% athletes and the total prevalence of erosion was 36.5%. Nonparametric tests and stepwise regression analysis using history variables showed no association between dental erosion and the use of sports drinks, quantity and frequency of consumption, years of usage and nonsport usage of sports drinks. The most significant predictor of erosion was found to be not belonging to the African race (p < 0.0001). The results of this study reveal no relationship between consumption of sports drinks and dental erosion.",what Study population ?,Athletes,athletes,True,True
"In this paper, we examine the emerging use of ICT in social phenomena such as natural disasters. Researchers have acknowledged that a community possesses the capacity to manage the challenges in crisis response on its own. However, extant IS studies focus predominantly on IS use from the crisis response agency’s perspective, which undermines communities’ role. By adopting an empowerment perspective, we focus on understanding how social media empowers communities during crisis response. As such, we present a qualitative case study of the 2011 Thailand flooding. Using an interpretive approach, we show how social media can empower the community from three dimensions of empowerment process (structural, psychological, and resource empowerment) to achieve collective participation, shared identification, and collaborative control in the community. We make two contributions: 1) we explore an emerging social consequence of ICT by illustrating the roles of social media in empowering communities when responding to crises, and 2) we address the literature gap in empowerment by elucidating the actualization process of empowerment that social media as a mediating structure enables.",what Emergency Type ?,Flood,,False,False
"Social media for emergency management has emerged as a vital resource for government agencies across the globe. In this study, we explore social media strategies employed by governments to respond to major weather-related events. Using social media monitoring software, we analyze how social media is used in six cities following storms in the winter of 2012. We listen, monitor, and assess online discourse available on the full range of social media outlets (e.g., Twitter, Facebook, blogs). To glean further insight, we conduct a survey and extract themes from citizen comments and government's response. We conclude with recommendations on how practitioners can develop social media strategies that enable citizen participation in emergency management.",what Emergency Type ?,weather-related events,storms,False,False
"Two weeks after the Great Tohoku earthquake followed by the devastating tsunami, we have sent open-ended questionnaires to a randomly selected sample of Twitter users and also analysed the tweets sent from the disaster-hit areas. We found that people in directly affected areas tend to tweet about their unsafe and uncertain situation while people in remote areas post messages to let their followers know that they are safe. Our analysis of the open-ended answers has revealed that unreliable retweets (RTs) on Twitter was the biggest problem the users have faced during the disaster. Some of the solutions offered by the respondents included introducing official hash tags, limiting the number of RTs for each hash tag and adding features that allow users to trace information by maintaining anonymity.",what Emergency Type ?,Tsunami,"tsunami,",True,True
"We present a novel approach to localizing parts in images of human faces. The approach combines the output of local detectors with a nonparametric set of global models for the part locations based on over 1,000 hand-labeled exemplar images. By assuming that the global models generate the part locations as hidden variables, we derive a Bayesian objective function. This function is optimized using a consensus of models for these hidden variables. The resulting localizer handles a much wider range of expression, pose, lighting, and occlusion than prior ones. We show excellent performance on real-world face datasets such as Labeled Faces in the Wild (LFW) and a new Labeled Face Parts in the Wild (LFPW) and show that our localizer achieves state-of-the-art performance on the less challenging BioID dataset.",what Variations ?,occlusion,"expression, pose,",False,False
"Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since they fail to provide a principled way of handling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR's performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.",what Variations ?,expression,"pose,",False,False
"Face alignment is a crucial step in face recognition tasks. Especially, using landmark localization for geometric face normalization has shown to be very effective, clearly improving the recognition results. However, no adequate databases exist that provide a sufficient number of annotated facial landmarks. The databases are either limited to frontal views, provide only a small number of annotated images or have been acquired under controlled conditions. Hence, we introduce a novel database overcoming these limitations: Annotated Facial Landmarks in the Wild (AFLW). AFLW provides a large-scale collection of images gathered from Flickr, exhibiting a large variety in face appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. In total 25,993 faces in 21,997 real-world images are annotated with up to 21 landmarks per image. Due to the comprehensive set of annotations AFLW is well suited to train and test algorithms for multi-view face detection, facial landmark localization and face pose estimation. Further, we offer a rich set of tools that ease the integration of other face databases and associated annotations into our joint framework.",what Variations ?,expression,"expression, ethnicity,",False,True
"We present a novel approach to localizing parts in images of human faces. The approach combines the output of local detectors with a nonparametric set of global models for the part locations based on over 1,000 hand-labeled exemplar images. By assuming that the global models generate the part locations as hidden variables, we derive a Bayesian objective function. This function is optimized using a consensus of models for these hidden variables. The resulting localizer handles a much wider range of expression, pose, lighting, and occlusion than prior ones. We show excellent performance on real-world face datasets such as Labeled Faces in the Wild (LFW) and a new Labeled Face Parts in the Wild (LFPW) and show that our localizer achieves state-of-the-art performance on the less challenging BioID dataset.",what Variations ?,pose,"expression, pose,",False,True
"Abstract Objective: Surveillance of surgical site infections (SSIs) is important for infection control and is usually performed through retrospective manual chart review. The aim of this study was to develop an algorithm for the surveillance of deep SSIs based on clinical variables to enhance efficiency of surveillance. Design: Retrospective cohort study (2012–2015). Setting: A Dutch teaching hospital. Participants: We included all consecutive patients who underwent colorectal surgery excluding those with contaminated wounds at the time of surgery. All patients were evaluated for deep SSIs through manual chart review, using the Centers for Disease Control and Prevention (CDC) criteria as the reference standard. Analysis: We used logistic regression modeling to identify predictors that contributed to the estimation of diagnostic probability. Bootstrapping was applied to increase generalizability, followed by assessment of statistical performance and clinical implications. Results: In total, 1,606 patients were included, of whom 129 (8.0%) acquired a deep SSI. The final model included postoperative length of stay, wound class, readmission, reoperation, and 30-day mortality. The model achieved 68.7% specificity and 98.5% sensitivity and an area under the receiver operator characteristic (ROC) curve (AUC) of 0.950 (95% CI, 0.932–0.969). Positive and negative predictive values were 21.5% and 99.8%, respectively. Applying the algorithm resulted in a 63.4% reduction in the number of records requiring full manual review (from 1,606 to 590). Conclusions: This 5-parameter model identified 98.5% of patients with a deep SSI. The model can be used to develop semiautomatic surveillance of deep SSIs after colorectal surgery, which may further improve efficiency and quality of SSI surveillance.",what Infection ?,Surgical Site Infection,surgical site infections,False,True
"Objective. A major challenge in treating Clostridium difficile infection (CDI) is relapse. Many new therapies are being developed to help prevent this outcome. We sought to establish risk factors for relapse and determine whether fields available in an electronic health record (EHR) could be used to identify high-risk patients for targeted relapse prevention strategies. Design. Retrospective cohort study. Setting. Large clinical data warehouse at a 4-hospital healthcare organization. Participants. Data were gathered from January 2006 through October 2010. Subjects were all inpatient episodes of a positive C. difficile test where patients were available for 56 days of follow-up. Methods. Relapse was defined as another positive test between 15 and 56 days after the initial test. Multivariable regression was performed to identify factors independently associated with CDI relapse. Results. Eight hundred twenty-nine episodes met eligibility criteria, and 198 resulted in relapse (23.9%). In the final multivariable analysis, risk of relapse was associated with age (odds ratio [OR], 1.02 per year [95% confidence interval (CI), 1.01–1.03]), fluoroquinolone exposure in the 90 days before diagnosis (OR, 1.58 [95% CI, 1.11–2.26]), intensive care unit stay in the 30 days before diagnosis (OR, 0.47 [95% CI, 0.30–0.75]), cephalosporin (OR, 1.80 [95% CI, 1.19–2.71]), proton pump inhibitor (PPI; OR, 1.55 [95% CI, 1.05–2.29]), and metronidazole exposure after diagnosis (OR, 2.74 [95% CI, 1.64–4.60]). A prediction model tuned to ensure a 50% probability of relapse would flag 14.6% of CDI episodes. Conclusions. Data from a comprehensive EHR can be used to identify patients at high risk for CDI relapse. Major risk factors include antibiotic and PPI exposure.",what Infection ?,Clostridium difficile infection,clostridium difficile infection,True,True
"<jats:p>As part of a data mining competition, a training and test set of laboratory test data about patients with and without surgical site infection (SSI) were provided. The task was to develop predictive models with training set and identify patients with SSI in the no label test set. Lab test results are vital resources that guide healthcare providers make decisions about all aspects of surgical patient management. Many machine learning models were developed after pre-processing and imputing the lab tests data and only the top performing methods are discussed. Overall, RANDOM FOREST algorithms performed better than Support Vector Machine and Logistic Regression. Using a set of 74 lab tests, with RF, there were only 4 false positives in the training set and predicted 35 out of 50 SSI patients in the test set (Accuracy 0.86, Sensitivity 0.68, and Specificity 0.91). Optimal ways to address healthcare data quality concerns and imputation methods as well as newer generalizable algorithms need to be explored further to decipher new associations and knowledge among laboratory biomarkers and SSI.</jats:p>",what Infection ?,Surgical Site Infection,surgical site infection,True,True
"Sepsis, a dysregulated host response to infection, is a major health burden in terms of both mortality and cost. The difficulties clinicians face in diagnosing sepsis, alongside the insufficiencies of diagnostic biomarkers, motivate the present study. This work develops a machine-learning-based sepsis diagnostic for a high-risk patient group, using a geographically and institutionally diverse collection of nearly 500,000 patient health records. Using only a minimal set of clinical variables, our diagnostics outperform common severity scoring systems and sepsis biomarkers and benefit from being available immediately upon ordering.",what Infection ?,Sepsis,"sepsis,",True,True
"Background and Aims Prediction of severe clinical outcomes in Clostridium difficile infection (CDI) is important to inform management decisions for optimum patient care. Currently, treatment recommendations for CDI vary based on disease severity but validated methods to predict severe disease are lacking. The aim of the study was to derive and validate a clinical prediction tool for severe outcomes in CDI. Methods A cohort totaling 638 patients with CDI was prospectively studied at three tertiary care clinical sites (Boston, Dublin and Houston). The clinical prediction rule (CPR) was developed by multivariate logistic regression analysis using the Boston cohort and the performance of this model was then evaluated in the combined Houston and Dublin cohorts. Results The CPR included the following three binary variables: age ≥ 65 years, peak serum creatinine ≥2 mg/dL and peak peripheral blood leukocyte count of ≥20,000 cells/μL. The Clostridium difficile severity score (CDSS) correctly classified 76.5% (95% CI: 70.87-81.31) and 72.5% (95% CI: 67.52-76.91) of patients in the derivation and validation cohorts, respectively. In the validation cohort, CDSS scores of 0, 1, 2 or 3 were associated with severe clinical outcomes of CDI in 4.7%, 13.8%, 33.3% and 40.0% of cases respectively. Conclusions We prospectively derived and validated a clinical prediction rule for severe CDI that is simple, reliable and accurate and can be used to identify high-risk patients most likely to benefit from measures to prevent complications of CDI.",what Infection ?,Clostridium difficile infection,clostridium difficile infection,True,True
"Intensive Care Unit (ICU) patients have significant morbidity and mortality, often from complications that arise during the hospital stay. Severe sepsis is one of the leading causes of death among these patients. Predictive models have the potential to allow for earlier detection of severe sepsis and ultimately earlier intervention. However, current methods for identifying and predicting severe sepsis are biased and inadequate. The goal of this work is to identify a new framework for the prediction of severe sepsis and identify early predictors utilizing clinical laboratory values and vital signs collected in adult ICU patients. We explore models with logistic regression (LR), support vector machines (SVM), and logistic model trees (LMT) utilizing vital signs, laboratory values, or a combination of vital and laboratory values. When applied to a retrospective cohort of ICU patients, the SVM model using laboratory and vital signs as predictors identified 339 (65%) of the 3,446 patients as developing severe sepsis correctly. Based on this new framework and developed models, we provide a recommendation for the use in clinical decision support in ICU and non-ICU environments.",what Infection ?,Sepsis,severe sepsis,False,True
"In recent times, social media has been increasingly playing a critical role in response actions following natural catastrophes. From facilitating the recruitment of volunteers during an earthquake to supporting emotional recovery after a hurricane, social media has demonstrated its power in serving as an effective disaster response platform. Based on a case study of Thailand flooding in 2011 – one of the worst flooding disasters in more than 50 years that left the country severely impaired – this paper provides an in‐depth understanding on the emergent roles of social media in disaster response. Employing the perspective of boundary object, we shed light on how different boundary spanning competences of social media emerged in practice to facilitate cross‐boundary response actions during a disaster, with an aim to promote further research in this area. We conclude this paper with guidelines for response agencies and impacted communities to deploy social media for future disaster response.",what Emergency Management Phase ?,response,earthquake,False,False
"Social media for emergency management has emerged as a vital resource for government agencies across the globe. In this study, we explore social media strategies employed by governments to respond to major weather-related events. Using social media monitoring software, we analyze how social media is used in six cities following storms in the winter of 2012. We listen, monitor, and assess online discourse available on the full range of social media outlets (e.g., Twitter, Facebook, blogs). To glean further insight, we conduct a survey and extract themes from citizen comments and government's response. We conclude with recommendations on how practitioners can develop social media strategies that enable citizen participation in emergency management.",what Emergency Management Phase ?,response,,False,False
"ABSTRACT This paper systematically develops a set of general and supporting design principles and specifications for a ""Dynamic Emergency Response Management Information System"" (DERMIS) by identifying design premises resulting from the use of the ""Emergency Management Information System and Reference Index"" (EMISARI) and design concepts resulting from a comprehensive literature review. Implicit in crises of varying scopes and proportions are communication and information needs that can be addressed by today's information and communication technologies. However, what is required is organizing the premises and concepts that can be mapped into a set of generic design principles in turn providing a framework for the sensible development of flexible and dynamic Emergency Response Information Systems. A framework is presented for the system design and development that addresses the communication and information needs of first responders as well as the decision making needs of command and control personnel. The framework also incorporates thinking about the value of insights and information from communities of geographically dispersed experts and suggests how that expertise can be brought to bear on crisis decision making. Historic experience is used to suggest nine design premises. These premises are complemented by a series of five design concepts based upon the review of pertinent and applicable research. The result is a set of eight general design principles and three supporting design considerations that are recommended to be woven into the detailed specifications of a DERMIS. The resulting DERMIS design model graphically indicates the heuristic taken by this paper and suggests that the result will be an emergency response system flexible, robust, and dynamic enough to support the communication and information needs of emergency and crisis personnel on all levels. In addition it permits the development of dynamic emergency response information systems with tailored flexibility to support and be integrated across different sizes and types of organizations. This paper provides guidelines for system analysts and designers, system engineers, first responders, communities of experts, emergency command and control personnel, and MIS/IT researchers. SECTIONS 1. Introduction 2. Historical Insights about EMISARI 3. The emergency Response Atmosphere of OEP 4. Resulting Requirements for Emergency Response and Conceptual Design Specifics 4.1 Metaphors 4.2 Roles 4.3 Notifications 4.4 Context Visibility 4.5 Hypertext 5. Generalized Design Principles 6. Supporting Design Considerations 6.1 Resource Databases and Community Collaboration 6.2 Collective Memory 6.3 Online Communities of Experts 7. Conclusions and Final Observations 8. References 1. INTRODUCTION There have been, since 9/11, considerable efforts to propose improvements in the ability to respond to emergencies. However, the vast majority of these efforts have concentrated on infrastructure improvements to aid in mitigation of the impacts of either a man-made or natural disaster. In the area of communication and information systems to support the actual ongoing reaction to a disaster situation, the vast majority of the efforts have focused on the underlying technology to reliably support survivability of the underlying networks and physical facilities (Kunreuther and LernerLam 2002; Mork 2002). The fact that there were major failures of the basic technology and loss of the command center for 48 hours in the 9/11 event has made this an understandable result. The very workable commercial paging and digital mail systems supplied immediately afterwards by commercial firms (Michaels 2001; Vatis 2002) to the emergency response workers demonstrated that the correction of underlying technology is largely a process of setting integration standards and deciding to spend the necessary funds to update antiquated systems. …",what Emergency Management Phase ?,Response,emergency response,False,True
"<jats:p><jats:italic>Objective</jats:italic>. Achieving accurate prediction of sepsis detection moment based on bedside monitor data in the intensive care unit (ICU). A good clinical outcome is more probable when onset is suspected and treated on time, thus early insight of sepsis onset may save lives and reduce costs. <jats:italic>Methodology</jats:italic>. We present a novel approach for feature extraction, which focuses on the hypothesis that unstable patients are more prone to develop sepsis during ICU stay. These features are used in machine learning algorithms to provide a prediction of a patient’s likelihood to develop sepsis during ICU stay, hours before it is diagnosed. <jats:italic>Results</jats:italic>. Five machine learning algorithms were implemented using R software packages. The algorithms were trained and tested with a set of 4 features which represent the variability in vital signs. These algorithms aimed to calculate a patient’s probability to become septic within the next 4 hours, based on recordings from the last 8 hours. The best area under the curve (AUC) was achieved with Support Vector Machine (SVM) with radial basis function, which was 88.38%. <jats:italic>Conclusions</jats:italic>. The high level of predictive accuracy along with the simplicity and availability of input variables present great potential if applied in ICUs. Variability of a patient’s vital signs proves to be a good indicator of one’s chance to become septic during ICU stay.</jats:p>",what Objective ?,Sepsis,sepsis detection moment,False,True
"A new glucose-insulin model is introduced which fits with the clinical data from in- and outpatients for two days. Its stability property is consistent with the glycemia behavior for type 1 diabetes. This is in contrast to traditional glucose-insulin models. Prior models fit with clinical data for a few hours only or display some nonnatural equilibria. The parameters of this new model are identifiable from standard clinical data as continuous glucose monitoring, insulin injection, and carbohydrate estimate. Moreover, it is shown that the parameters from the model allow the computation of the standard tools used in functional insulin therapy as the basal rate of insulin and the insulin sensitivity factor. This is a major outcome as they are required in therapeutic education of type 1 diabetic patients.",what Objective ?,For two days,,False,False
"Early pathogen exposure detection allows better patient care and faster implementation of public health measures (patient isolation, contact tracing). Existing exposure detection most frequently relies on overt clinical symptoms, namely fever, during the infectious prodromal period. We have developed a robust machine learning based method to better detect asymptomatic states during the incubation period using subtle, sub-clinical physiological markers. Starting with high-resolution physiological waveform data from non-human primate studies of viral (Ebola, Marburg, Lassa, and Nipah viruses) and bacterial (Y. pestis) exposure, we processed the data to reduce short-term variability and normalize diurnal variations, then provided these to a supervised random forest classification algorithm and post-classifier declaration logic step to reduce false alarms. In most subjects detection is achieved well before the onset of fever; subject cross-validation across exposure studies (varying viruses, exposure routes, animal species, and target dose) lead to 51h mean early detection (at 0.93 area under the receiver-operating characteristic curve [AUCROC]). Evaluating the algorithm against entirely independent datasets for Lassa, Nipah, and Y. pestis exposures un-used in algorithm training and development yields a mean 51h early warning time (at AUCROC=0.95). We discuss which physiological indicators are most informative for early detection and options for extending this capability to limited datasets such as those available from wearable, non-invasive, ECG-based sensors.",what Objective ?,Early warning,early warning time ( at aucroc = 0. 95 ),False,True
"Abstract Background Data papers have emerged as a powerful instrument for open data publishing, obtaining credit, and establishing priority for datasets generated in scientific experiments. Academic publishing improves data and metadata quality through peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. Objective We aimed to establish a new type of article structure and template for omics studies: the omics data paper. To improve data interoperability and further incentivize researchers to publish well-described datasets, we created a prototype workflow for streamlined import of genomics metadata from the European Nucleotide Archive directly into a data paper manuscript. Methods An omics data paper template was designed by defining key article sections that encourage the description of omics datasets and methodologies. A metadata import workflow, based on REpresentational State Transfer services and Xpath, was prototyped to extract information from the European Nucleotide Archive, ArrayExpress, and BioSamples databases. Findings The template and workflow for automatic import of standard-compliant metadata into an omics data paper manuscript provide a mechanism for enhancing existing metadata through publishing. Conclusion The omics data paper structure and workflow for import of genomics metadata will help to bring genomic and other omics datasets into the spotlight. Promoting enhanced metadata descriptions and enforcing manuscript peer review and data auditing of the underlying datasets brings additional quality to datasets. We hope that streamlined metadata reuse for scholarly publishing encourages authors to create enhanced metadata descriptions in the form of data papers to improve both the quality of their metadata and its findability and accessibility.",what Process ?,manuscript peer review,"peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. objective we aimed to establish a new type of article structure and template for omics studies : the omics data paper. to improve data interoperability and further incentivize researchers to publish well - described datasets, we created a prototype workflow",False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Process ?,Network-based Co-operative Planning Processes,,False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Process ?,Co-operative Building Planning,,False,False
"The molecular chaperone Hsp90-dependent proteome represents a complex protein network of critical biological and medical relevance. Known to associate with proteins with a broad variety of functions termed clients, Hsp90 maintains key essential and oncogenic signalling pathways. Consequently, Hsp90 inhibitors are being tested as anti-cancer drugs. Using an integrated systematic approach to analyse the effects of Hsp90 inhibition in T-cells, we quantified differential changes in the Hsp90-dependent proteome, Hsp90 interactome, and a selection of the transcriptome. Kinetic behaviours in the Hsp90-dependent proteome were assessed using a novel pulse-chase strategy (Fierro-Monti et al., accompanying article), detecting effects on both protein stability and synthesis. Global and specific dynamic impacts, including proteostatic responses, are due to direct inhibition of Hsp90 as well as indirect effects. As a result, a decrease was detected in most proteins that changed their levels, including known Hsp90 clients. Most likely, consequences of the role of Hsp90 in gene expression determined a global reduction in net de novo protein synthesis. This decrease appeared to be greater in magnitude than a concomitantly observed global increase in protein decay rates. Several novel putative Hsp90 clients were validated, and interestingly, protein families with critical functions, particularly the Hsp90 family and cofactors themselves as well as protein kinases, displayed strongly increased decay rates due to Hsp90 inhibitor treatment. Remarkably, an upsurge in survival pathways, involving molecular chaperones and several oncoproteins, and decreased levels of some tumour suppressors, have implications for anti-cancer therapy with Hsp90 inhibitors. The diversity of global effects may represent a paradigm of mechanisms that are operating to shield cells from proteotoxic stress, by promoting pro-survival and anti-proliferative functions. Data are available via ProteomeXchange with identifier PXD000537.",what Process ?,net de novo protein synthesis,inhibition,False,False
"Abstract Background Data papers have emerged as a powerful instrument for open data publishing, obtaining credit, and establishing priority for datasets generated in scientific experiments. Academic publishing improves data and metadata quality through peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. Objective We aimed to establish a new type of article structure and template for omics studies: the omics data paper. To improve data interoperability and further incentivize researchers to publish well-described datasets, we created a prototype workflow for streamlined import of genomics metadata from the European Nucleotide Archive directly into a data paper manuscript. Methods An omics data paper template was designed by defining key article sections that encourage the description of omics datasets and methodologies. A metadata import workflow, based on REpresentational State Transfer services and Xpath, was prototyped to extract information from the European Nucleotide Archive, ArrayExpress, and BioSamples databases. Findings The template and workflow for automatic import of standard-compliant metadata into an omics data paper manuscript provide a mechanism for enhancing existing metadata through publishing. Conclusion The omics data paper structure and workflow for import of genomics metadata will help to bring genomic and other omics datasets into the spotlight. Promoting enhanced metadata descriptions and enforcing manuscript peer review and data auditing of the underlying datasets brings additional quality to datasets. We hope that streamlined metadata reuse for scholarly publishing encourages authors to create enhanced metadata descriptions in the form of data papers to improve both the quality of their metadata and its findability and accessibility.",what Process ?,streamlined import,"peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. objective we aimed to establish a new type of article structure and template for omics studies : the omics data paper. to improve data interoperability and further incentivize researchers to publish well - described datasets, we created a prototype workflow",False,False
"Purpose – The purpose of this paper is to investigate what employers seek when recruiting library and information professionals in the UK and whether professional skills, generic skills or personal qualities are most in demand.Design/methodology/approach – A content analysis of a sample of 180 advertisements requiring a professional library or information qualification from Chartered Institute of Library and Information Professional's Library + Information Gazette over the period May 2006‐2007.Findings – The findings reveal that a multitude of skills and qualities are required in the profession. When the results were compared with Information National Training Organisation and Library and Information Management Employability Skills research, customer service, interpersonal and communication skills, and general computing skills emerged as the requirements most frequently sought by employers. Overall, requirements from the generic skills area were most important to employers, but the research also demonstra...",what Process ?,recruiting library and information professionals,content analysis,False,False
"With the rapid growth of online social media content, and the impact these have made on people’s behavior, many researchers have been interested in studying these media platforms. A major part of their work focused on sentiment analysis and opinion mining. These refer to the automatic identification of opinions of people toward specific topics by analyzing their posts and publications. Multi-class sentiment analysis, in particular, addresses the identification of the exact sentiment conveyed by the user rather than the overall sentiment polarity of his text message or post. That being the case, we introduce a task different from the conventional multi-class classification, which we run on a data set collected from Twitter. We refer to this task as “quantification.” By the term “quantification,” we mean the identification of all the existing sentiments within an online post (i.e., tweet) instead of attributing a single sentiment label to it. For this sake, we propose an approach that automatically attributes different scores to each sentiment in a tweet, and selects the sentiments with the highest scores which we judge as conveyed in the text. To reach this target, we added to our previously introduced tool SENTA the necessary components to run and perform such a task. Throughout this work, we present the added components; we study the feasibility of quantification, and propose an approach to perform it on a data set made of tweets for 11 different sentiment classes. The data set was manually labeled and the results of the automatic analysis were checked against the human annotation. Our experiments show the feasibility of this task and reach an F1 score equal to 45.9%.",what Process ?,rapid growth,automatic,False,False
"Abstract Background Data papers have emerged as a powerful instrument for open data publishing, obtaining credit, and establishing priority for datasets generated in scientific experiments. Academic publishing improves data and metadata quality through peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. Objective We aimed to establish a new type of article structure and template for omics studies: the omics data paper. To improve data interoperability and further incentivize researchers to publish well-described datasets, we created a prototype workflow for streamlined import of genomics metadata from the European Nucleotide Archive directly into a data paper manuscript. Methods An omics data paper template was designed by defining key article sections that encourage the description of omics datasets and methodologies. A metadata import workflow, based on REpresentational State Transfer services and Xpath, was prototyped to extract information from the European Nucleotide Archive, ArrayExpress, and BioSamples databases. Findings The template and workflow for automatic import of standard-compliant metadata into an omics data paper manuscript provide a mechanism for enhancing existing metadata through publishing. Conclusion The omics data paper structure and workflow for import of genomics metadata will help to bring genomic and other omics datasets into the spotlight. Promoting enhanced metadata descriptions and enforcing manuscript peer review and data auditing of the underlying datasets brings additional quality to datasets. We hope that streamlined metadata reuse for scholarly publishing encourages authors to create enhanced metadata descriptions in the form of data papers to improve both the quality of their metadata and its findability and accessibility.",what Process ?,open data publishing,"peer review and increases the impact of datasets by enhancing their visibility, accessibility, and reusability. objective we aimed to establish a new type of article structure and template for omics studies : the omics data paper. to improve data interoperability and further incentivize researchers to publish well - described datasets, we created a prototype workflow",False,False
"The planning process of a building is very complex. Many participants with different technical disciplines are involved and work on certain tasks. To manage the planning process the project leader has to organize participants, tasks and building data. For this purpose modern information and communication technologies can be used very effi ciently. But these technologies require a formal description of the planning process. Within the research project “Relation Based Process Modelling of Co-operative Building Planning” we have defined a consistent mathematical process model for planning processes and have developed a prototype implementation of an application for modelling these processes. Our project is embedded in the priori ty program 1103 “Network-based Co-operative Planning Processes in Structural Engineering” promoted by the German Research Foundation (DFG). In this paper we present the mathematical concept of our relational process model and the tool for building up the m odel and checking the structural consistency and correctness.",what Process ?,Co-operative Building Planning,,False,False
"Aligning two representations of the same domain with different expressiveness is a crucial topic in nowadays semantic web and big data research. OWL ontologies and Entity Relation Diagrams are the most widespread representations whose alignment allows for semantic data access via ontology interface, and ontology storing techniques. The term """"alignment"" encompasses three different processes: OWL-to-ERD and ERD-to-OWL transformation, and OWL-ERD mapping. In this paper an innovative statistical tool is presented to accomplish all the three aspects of the alignment. The main idea relies on the use of a HMM to estimate the most likely ERD sentence that is stated in a suitable grammar, and corresponds to the observed OWL axiom. The system and its theoretical background are presented, and some experiments are reported.",what Learning method ?,Mapping,hmm,False,False
"One of the main challenges in content-based or semantic image retrieval is still to bridge the gap between low-level features and semantic information. In this paper, An approach is presented using integrated multi-level image features in ontology fusion construction by a fusion framework, which based on the latent semantic analysis. The proposed method promotes images ontology fusion efficiently and broadens the application fields of image ontology retrieval system. The relevant experiment shows that this method ameliorates the problem, such as too many redundant data and relations, in the traditional ontology system construction, as well as improves the performance of semantic images retrieval.",what Learning method ?,Ontology fusion,"ontology fusion construction by a fusion framework,",False,True
"Learning ontologies requires the acquisition of relevant domain concepts and taxonomic, as well as non-taxonomic, relations. In this chapter, we present a methodology for automatic ontology enrichment and document annotation with concepts and relations of an existing domain core ontology. Natural language definitions from available glossaries in a given domain are processed and regular expressions are applied to identify general-purpose and domain-specific relations. We evaluate the methodology performance in extracting hypernymy and non-taxonomic relations. To this end, we annotated and formalized a relevant fragment of the glossary of Art and Architecture (AAT) with a set of 10 relations (plus the hypernymy relation) defined in the CRM CIDOC cultural heritage core ontology, a recent W3C standard. Finally, we assessed the generality of the approach on a set of web pages from the domains of history and biography.",what Learning method ?,Regular expression,automatic,False,False
"With the increase in smart devices and abundance of video contents, efficient techniques for the indexing, analysis and retrieval of videos are becoming more and more desirable. Improved indexing and automated analysis of millions of videos could be accomplished by getting videos tagged automatically. A lot of existing methods fail to precisely tag videos because of their lack of ability to capture the video context. The context in a video represents the interactions of objects in a scene and their overall meaning. In this work, we propose a novel approach that integrates the video scene ontology with CNN (Convolutional Neural Network) for improved video tagging. Our method captures the content of a video by extracting the information from individual key frames. The key frames are then fed to a CNN based deep learning model to train its parameters. The trained parameters are used to generate the most frequent tags. Highly frequent tags are used to summarize the input video. The proposed technique is benchmarked on the most widely used dataset of video activities, namely, UCF-101. Our method managed to achieve an overall accuracy of 99.8% with an F1- score of 96.2%.",what Learning method ?,Convolutional Neural Network,,False,False
"This paper describes CrowdREquire, a platform that supports requirements engineering using the crowdsourcing concept. The power of the crowd is in the diversity of talents and expertise available within the crowd and CrowdREquire specifies how requirements engineering can harness skills available in the crowd. In developing CrowdREquire, this paper designs a crowdsourcing business model and market strategy for crowdsourcing requirements engineering irrespective of the professions and areas of expertise of the crowd involved. This is also a specific application of crowdsourcing which establishes the general applicability and efficacy of crowdsourcing. The results obtained could be used as a reference for other crowdsourcing systems as well.",what Utilities in CrowdRE ?,Crowd,requirements engineering can harness skills,False,False
"To build needed mobile applications in specific domains, requirements should be collected and analyzed in holistic approach. However, resource is limited for small vendor groups to perform holistic requirement acquisition and elicitation. The rise of crowdsourcing and crowdfunding gives small vendor groups new opportunities to build needed mobile applications for the crowd. By finding prior stakeholders and gathering requirements effectively from the crowd, mobile application projects can establish sound foundation in early phase of software process. Therefore, integration of crowd-based requirement engineering into software process is important for small vendor groups. Conventional requirement acquisition and elicitation methods are analyst-centric. Very little discussion is in adapting requirement acquisition tools for crowdcentric context. In this study, several tool features of use case documentation are revised in crowd-centric context. These features constitute a use case-based framework, called UCFrame, for crowd-centric requirement acquisition. An instantiation of UCFrame is also presented to demonstrate the effectiveness of UCFrame in collecting crowd requirements for building two mobile applications.",what Utilities in CrowdRE ?,Crowd,"requirements should be collected and analyzed in holistic approach. however, resource is limited for small vendor groups to perform holistic requirement acquisition and elicitation. the rise of crowdsourcing and crowdfunding gives small vendor groups new opportunities to build needed mobile applications for the crowd. by finding prior stakeholders and gathering requirements",False,True
"Internetware is required to respond quickly to emergent user requirements or requirements changes by providing application upgrade or making context-aware recommendations. As user requirements in Internet computing environment are often changing fast and new requirements emerge more and more in a creative way, traditional requirements engineering approaches based on requirements elicitation and analysis cannot ensure the quick response of Internetware. In this paper, we propose an approach for mining context-aware user requirements from crowd contributed mobile data. The approach captures behavior records contributed by a crowd of mobile users and automatically mines context-aware user behavior patterns (i.e., when, where and under what conditions users require a specific service) from them using Apriori-M algorithm. Based on the mined user behaviors, emergent requirements or requirements changes can be inferred from the mined user behavior patterns and solutions that satisfy the requirements can be recommended to users. To evaluate the proposed approach, we conduct an experimental study and show the effectiveness of the requirements mining approach.",what Utilities in CrowdRE ?,Crowd,"requirements elicitation and analysis cannot ensure the quick response of internetware. in this paper, we propose an approach for mining context - aware user requirements from crowd contributed mobile data. the approach captures behavior records",False,True
"Automated app review analysis is an important avenue for extracting a variety of requirements-related information. Typically, a first step toward performing such analysis is preparing a training dataset, where developers (experts) identify a set of reviews and, manually, annotate them according to a given task. Having sufficiently large training data is important for both achieving a high prediction accuracy and avoiding overfitting. Given millions of reviews, preparing a training set is laborious. We propose to incorporate active learning, a machine learning paradigm, in order to reduce the human effort involved in app review analysis. Our app review classification framework exploits three active learning strategies based on uncertainty sampling. We apply these strategies to an existing dataset of 4,400 app reviews for classifying app reviews as features, bugs, rating, and user experience. We find that active learning, compared to a training dataset chosen randomly, yields a significantly higher prediction accuracy under multiple scenarios.",what Utilities in CrowdRE ?,Task,,False,False
"The article covers the issues of ensuring sustainable city development based on the achievements of digitalization. Attention is also paid to the use of quality economy tools in managing 'smart' cities under conditions of the digital transformation of the national economy. The current state of 'smart' cities and the main factors contributing to their sustainable development, including the digitalization requirements is analyzed. Based on the analysis of statistical material, the main prospects to form the 'smart city' concept, the possibility to assess such parameters as 'life quality', 'comfort', 'rational organization', 'opportunities', 'sustainable development', 'city environment accessibility', 'use of communication technologies'. The role of tools for quality economics is revealed in ensuring the big city life under conditions of digital economy. The concept of 'life quality' is considered, which currently is becoming one of the fundamental vectors of the human civilization development, a criterion that is increasingly used to compare countries and territories. Special attention is paid to such tools and methods of quality economics as standardization, metrology and quality management. It is proposed to consider these tools as a mechanism for solving the most important problems in the national economy development under conditions of digital transformation.",what Components  ?,Sustainable development,quality economy,False,False
"The article covers the issues of ensuring sustainable city development based on the achievements of digitalization. Attention is also paid to the use of quality economy tools in managing 'smart' cities under conditions of the digital transformation of the national economy. The current state of 'smart' cities and the main factors contributing to their sustainable development, including the digitalization requirements is analyzed. Based on the analysis of statistical material, the main prospects to form the 'smart city' concept, the possibility to assess such parameters as 'life quality', 'comfort', 'rational organization', 'opportunities', 'sustainable development', 'city environment accessibility', 'use of communication technologies'. The role of tools for quality economics is revealed in ensuring the big city life under conditions of digital economy. The concept of 'life quality' is considered, which currently is becoming one of the fundamental vectors of the human civilization development, a criterion that is increasingly used to compare countries and territories. Special attention is paid to such tools and methods of quality economics as standardization, metrology and quality management. It is proposed to consider these tools as a mechanism for solving the most important problems in the national economy development under conditions of digital transformation.",what Components  ?,Metrology,quality economy,False,False
"The digital transformation of our life changes the way we work, learn, communicate, and collaborate. Enterprises are presently transforming their strategy, culture, processes, and their information systems to become digital. The digital transformation deeply disrupts existing enterprises and economies. Digitization fosters the development of IT systems with many rather small and distributed structures, like Internet of Things, Microservices and mobile services. Since years a lot of new business opportunities appear using the potential of services computing, Internet of Things, mobile systems, big data with analytics, cloud computing, collaboration networks, and decision support. Biological metaphors of living and adaptable ecosystems provide the logical foundation for self-optimizing and resilient run-time environments for intelligent business services and adaptable distributed information systems with service-oriented enterprise architectures. This has a strong impact for architecting digital services and products following both a value-oriented and a service perspective. The change from a closed-world modeling world to a more flexible open-world composition and evolution of enterprise architectures defines the moving context for adaptable and high distributed systems, which are essential to enable the digital transformation. The present research paper investigates the evolution of Enterprise Architecture considering new defined value-oriented mappings between digital strategies, digital business models and an improved digital enterprise architecture.",what Components  ?,Value,,False,False
"Abstract The study demonstrates a methodology for mapping various hematite ore classes based on their reflectance and absorption spectra, using Hyperion satellite imagery. Substantial validation is carried out, using the spectral feature fitting technique, with the field spectra measured over the Bailadila hill range in Chhattisgarh State in India. The results of the study showed a good correlation between the concentration of iron oxide with the depth of the near-infrared absorption feature (R 2 = 0.843) and the width of the near-infrared absorption feature (R 2 = 0.812) through different empirical models, with a root-mean-square error (RMSE) between &lt; 0.317 and &lt; 0.409. The overall accuracy of the study is 88.2% with a Kappa coefficient value of 0.81. Geochemical analysis and X-ray fluorescence (XRF) of field ore samples are performed to ensure different classes of hematite ore minerals. Results showed a high content of Fe &gt; 60 wt% in most of the hematite ore samples, except banded hematite quartzite (BHQ) (&lt; 47 wt%).",what Analysis ?,X-ray fluorescence (XRF),geochemical analysis and x - ray fluorescence,False,False
"Abstract Spatial distribution of altered minerals in rocks and soils in the Gadag Schist Belt (GSB) is carried out using Hyperion data of March 2013. The entire spectral range is processed with emphasis on VNIR (0.4–1.0 μm) and SWIR regions (2.0–2.4 μm). Processing methodology includes Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes correction, minimum noise fraction transformation, spectral feature fitting (SFF) and spectral angle mapper (SAM) in conjunction with spectra collected, using an analytical spectral device spectroradiometer. A total of 155 bands were analysed to identify and map the major altered minerals by studying the absorption bands between the 0.4–1.0-μm and 2.0–2.3-μm wavelength regions. The most important and diagnostic spectral absorption features occur at 0.6–0.7 μm, 0.86 and at 0.9 μm in the VNIR region due to charge transfer of crystal field effect in the transition elements, whereas absorption near 2.1, 2.2, 2.25 and 2.33 μm in the SWIR region is related to the bending and stretching of the bonds in hydrous minerals (Al-OH, Fe-OH and Mg-OH), particularly in clay minerals. SAM and SFF techniques are implemented to identify the minerals present. A score of 0.33–1 was assigned for both SAM and SFF, where a value of 1 indicates the exact mineral type. However, endmember spectra were compared with United States Geological Survey and John Hopkins University spectral libraries for minerals and soils. Five minerals, i.e. kaolinite-5, kaolinite-2, muscovite, haematite, kaosmec and one soil, i.e. greyish brown loam have been identified. Greyish brown loam and kaosmec have been mapped as the major weathering/altered products present in soils and rocks of the GSB. This was followed by haematite and kaolinite. The SAM classifier was then applied on a Hyperion image to produce a mineral map. The dominant lithology of the area included greywacke, argillite and granite gneiss.",what Analysis ?,Spectral Angle Mapper (SAM),hyperion,False,False
"This study describes the utility of Earth Observation (EO)-1 Hyperion data for sub-pixel mineral investigation using Mixture Tuned Target Constrained Interference Minimized Filter (MTTCIMF) algorithm in hostile mountainous terrain of Rajsamand district of Rajasthan, which hosts economic mineralization such as lead, zinc, and copper etc. The study encompasses pre-processing, data reduction, Pixel Purity Index (PPI) and endmember extraction from reflectance image of surface minerals such as illite, montmorillonite, phlogopite, dolomite and chlorite. These endmembers were then assessed with USGS mineral spectral library and lab spectra of rock samples collected from field for spectral inspection. Subsequently, MTTCIMF algorithm was implemented on processed image to obtain mineral distribution map of each detected mineral. A virtual verification method has been adopted to evaluate the classified image, which uses directly image information to evaluate the result and confirm the overall accuracy and kappa coefficient of 68 % and 0.6 respectively. The sub-pixel level mineral information with reasonable accuracy could be a valuable guide to geological and exploration community for expensive ground and/or lab experiments to discover economic deposits. Thus, the study demonstrates the feasibility of Hyperion data for sub-pixel mineral mapping using MTTCIMF algorithm with cost and time effective approach.",what Analysis ?,Mixture Tuned Target Constrained Interference Minimized Filter (MTTCIMF) algorithm,pixel purity index,False,False
"Abstract. Hyperspectral remote sensing has been widely used in mineral identification using the particularly useful short-wave infrared (SWIR) wavelengths (1.0 to 2.5 μm). Current mineral mapping methods are easily limited by the sensor’s radiometric sensitivity and atmospheric effects. Therefore, a simple mineral mapping algorithm (SMMA) based on the combined application with multitype diagnostic SWIR absorption features for hyperspectral data is proposed. A total of nine absorption features are calculated, respectively, from the airborne visible/infrared imaging spectrometer data, the Hyperion hyperspectral data, and the ground reference spectra data collected from the United States Geological Survey (USGS) spectral library. Based on spectral analysis and statistics, a mineral mapping decision-tree model for the Cuprite mining district in Nevada, USA, is constructed. Then, the SMMA algorithm is used to perform mineral mapping experiments. The mineral map from the USGS (USGS map) in the Cuprite area is selected for validation purposes. Results showed that the SMMA algorithm is able to identify most minerals with high coincidence with USGS map results. Compared with Hyperion data (overall accuracy=74.54%), AVIRIS data showed overall better mineral mapping results (overall accuracy=94.82%) due to low signal-to-noise ratio and high spatial resolution.",what Analysis ?,Simple mineral mapping algorithm (SMMA),smma ),False,False
"A commonly cited mechanism for invasion resistance is more complete resource use by diverse plant assemblages with maximum niche complementarity. We investigated the invasion resistance of several plant functional groups against the nonindigenous forb Spotted knapweed (Centaurea maculosa). The study consisted of a factorial combination of seven functional group removals (groups singularly or in combination) and two C. maculosa treatments (addition vs. no addition) applied in a randomized complete block design replicated four times at each of two sites. We quantified aboveground plant material nutrient concentration and uptake (concentration × biomass) by indigenous functional groups: grasses, shallow‐rooted forbs, deep‐rooted forbs, spikemoss, and the nonindigenous invader C. maculosa. In 2001, C. maculosa density depended upon which functional groups were removed. The highest C. maculosa densities occurred where all vegetation or all forbs were removed. Centaurea maculosa densities were the lowest in plots where nothing, shallow‐rooted forbs, deep‐rooted forbs, grasses, or spikemoss were removed. Functional group biomass was also collected and analyzed for nitrogen, phosphorus, potassium, and sulphur. Based on covariate analyses, postremoval indigenous plot biomass did not relate to invasion by C. maculosa. Analysis of variance indicated that C. maculosa tissue nutrient percentage and net nutrient uptake were most similar to indigenous forb functional groups. Our study suggests that establishing and maintaining a diversity of plant functional groups within the plant community enhances resistance to invasion. Indigenous plants of functionally similar groups as an invader may be particularly important in invasion resistance.",what Measure of species similarity ?,Functional groups,,False,False
"<jats:title>Abstract</jats:title><jats:p>Understanding the relative importance of various functional groups in minimizing invasion by medusahead is central to increasing the resistance of native plant communities. The objective of this study was to determine the relative importance of key functional groups within an intact Wyoming big sagebrush–bluebunch wheatgrass community type on minimizing medusahead invasion. Treatments consisted of removal of seven functional groups at each of two sites, one with shrubs and one without shrubs. Removal treatments included (1) everything, (2) shrubs, (3) perennial grasses, (4) taprooted forbs, (5) rhizomatous forbs, (6) annual forbs, and (7) mosses. A control where nothing was removed was also established. Plots were arranged in a randomized complete block with 4 replications (blocks) at each site. Functional groups were removed beginning in the spring of 2004 and maintained monthly throughout each growing season through 2009. Medusahead was seeded at a rate of 2,000 seeds m<jats:sup>−2</jats:sup> (186 seeds ft<jats:sup>−2</jats:sup>) in fall 2005. Removing perennial grasses nearly doubled medusahead density and biomass compared with any other removal treatment. The second highest density and biomass of medusahead occurred from removing rhizomatous forbs (phlox). We found perennial grasses played a relatively more significant role than other species in minimizing invasion by medusahead. We suggest that the most effective basis for establishing medusahead-resistant plant communities is to establish 2 or 3 highly productive grasses that are complementary in niche and that overlap that of the invading species.</jats:p>",what Measure of species similarity ?,Functional groups,functional groups,True,True
"1 Although experimental studies usually reveal that resistance to invasion increases with species diversity, observational studies sometimes show the opposite trend. The higher resistance of diverse plots to invasion may be partly due to the increased probability of a plot containing a species with similar resource requirements to the invader. 2 We conducted a study of the invasibility of monocultures belonging to three different functional groups by seven sown species of legume. By only using experimentally established monocultures, rather than manipulating the abundance of particular functional groups, we removed both species diversity and differences in underlying abiotic conditions as potentially confounding variables. 3 We found that legume monocultures were more resistant than monocultures of grasses or non‐leguminous forbs to invasion by sown legumes but not to invasion by other unsown species. The functional group effect remained after controlling for differences in total biomass and the average height of the above‐ground biomass. 4 The relative success of legume species and types also varied with monoculture characteristics. The proportional biomass of climbing legumes increased strongly with biomass height in non‐leguminous forb monocultures, while it declined with biomass height in grass monocultures. Trifolium pratense was the most successful invader in grass monocultures, while Vicia cracca was the most successful in non‐leguminous forb monocultures. 5 Our results suggest that non‐random assembly rules operate in grassland communities both between and within functional groups. Legume invaders found it much more difficult to invade legume plots, while grass and non‐leguminous forb plots favoured non‐climbing and climbing legumes, respectively. If plots mimic monospecific patches, the effect of these assembly rules in diverse communities might depend upon the patch structure of diverse communities. This dependency on patch structure may contribute to differences in results of research from experimental vs. natural communities.",what Measure of species similarity ?,Functional groups,height,False,False
"Abstract. One of the major objectives of the BIOSOPE cruise, carried out on the R/V Atalante from October-November 2004 in the South Pacific Ocean, was to establish productivity rates along a zonal section traversing the oligotrophic South Pacific Gyre (SPG). These results were then compared to measurements obtained from the nutrient – replete waters in the Chilean upwelling and around the Marquesas Islands. A dual 13C/15N isotope technique was used to estimate the carbon fixation rates, inorganic nitrogen uptake (including dinitrogen fixation), ammonium (NH4) and nitrate (NO3) regeneration and release of dissolved organic nitrogen (DON). The SPG exhibited the lowest primary production rates (0.15 g C m−2 d−1), while rates were 7 to 20 times higher around the Marquesas Islands and in the Chilean upwelling, respectively. In the very low productive area of the SPG, most of the primary production was sustained by active regeneration processes that fuelled up to 95% of the biological nitrogen demand. Nitrification was active in the surface layer and often balanced the biological demand for nitrate, especially in the SPG. The percentage of nitrogen released as DON represented a large proportion of the inorganic nitrogen uptake (13–15% in average), reaching 26–41% in the SPG, where DON production played a major role in nitrogen cycling. Dinitrogen fixation was detectable over the whole study area; even in the Chilean upwelling, where rates as high as 3 nmoles l−1 d−1 were measured. In these nutrient-replete waters new production was very high (0.69±0.49 g C m−2 d−1) and essentially sustained by nitrate levels. In the SPG, dinitrogen fixation, although occurring at much lower daily rates (≈1–2 nmoles l−1 d−1), sustained up to 100% of the new production (0.008±0.007 g C m−2 d−1) which was two orders of magnitude lower than that measured in the upwelling. The annual N2-fixation of the South Pacific is estimated to 21×1012g, of which 1.34×1012g is for the SPG only. Even if our ""snapshot"" estimates of N2-fixation rates were lower than that expected from a recent ocean circulation model, these data confirm that the N-deficiency South Pacific Ocean would provide an ideal ecological niche for the proliferation of N2-fixers which are not yet identified.",what Region of data collection ?,South Pacific Ocean,"south pacific ocean,",True,True
"Nitrogen (N) is an essential element for life and controls the magnitude of primary productivity in the ocean. In order to describe the microorganisms that catalyze N transformations in surface waters in the South Pacific Ocean, we collected high-resolution biotic and abiotic data along a 7000 km transect, from the Antarctic ice edge to the equator. The transect, conducted between late Austral autumn and early winter 2016, covered major oceanographic features such as the polar front (PF), the subtropical front (STF) and the Pacific equatorial divergence (PED). We measured N2 fixation and nitrification rates and quantified the relative abundances of diazotrophs and nitrifiers in a region where few to no rate measurements are available. Even though N2 fixation rates are usually below detection limits in cold environments, we were able to measure this N pathway at 7/10 stations in the cold and nutrient rich waters near the PF. This result highlights that N2 fixation rates continue to be measured outside the well-known subtropical regions. The majority of the mid to high N2 fixation rates (>∼20 nmol L–1 d–1), however, still occurred in the expected tropical and subtropical regions. High throughput sequence analyses of the dinitrogenase reductase gene (nifH) revealed that the nifH Cluster I dominated the diazotroph diversity throughout the transect. nifH gene richness did not show a latitudinal trend, nor was it significantly correlated with N2 fixation rates. Nitrification rates above the mixed layer in the Southern Ocean ranged between 56 and 1440 nmol L–1 d–1. Our data showed a decoupling between carbon and N assimilation (NO3– and NH4+ assimilation rates) in winter in the South Pacific Ocean. Phytoplankton community structure showed clear changes across the PF, the STF and the PED, defining clear biomes. Overall, these findings provide a better understanding of the ecosystem functionality in the South Pacific Ocean across key oceanographic biomes.",what Region of data collection ?,South Pacific Ocean,"south pacific ocean,",True,True
"Biogeochemical implications of global imbalance between the rates of marine dinitrogen (N2) fixation and denitrification have spurred us to understand the former process in the Arabian Sea, which contributes considerably to the global nitrogen budget. Heterotrophic bacteria have gained recent appreciation for their major role in marine N budget by fixing a significant amount of N2. Accordingly, we hypothesize a probable role of heterotrophic diazotrophs from the 15N2 enriched isotope labelling dark incubations that witnessed rates comparable to the light incubations in the eastern Arabian Sea during spring 2010. Maximum areal rates (8 mmol N m-2 d-1) were the highest ever observed anywhere in world oceans. Our results suggest that the eastern Arabian Sea gains ~92% of its new nitrogen through N2 fixation. Our results are consistent with the observations made in the same region in preceding year, i.e., during the spring of 2009.",what Region of data collection ?,Eastern Arabian Sea,"arabian sea,",False,False
"The partial pressure of CO2 (pCO2) was measured during the 1995 South‐West Monsoon in the Arabian Sea. The Arabian Sea was characterized throughout by a moderate supersaturation of 12–30 µatm. The stable atmospheric pCO2 level was around 345 µatm. An extreme supersaturation was found in areas of coastal upwelling off the Omani coast with pCO2 peak values in surface waters of 750 µatm. Such two‐fold saturation (218%) is rarely found elsewhere in open ocean environments. We also encountered cold upwelled water 300 nm off the Omani coast in the region of Ekman pumping, which was also characterized by a strongly elevated seawater pCO2 of up to 525 µatm. Due to the strong monsoonal wind forcing the Arabian Sea as a whole and the areas of upwelling in particular represent a significant source of atmospheric CO2 with flux densities from around 2 mmol m−2 d−1 in the open ocean to 119 mmol m−2 d−1 in coastal upwelling. Local air masses passing the area of coastal upwelling showed increasing CO2 concentrations, which are consistent with such strong emissions.",what Region of data collection ?,Arabian Sea,arabian sea.,True,True
"The import of nitrogen via dinitrogen fixation supports primary production, particularly in the oligotrophic ocean; however, to what extent dinitrogen fixation influences primary production, and the role of specific types of diazotrophs, remains poorly understood. We examined the relationship between primary production and dinitrogen fixation together with diazotroph community structure in the oligotrophic western and eastern South Pacific Ocean and found that dinitrogen fixation was higher than nitrate‐based new production. Primary production increased in the middle of the western subtropical region, where the cyanobacterium Trichodesmium dominated the diazotroph community and accounted for up to 7.8% of the phytoplankton community, and the abundance of other phytoplankton taxa (especially Prochlorococcus) was high. These results suggest that regenerated production was enhanced by nitrogen released from Trichodesmium and that carbon fixation by Trichodesmium also contributed significantly to total primary production. Although volumetric dinitrogen fixation was comparable between the western and eastern subtropical regions, primary production in the western waters was more than twice as high as that in the eastern waters, where UCYN‐A1 (photoheterotroph) and heterotrophic bacteria were the dominant diazotrophs. This suggests that dinitrogen fixed by these diazotrophs contributed relatively little to primary production of the wider community, and there was limited carbon fixation by these diazotrophs. Hence, we document how the community composition of diazotrophs in the field can be reflected in how much nitrogen becomes available to the wider phytoplankton community and in how much autotrophic diazotrophs themselves fix carbon and thereby influences the magnitude of local primary production.",what Region of data collection ?,South Pacific Ocean,south pacific ocean,True,True
"Young-of-the-year (YOY) bay anchovy Anchoa mitchilli occur in higher proportion rel- ative to larvae in the upper Chesapeake Bay. This has led to the hypothesis that up-bay dispersal favors recruitment. Here we test whether recruitment of bay anchovy to different parts of the Chesa- peake Bay results from differential dispersal rates. Electron microprobe analysis of otolith strontium was used to hind-cast patterns and rates of movement across salinity zones. Individual chronologies of strontium were constructed for 55 bay anchovy aged 43 to 103 d collected at 5 Chesapeake Bay mainstem sites representing upper, middle, and lower regions of the bay during September 1998. Most YOY anchovy were estimated to have originated in the lower bay. Those collected at 5 and 11 psu sites exhibited the highest past dispersal rates, all in an up-estuary direction. No significant net dispersal up- or down-estuary occurred for recruits captured at the polyhaline (♢18 psu) site. Ini- tiation of ingress to lower salinity waters (<15 psu) was estimated to occur near metamorphosis, dur- ing the early juvenile stage, at sizes ♢ 25 mm standard length (SL) and ages ♢ 50 d after hatch. Esti- mated maximum upstream dispersal rate (over-the-ground speed) during the first 50 to 100 d of life exceeded 50 mm s -1 .",what Species Order ?,Anchoa mitchilli,otolith strontium,False,False
"We used an electron probe microanalyzer (EPMA) to determine the migratory environ- mental history of the catadromous grey mullet Mugil cephalus from the Sr:Ca ratios in otoliths of 10 newly recruited juveniles collected from estuaries and 30 adults collected from estuaries, nearshore (coastal waters and bay) and offshore, in the adjacent waters off Taiwan. Mean (±SD) Sr:Ca ratios at the edges of adult otoliths increased significantly from 6.5 ± 0.9 × 10 -3 in estuaries and nearshore waters to 8.9 ± 1.4 × 10 -3 in offshore waters (p < 0.01), corresponding to increasing ambi- ent salinity from estuaries and nearshore to offshore waters. The mean Sr:Ca ratios decreased sig- nificantly from the core (11.2 ± 1.2 × 10 -3 ) to the otolith edge (6.2 ± 1.4 × 10 -3 ) in juvenile otoliths (p < 0.001). The mullet generally spawned offshore and recruited to the estuary at the juvenile stage; therefore, these data support the use of Sr:Ca ratios in otoliths to reconstruct the past salinity history of the mullet. A life-history scan of the otolith Sr:Ca ratios indicated that the migratory environmen- tal history of the mullet beyond the juvenile stage consists of 2 types. In Type 1 mullet, Sr:Ca ratios range between 4.0 × 10 -3 and 13.9 × 10 -3 , indicating that they migrated between estuary and offshore waters but rarely entered the freshwater habitat. In Type 2 mullet, the Sr:Ca ratios decreased to a minimum value of 0.4 × 10 -3 , indicating that the mullet migrated to a freshwater habitat. Most mullet beyond the juvenile stage migrated from estuary to offshore waters, but a few mullet less than 2 yr old may have migrated into a freshwater habitat. Most mullet collected nearshore and offshore were of Type 1, while those collected from the estuaries were a mixture of Types 1 and 2. The mullet spawning stock consisted mainly of Type 1 fish. The growth rates of the mullet were similar for Types 1 and 2. The migratory patterns of the mullet were more divergent than indicated by previous reports of their catadromous behavior.",what Species Order ?,Mugil cephalus,mugil cephalus,True,True
"The habitat use and migratory patterns of Osbeck’s grenadier anchovy Coilia mystus in the Yangtze estuary and the estuarine tapertail anchovy Coilia ectenes from the Yangtze estuary and Taihu Lake, China, were studied by examining the environmental signatures of strontium and calcium in their otoliths using electron probe microanalysis. The results indicated that Taihu C. ectenes utilizes only freshwater habitats, whereas the habitat use patterns of Yangtze C. ectenes and C. mystus were much more flexible, apparently varying among fresh, brackish and marine areas. The present study suggests that the spawning populations of Yangtze C. ectenes and C. mystus in the Yangtze estuary consist of individuals with different migration histories, and individuals of these two Yangtze Coilia species seem to use a variety of different habitats during the non-spawning seasons.",what Species Order ?,Coilia mystus,anchovy coilia mystus,False,True
"The novel coronavirus (2019-nCoV) is a recently emerged human pathogen that has spread widely since January 2020. Initially, the basic reproductive number, R0, was estimated to be 2.2 to 2.7. Here we provide a new estimate of this quantity. We collected extensive individual case reports and estimated key epidemiology parameters, including the incubation period. Integrating these estimates and high-resolution real-time human travel and infection data with mathematical models, we estimated that the number of infected individuals during early epidemic double every 2.4 days, and the R0 value is likely to be between 4.7 and 6.6. We further show that quarantine and contact tracing of symptomatic individuals alone may not be effective and early, strong control measures are needed to stop transmission of the virus.",what R0 estimates (average) ?,6.6,,False,False
"The exported cases of 2019 novel coronavirus (COVID-19) infection that were confirmed outside China provide an opportunity to estimate the cumulative incidence and confirmed case fatality risk (cCFR) in mainland China. Knowledge of the cCFR is critical to characterize the severity and understand the pandemic potential of COVID-19 in the early stage of the epidemic. Using the exponential growth rate of the incidence, the present study statistically estimated the cCFR and the basic reproduction number—the average number of secondary cases generated by a single primary case in a naïve population. We modeled epidemic growth either from a single index case with illness onset on 8 December 2019 (Scenario 1), or using the growth rate fitted along with the other parameters (Scenario 2) based on data from 20 exported cases reported by 24 January 2020. The cumulative incidence in China by 24 January was estimated at 6924 cases (95% confidence interval [CI]: 4885, 9211) and 19,289 cases (95% CI: 10,901, 30,158), respectively. The latest estimated values of the cCFR were 5.3% (95% CI: 3.5%, 7.5%) for Scenario 1 and 8.4% (95% CI: 5.3%, 12.3%) for Scenario 2. The basic reproduction number was estimated to be 2.1 (95% CI: 2.0, 2.2) and 3.2 (95% CI: 2.7, 3.7) for Scenarios 1 and 2, respectively. Based on these results, we argued that the current COVID-19 epidemic has a substantial potential for causing a pandemic. The proposed approach provides insights in early risk assessment using publicly available data.",what R0 estimates (average) ?,2.1,3. 2,False,False
"Background The 2019 novel Coronavirus (COVID-19) emerged in Wuhan, China in December 2019 and has been spreading rapidly in China. Decisions about its pandemic threat and the appropriate level of public health response depend heavily on estimates of its basic reproduction number and assessments of interventions conducted in the early stages of the epidemic. Methods We conducted a mathematical modeling study using five independent methods to assess the basic reproduction number (R0) of COVID-19, using data on confirmed cases obtained from the China National Health Commission for the period 10th January to 8th February. We analyzed the data for the period before the closure of Wuhan city (10th January to 23rd January) and the post-closure period (23rd January to 8th February) and for the whole period, to assess both the epidemic risk of the virus and the effectiveness of the closure of Wuhan city on spread of COVID-19. Findings Before the closure of Wuhan city the basic reproduction number of COVID-19 was 4.38 (95% CI: 3.63-5.13), dropping to 3.41 (95% CI: 3.16-3.65) after the closure of Wuhan city. Over the entire epidemic period COVID-19 had a basic reproduction number of 3.39 (95% CI: 3.09-3.70), indicating it has a very high transmissibility. Interpretation COVID-19 is a highly transmissible virus with a very high risk of epidemic outbreak once it emerges in metropolitan areas. The closure of Wuhan city was effective in reducing the severity of the epidemic, but even after closure of the city and the subsequent expansion of that closure to other parts of Hubei the virus remained extremely infectious. Emergency planners in other cities should consider this high infectiousness when considering responses to this virus.",what R0 estimates (average) ?,3.41,4. 38,False,False
"Abstract Background As the COVID-19 epidemic is spreading, incoming data allows us to quantify values of key variables that determine the transmission and the effort required to control the epidemic. We determine the incubation period and serial interval distribution for transmission clusters in Singapore and in Tianjin. We infer the basic reproduction number and identify the extent of pre-symptomatic transmission. Methods We collected outbreak information from Singapore and Tianjin, China, reported from Jan.19-Feb.26 and Jan.21-Feb.27, respectively. We estimated incubation periods and serial intervals in both populations. Results The mean incubation period was 7.1 (6.13, 8.25) days for Singapore and 9 (7.92, 10.2) days for Tianjin. Both datasets had shorter incubation periods for earlier-occurring cases. The mean serial interval was 4.56 (2.69, 6.42) days for Singapore and 4.22 (3.43, 5.01) for Tianjin. We inferred that early in the outbreaks, infection was transmitted on average 2.55 and 2.89 days before symptom onset (Singapore, Tianjin). The estimated basic reproduction number for Singapore was 1.97 (1.45, 2.48) secondary cases per infective; for Tianjin it was 1.87 (1.65, 2.09) secondary cases per infective. Conclusions Estimated serial intervals are shorter than incubation periods in both Singapore and Tianjin, suggesting that pre-symptomatic transmission is occurring. Shorter serial intervals lead to lower estimates of R0, which suggest that half of all secondary infections should be prevented to control spread.",what R0 estimates (average) ?,1.97,4. 56,False,False
"Knowledge graph embedding is an important task and it will benefit lots of downstream applications. Currently, deep neural networks based methods achieve state-of-the-art performance. However, most of these existing methods are very complex and need much time for training and inference. To address this issue, we propose a simple but effective atrous convolution based knowledge graph embedding method. Compared with existing state-of-the-art methods, our method has following main characteristics. First, it effectively increases feature interactions by using atrous convolutions. Second, to address the original information forgotten issue and vanishing/exploding gradient issue, it uses the residual learning method. Third, it has simpler structure but much higher parameter efficiency. We evaluate our method on six benchmark datasets with different evaluation metrics. Extensive experiments show that our model is very effective. On these diverse datasets, it achieves better results than the compared state-of-the-art methods on most of evaluation metrics. The source codes of our model could be found at https://github.com/neukg/AcrE.",what has source code ?,https://github.com/neukg/AcrE,https : / / github. com / neukg / acre.,False,False
"While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https://github.com/artetxem/monoses.",what has source code ?,https://github.com/artetxem/monoses,https : / / github. com /,False,False
"We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",what has source code ?,https://github.com/dwadden/dygiepp,https : / / github. com / dwadden / dygiepp,False,False
"Luo J & Cardina J (2012). Germination patterns and implications for invasiveness in three Taraxacum (Asteraceae) species. Weed Research 52, 112–121. Summary The ability to germinate across different environments has been considered an important trait of invasive plant species that allows for establishment success in new habitats. Using two alien congener species of Asteraceae –Taraxacum officinale (invasive) and Taraxacum laevigatum laevigatum (non-invasive) – we tested the hypothesis that invasive species germinate better than non-invasives under various conditions. The germination patterns of Taraxacum brevicorniculatum, a contaminant found in seeds of the crop Taraxacum kok-saghyz, were also investigated to evaluate its invasive potential. In four experiments, we germinated seeds along gradients of alternating temperature, constant temperature (with or without light), water potential and following accelerated ageing. Neither higher nor lower germination per se explained invasion success for the Taraxacum species tested here. At alternating temperature, the invasive T. officinale had higher germination than or similar to the non-invasive T. laevigatum. Contrary to predictions, T. laevigatum exhibited higher germination than T. officinale in environments of darkness, low water potential or after the seeds were exposed to an ageing process. These results suggested a complicated role of germination in the success of T. officinale. Taraxacum brevicorniculatum showed the highest germination among the three species in all environments. The invasive potential of this species is thus unclear and will probably depend on its performance at other life stages along environmental gradients.",what Study date ?,2012,2012 ),True,True
"Aims. To evaluate the role of native predators (birds) within an Australian foodweb (lerp psyllids and eucalyptus trees) reassembled in California. Location. Eucalyptus groves within Santa Cruz, California. Methods. We compared bird diversity and abundance between a eucalyptus grove infested with lerp psyllids and a grove that was uninfested, using point counts. We documented shifts in the foraging behaviour of birds between the groves using structured behavioural observations. Additionally, we judged the effect of bird foraging on lerp psyllid abundance using exclosure experiments. Results. We found a greater richness and abundance of Californian birds within a psyllid infested eucalyptus grove compared to a matched non-infested grove, and that Californian birds modify their foraging behaviour within the infested grove in order to concentrate on ingesting psyllids. This suggests that Californian birds could provide indirect top-down benefits to eucalyptus trees similar to those observed in Australia. However, using bird exclosure experiments, we found no evidence of top-down control of lerp psyllids by Californian birds. Main conclusions. We suggest that physiological and foraging differences between Californian and Australian pysllid-eating birds account for the failure to observe top-down control of psyllid populations in California. The increasing rate of non-indigenous species invasions has produced local biotas that are almost entirely composed of non-indigenous species. This example illustrates the complex nature of cosmopolitan native-exotic food webs, and the ecological insights obtainable through their study. © 2004 Blackwell Publishing Ltd.",what Study date ?,2004,2004,True,True
"1. Biological invasion theory predicts that the introduction and establishment of non-native species is positively correlated with propagule pressure. Releases of pet and aquarium fishes to inland waters has a long history; however, few studies have examined the demographic basis of their importation and incidence in the wild. 2. For the 1500 grid squares (10×10 km) that make up England, data on human demographics (population density, numbers of pet shops, garden centres and fish farms), the numbers of non-native freshwater fishes (from consented licences) imported in those grid squares (i.e. propagule pressure), and the reported incidences (in a national database) of non-native fishes in the wild were used to examine spatial relationships between the occurrence of non-native fishes and the demographic factors associated with propagule pressure, as well as to test whether the demographic factors are statistically reliable predictors of the incidence of non-native fishes, and as such surrogate estimators of propagule pressure. 3. Principal coordinates of neighbour matrices analyses, used to generate spatially explicit models, and confirmatory factor analysis revealed that spatial distributions of non-native species in England were significantly related to human population density, garden centre density and fish farm density. Human population density and the number of fish imports were identified as the best predictors of propagule pressure. 4. Human population density is an effective surrogate estimator of non-native fish propagule pressure and can be used to predict likely areas of non-native fish introductions. In conjunction with fish movements, where available, human population densities can be used to support biological invasion monitoring programmes across Europe (and perhaps globally) and to inform management decisions as regards the prioritization of areas for the control of non-native fish introductions. © Crown copyright 2010. Reproduced with the permission of her Majesty's Stationery Office. Published by John Wiley & Sons, Ltd.",what Study date ?,2010,2010.,True,True
"Channelization is often a major cause of human impacts on river systems. It affects both hydrogeomorphic features and habitat characteristics and potentially impacts riverine flora and fauna. Human-disturbed fluvial ecosystems also appear to be particularly vulnerable to exotic plant establishment. Following a 12-year recovery period, the distribution, composition and cover of both exotic and native plant species were studied along a Portuguese lowland river segment, which had been subjected to resectioning, straightening and two-stage bank reinforcement, and were compared with those of a nearby, less impacted segment. The species distribution was also related to environmental data. Species richness and floristic composition in the channelized river segment were found to be similar to those at the more ‘natural’ river sites. Floral differences were primarily consistent with the dominance of cover by certain species. However, there were significant differences in exotic and native species richness and cover between the ‘natural’ corridor and the channelized segment, which was more susceptible to invasion by exotic perennial taxa, such as Eryngium pandanifolium, Paspalum paspalodes, Tradescantia fluminensis and Acacia dealbata. Factorial and canonical correspondence analyses revealed considerable patchiness in the distribution of species assemblages. The latter were associated with small differences in substrate composition and their own relative position across the banks and along the river segments in question. Data was also subjected to an unweighted pair-group arithmetic average clustering, and the Indicator Value methodology was applied to selected cluster noda in order to obtain significant indicator species. Copyright © 2001 John Wiley & Sons, Ltd.",what Study date ?,2001,2001,True,True
"Nitrogen fixation is an essential process that biologically transforms atmospheric dinitrogen gas to ammonia, therefore compensating for nitrogen losses occurring via denitrification and anammox. Currently, inputs and losses of nitrogen to the ocean resulting from these processes are thought to be spatially separated: nitrogen fixation takes place primarily in open ocean environments (mainly through diazotrophic cyanobacteria), whereas nitrogen losses occur in oxygen-depleted intermediate waters and sediments (mostly via denitrifying and anammox bacteria). Here we report on rates of nitrogen fixation obtained during two oceanographic cruises in 2005 and 2007 in the eastern tropical South Pacific (ETSP), a region characterized by the presence of coastal upwelling and a major permanent oxygen minimum zone (OMZ). Our results show significant rates of nitrogen fixation in the water column; however, integrated rates from the surface down to 120 m varied by ∼30 fold between cruises (7.5±4.6 versus 190±82.3 µmol m−2 d−1). Moreover, rates were measured down to 400 m depth in 2007, indicating that the contribution to the integrated rates of the subsurface oxygen-deficient layer was ∼5 times higher (574±294 µmol m−2 d−1) than the oxic euphotic layer (48±68 µmol m−2 d−1). Concurrent molecular measurements detected the dinitrogenase reductase gene nifH in surface and subsurface waters. Phylogenetic analysis of the nifH sequences showed the presence of a diverse diazotrophic community at the time of the highest measured nitrogen fixation rates. Our results thus demonstrate the occurrence of nitrogen fixation in nutrient-rich coastal upwelling systems and, importantly, within the underlying OMZ. They also suggest that nitrogen fixation is a widespread process that can sporadically provide a supplementary source of fixed nitrogen in these regions.",what Sampling year ?,2007,2005,False,False
"The classical paradigm about marine N2 fixation establishes that this process is mainly constrained to nitrogen-poor tropical and subtropical regions, and sustained by the colonial cyanobacterium Trichodesmium spp. and diatom-diazotroph symbiosis. However, the application of molecular techniques allowed determining a high phylogenic diversity and a wide distribution of marine diazotrophs, which extends the range of ocean environments where biological N2 fixation may be relevant. Between February 2014 and December 2015, we carried out 10 one-day samplings in the upwelling system off NW Iberia in order to: 1) investigate the seasonal variability in the magnitude of N2 fixation, 2) determine its biogeochemical role as a mechanism of new nitrogen supply, and 3) quantify the main diazotrophs in the region under contrasting hydrographic regimes. Our results indicate that the magnitude of N2 fixation in this region was relatively low (0.001±0.002 – 0.095±0.024 µmol N m-3 d-1), comparable to the lower-end of rates described for the subtropical NE Atlantic. Maximum rates were observed at surface during both upwelling and relaxation conditions. The comparison with nitrate diffusive fluxes revealed the minor role of N2 fixation (2 fixation activity detected in the region. Quantitative PCR targeting the nifH gene revealed the highest abundances of two sublineages of Candidatus Atelocyanobacterium thalassa or UCYN-A (UCYN-A1 and UCYN-A2) mainly at surface waters during upwelling and relaxation conditions, and of Gammaproteobacteria γ-24774A11 at deep waters during downwelling. Maximum abundance for the three groups were up to 6.7 × 102, 1.5 × 103 and 2.4 × 104 nifH copies L-1, respectively. Our findings demonstrate measurable N2 fixation activity and presence of diazotrophs throughout the year in a nitrogen-rich temperate region.",what Sampling year ?,2015,"2014 and december 2015,",False,True
"Biological N2 fixation rates were quantified in the Eastern Tropical South Pacific (ETSP) during both El Niño (February 2010) and La Niña (March–April 2011) conditions, and from Low‐Nutrient, Low‐Chlorophyll (20°S) to High‐Nutrient, Low‐Chlorophyll (HNLC) (10°S) conditions. N2 fixation was detected at all stations with rates ranging from 0.01 to 0.88 nmol N L−1 d−1, with higher rates measured during El Niño conditions compared to La Niña. High N2 fixations rates were reported at northern stations (HNLC conditions) at the oxycline and in the oxygen minimum zone (OMZ), despite nitrate concentrations up to 30 µmol L−1, indicating that inputs of new N can occur in parallel with N loss processes in OMZs. Water‐column integrated N2 fixation rates ranged from 4 to 53 µmol N m−2 d−1 at northern stations, and from 0 to 148 µmol m−2 d−1 at southern stations, which are of the same order of magnitude as N2 fixation rates measured in the oligotrophic ocean. N2 fixation rates responded significantly to Fe and organic carbon additions in the surface HNLC waters, and surprisingly by concomitant Fe and N additions in surface waters at the edge of the subtropical gyre. Recent studies have highlighted the predominance of heterotrophic diazotrophs in this area, and we hypothesize that N2 fixation could be directly limited by inorganic nutrient availability, or indirectly through the stimulation of primary production and the subsequent excretion of dissolved organic matter and/or the formation of micro‐environments favorable for heterotrophic N2 fixation.",what Sampling year ?,2010,,False,False
"Extensive measurements of nitrous oxide (N2O) have been made during April–May 1994 (intermonsoon), February–March 1995 (northeast monsoon), July–August 1995 and August 1996 (southwest monsoon) in the Arabian Sea. Low N2O supersaturations in the surface waters are observed during intermonsoon compared to those in northeast and southwest monsoons. Spatial distributions of supersaturations manifest the effects of larger mixing during winter cooling and wind‐driven upwelling during monsoon period off the Indian west coast. A net positive flux is observable during all the seasons, with no discernible differences from the open ocean to coastal regions. The average ocean‐to‐atmosphere fluxes of N2O are estimated, using wind speed dependent gas transfer velocity, to be of the order of 0.26, 0.003, and 0.51, and 0.78 pg (pico grams) cm−2 s−1 during northeast monsoon, intermonsoon, and southwest monsoon in 1995 and 1996, respectively. The lower range of annual emission of N2O is estimated to be 0.56–0.76 Tg N2O per year which constitutes 13–17% of the net global oceanic source. However, N2O emission from the Arabian Sea can be as high as 1.0 Tg N2O per year using different gas transfer models.",what Sampling year ?,1996,1994,False,False
"Depth profiles of dissolved nitrous oxide (N2O) were measured in the central and western Arabian Sea during four cruises in May and July–August 1995 and May–July 1997 as part of the German contribution to the Arabian Sea Process Study of the Joint Global Ocean Flux Study. The vertical distribution of N2O in the water column on a transect along 65°E showed a characteristic double-peak structure, indicating production of N2O associated with steep oxygen gradients at the top and bottom of the oxygen minimum zone. We propose a general scheme consisting of four ocean compartments to explain the N2O cycling as a result of nitrification and denitrification processes in the water column of the Arabian Sea. We observed a seasonal N2O accumulation at 600–800 m near the shelf break in the western Arabian Sea. We propose that, in the western Arabian Sea, N2O might also be formed during bacterial oxidation of organic matter by the reduction of IO3 − to I−, indicating that the biogeochemical cycling of N2O in the Arabian Sea during the SW monsoon might be more complex than previously thought. A compilation of sources and sinks of N2O in the Arabian Sea suggested that the N2O budget is reasonably balanced.",what Sampling year ?,1995,1997,False,False
"Abstract. Coastal upwelling ecosystems with marked oxyclines (redoxclines) present high availability of electron donors that favour chemoautotrophy, leading in turn to high N2O and CH4 cycling associated with aerobic NH4+ (AAO) and CH4 oxidation (AMO). This is the case of the highly productive coastal upwelling area off Central Chile (36° S), where we evaluated the importance of total chemolithoautotrophic vs. photoautotrophic production, the specific contributions of AAO and AMO to chemosynthesis and their role in gas cycling. Chemoautotrophy (involving bacteria and archaea) was studied at a time-series station during monthly (2002–2009) and seasonal cruises (January 2008, September 2008, January 2009) and was assessed in terms of dark carbon assimilation (CA), N2O and CH4 cycling, and the natural C isotopic ratio of particulate organic carbon (δ13POC). Total Integrated dark CA fluctuated between 19.4 and 2.924 mg C m−2 d−1. It was higher during active upwelling and represented on average 27% of the integrated photoautotrophic production (from 135 to 7.626 mg C m−2d−1). At the oxycline, δ13POC averaged -22.209‰ this was significantly lighter compared to the surface (-19.674‰) and bottom layers (-20.716‰). This pattern, along with low NH4+ content and high accumulations of N2O, NO2- and NO3- within the oxycline indicates that chemolithoautotrophs and specifically AA oxydisers were active. Dark CA was reduced from 27 to 48% after addition of a specific AAO inhibitor (ATU) and from 24 to 76% with GC7, a specific archaea inhibitor, indicating that AAO and maybe AMO microbes (most of them archaea) were performing dark CA through oxidation of NH4+ and CH4. AAO produced N2O at rates from 8.88 to 43 nM d−1 and a fraction of it was effluxed into the atmosphere (up to 42.85 μmol m−2 d−1). AMO on the other hand consumed CH4 at rates between 0.41 and 26.8 nM d−1 therefore preventing its efflux to the atmosphere (up to 18.69 μmol m−2 d−1). These findings show that chemically driven chemoautotrophy (with NH4+ and CH4 acting as electron donors) could be more important than previously thought in upwelling ecosystems and open new questions concerning its future relevance.",what Sampling year ?,2009,,False,False
"The classical paradigm about marine N2 fixation establishes that this process is mainly constrained to nitrogen-poor tropical and subtropical regions, and sustained by the colonial cyanobacterium Trichodesmium spp. and diatom-diazotroph symbiosis. However, the application of molecular techniques allowed determining a high phylogenic diversity and a wide distribution of marine diazotrophs, which extends the range of ocean environments where biological N2 fixation may be relevant. Between February 2014 and December 2015, we carried out 10 one-day samplings in the upwelling system off NW Iberia in order to: 1) investigate the seasonal variability in the magnitude of N2 fixation, 2) determine its biogeochemical role as a mechanism of new nitrogen supply, and 3) quantify the main diazotrophs in the region under contrasting hydrographic regimes. Our results indicate that the magnitude of N2 fixation in this region was relatively low (0.001±0.002 – 0.095±0.024 µmol N m-3 d-1), comparable to the lower-end of rates described for the subtropical NE Atlantic. Maximum rates were observed at surface during both upwelling and relaxation conditions. The comparison with nitrate diffusive fluxes revealed the minor role of N2 fixation (2 fixation activity detected in the region. Quantitative PCR targeting the nifH gene revealed the highest abundances of two sublineages of Candidatus Atelocyanobacterium thalassa or UCYN-A (UCYN-A1 and UCYN-A2) mainly at surface waters during upwelling and relaxation conditions, and of Gammaproteobacteria γ-24774A11 at deep waters during downwelling. Maximum abundance for the three groups were up to 6.7 × 102, 1.5 × 103 and 2.4 × 104 nifH copies L-1, respectively. Our findings demonstrate measurable N2 fixation activity and presence of diazotrophs throughout the year in a nitrogen-rich temperate region.",what Sampling year ?,2014,"2014 and december 2015,",False,True
"We examined rates of N2 fixation from the surface to 2000 m depth in the Eastern Tropical South Pacific (ETSP) during El Niño (2010) and La Niña (2011). Replicated vertical profiles performed under oxygen-free conditions show that N2 fixation takes place both in euphotic and aphotic waters, with rates reaching 155 to 509 µmol N m−2 d−1 in 2010 and 24±14 to 118±87 µmol N m−2 d−1 in 2011. In the aphotic layers, volumetric N2 fixation rates were relatively low (<1.00 nmol N L−1 d−1), but when integrated over the whole aphotic layer, they accounted for 87–90% of total rates (euphotic+aphotic) for the two cruises. Phylogenetic studies performed in microcosms experiments confirm the presence of diazotrophs in the deep waters of the Oxygen Minimum Zone (OMZ), which were comprised of non-cyanobacterial diazotrophs affiliated with nifH clusters 1K (predominantly comprised of α-proteobacteria), 1G (predominantly comprised of γ-proteobacteria), and 3 (sulfate reducing genera of the δ-proteobacteria and Clostridium spp., Vibrio spp.). Organic and inorganic nutrient addition bioassays revealed that amino acids significantly stimulated N2 fixation in the core of the OMZ at all stations tested and as did simple carbohydrates at stations located nearest the coast of Peru/Chile. The episodic supply of these substrates from upper layers are hypothesized to explain the observed variability of N2 fixation in the ETSP.",what Sampling year ?,2010,,False,False
"We present new data on the nitrate (new production), ammonium, urea uptake rates and f‐ratios for the eastern Arabian Sea (10° to 22°N) during the late winter (northeast) monsoon, 2004, including regions of green Noctilucascintillans bloom. A comparison of N‐uptake rates of the Noctiluca dominated northern zone to the southern non‐bloom zone indicates the presence of two biogeochemical regimes during the late winter monsoon: highly productive north and less productive south. The conservative estimates of photic zone‐integrated total N‐uptake and f‐ratio are high in the north (∼19 mmolNm−2d−1 and 0.82, respectively) during the bloom and low (∼5.5 mmolNm−2d−1 and 0.38 respectively) in the south. The present and earlier data imply persistence of high N‐uptake and f‐ratio during blooms year after year. This quantification of the enhanced seasonal sequestration of carbon is an important input to global biogeochemical models.",what Sampling year ?,2004,"2004,",True,True
"Abstract Picophytoplankton were investigated during spring 2015 and 2016 extending from near‐shore coastal waters to oligotrophic open waters in the eastern Indian Ocean (EIO). They were typically composed of Prochlorococcus (Pro), Synechococcus (Syn), and picoeukaryotes (PEuks). Pro dominated most regions of the entire EIO and were approximately 1–2 orders of magnitude more abundant than Syn and PEuks. Under the influence of physicochemical conditions induced by annual variations of circulations and water masses, no coherent abundance and horizontal distributions of picophytoplankton were observed between spring 2015 and 2016. Although previous studies reported the limited effects of nutrients and heavy metals around coastal waters or upwelling zones could constrain Pro growth, Pro abundance showed strong positive correlation with nutrients, indicating the increase in nutrient availability particularly in the oligotrophic EIO could appreciably elevate their abundance. The exceptional appearance of picophytoplankton with high abundance along the equator appeared to be associated with the advection processes supported by the Wyrtki jets. For vertical patterns of picophytoplankton, a simple conceptual model was built based upon physicochemical parameters. However, Pro and PEuks simultaneously formed a subsurface maximum, while Syn generally restricted to the upper waters, significantly correlating with the combined effects of temperature, light, and nutrient availability. The average chlorophyll a concentrations (Chl a) of picophytoplankton accounted for above 49.6% and 44.9% of the total Chl a during both years, respectively, suggesting that picophytoplankton contributed a significant proportion of the phytoplankton community in the whole EIO.",what Sampling year ?,2016,2015,False,False
